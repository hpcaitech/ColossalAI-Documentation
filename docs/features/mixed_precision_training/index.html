<!doctype html>
<html class="docs-version-v0.2.2" lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.14">
<link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Colossal-AI RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Colossal-AI Atom Feed">
<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-1XKZVCCKRZ"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-1XKZVCCKRZ",{anonymize_ip:!0})</script>
<link rel="search" type="application/opensearchdescription+xml" title="Colossal-AI" href="/opensearch.xml">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" integrity="sha384-Um5gpz1odJg5Z4HAmzPtgZKdTBHZdw8S29IecapCSB31ligYPhHQZMIlWLYQGVoc" crossorigin="anonymous">
<script src="https://snack.expo.io/embed.js" async></script>
<script src="https://js-eu1.hs-scripts.com/26563514.js" async defer="defer" id="hs-script-loader"></script><title data-react-helmet="true">Auto Mixed Precision Training | Colossal-AI</title><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:url" content="https://colossalai.org/docs/features/mixed_precision_training"><meta data-react-helmet="true" name="docsearch:language" content="en"><meta data-react-helmet="true" name="docsearch:version" content="v0.2.2"><meta data-react-helmet="true" name="docsearch:docusaurus_tag" content="docs-default-v0.2.2"><meta data-react-helmet="true" property="og:title" content="Auto Mixed Precision Training | Colossal-AI"><meta data-react-helmet="true" name="description" content="Author: Chuanrui Wang, Shenggui Li, Yongbin Li"><meta data-react-helmet="true" property="og:description" content="Author: Chuanrui Wang, Shenggui Li, Yongbin Li"><link data-react-helmet="true" rel="icon" href="/img/favicon.ico"><link data-react-helmet="true" rel="canonical" href="https://colossalai.org/docs/features/mixed_precision_training"><link data-react-helmet="true" rel="alternate" href="https://colossalai.org/docs/features/mixed_precision_training" hreflang="en"><link data-react-helmet="true" rel="alternate" href="https://colossalai.org/zh-Hans/docs/features/mixed_precision_training" hreflang="zh-Hans"><link data-react-helmet="true" rel="alternate" href="https://colossalai.org/docs/features/mixed_precision_training" hreflang="x-default"><link data-react-helmet="true" rel="preconnect" href="https://XP2V2KAOVI-dsn.algolia.net" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.b5c97f7b.css">
<link rel="preload" href="/assets/js/runtime~main.b6ce1973.js" as="script">
<link rel="preload" href="/assets/js/main.aef369b8.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<link rel="preconnect" href="https://fonts.googleapis.com"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&amp;display=swap" rel="stylesheet"><div class="Toastify"></div><div><a href="#" class="skipToContent_1oUP">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner InnerContainer_1wkI"><div class="navbar__items"><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="Colossal-AI" class="themedImage_1VuW themedImage--light_3UqQ"><img src="/img/logo.svg" alt="Colossal-AI" class="themedImage_1VuW themedImage--dark_hz6m"></div><b class="navbar__title"> </b></a><a class="navbar__item navbar__link" href="/docs/get_started/installation">Tutorials</a><a href="https://github.com/hpcaitech/ColossalAI/tree/main/examples" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Examples</a><a href="http://docs.colossalai.org" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">API Docs</a><a href="https://www.hpc-ai.tech/blog" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Blog</a></div><div class="navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/docs/advanced_tutorials/add_your_parallel">v0.2.2</a><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" class="navbar__link"><span><svg viewBox="0 0 20 20" width="20" height="20" aria-hidden="true" class="iconLanguage_3vod"><path fill="currentColor" d="M19.753 10.909c-.624-1.707-2.366-2.726-4.661-2.726-.09 0-.176.002-.262.006l-.016-2.063 3.525-.607c.115-.019.133-.119.109-.231-.023-.111-.167-.883-.188-.976-.027-.131-.102-.127-.207-.109-.104.018-3.25.461-3.25.461l-.013-2.078c-.001-.125-.069-.158-.194-.156l-1.025.016c-.105.002-.164.049-.162.148l.033 2.307s-3.061.527-3.144.543c-.084.014-.17.053-.151.143.019.09.19 1.094.208 1.172.018.08.072.129.188.107l2.924-.504.035 2.018c-1.077.281-1.801.824-2.256 1.303-.768.807-1.207 1.887-1.207 2.963 0 1.586.971 2.529 2.328 2.695 3.162.387 5.119-3.06 5.769-4.715 1.097 1.506.256 4.354-2.094 5.98-.043.029-.098.129-.033.207l.619.756c.08.096.206.059.256.023 2.51-1.73 3.661-4.515 2.869-6.683zm-7.386 3.188c-.966-.121-.944-.914-.944-1.453 0-.773.327-1.58.876-2.156a3.21 3.21 0 011.229-.799l.082 4.277a2.773 2.773 0 01-1.243.131zm2.427-.553l.046-4.109c.084-.004.166-.01.252-.01.773 0 1.494.145 1.885.361.391.217-1.023 2.713-2.183 3.758zm-8.95-7.668a.196.196 0 00-.196-.145h-1.95a.194.194 0 00-.194.144L.008 16.916c-.017.051-.011.076.062.076h1.733c.075 0 .099-.023.114-.072l1.008-3.318h3.496l1.008 3.318c.016.049.039.072.113.072h1.734c.072 0 .078-.025.062-.076-.014-.05-3.083-9.741-3.494-11.04zm-2.618 6.318l1.447-5.25 1.447 5.25H3.226z"></path></svg><span>English</span></span></a><ul class="dropdown__menu"><li><a href="/docs/features/mixed_precision_training" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active">English</a></li><li><a href="/zh-Hans/docs/features/mixed_precision_training" target="_self" rel="noopener noreferrer" class="dropdown__link">简体中文</a></li></ul></div><div class="displayOnlyInLargeViewport_2uzv IconContainer_aWwG"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" class="Icon_3e04" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></div><div class="toggle_71bT toggleDisabled_3cF-"><div class="toggleTrack_32Fl" role="button" tabindex="-1"><div class="toggleTrackCheck_3lV7"><span class="toggleIcon_O4iE">🌜</span></div><div class="toggleTrackX_S2yS"><span class="toggleIcon_O4iE">🌞</span></div><div class="toggleTrackThumb_xI_Z"></div></div><input type="checkbox" class="toggleScreenReader_28Tw" aria-label="Switch between dark and light mode"></div><div class="searchBox_1ZXk"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div aria-label="Navigation bar toggle" class="navbar__toggle" role="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="Wrapper_ReNv main-docs-wrapper"><div class="docPage_3AUJ"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_35hR" type="button"></button><main class="docMainContainer_2AUC"><div class="padding-vert--lg container docItemWrapper_1WZa"><div class="row"><div class="col docItemCol_3FnS"><div class="docItemContainer_33ec"><article><div class="tocCollapsible_1PrD theme-doc-toc-mobile tocMobile_3Hoh"><button type="button" class="clean-btn tocCollapsibleButton_2O1e">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Auto Mixed Precision Training</h1></header><p>Author: Chuanrui Wang, Shenggui Li, Yongbin Li</p><p><strong>Prerequisite</strong></p><ul><li><a href="/docs/basics/define_your_config">Define Your Configuration</a></li><li><a href="/docs/basics/engine_trainer">Use Engine and Trainer in Training</a></li></ul><p><strong>Example Code</strong></p><ul><li><a href="https://github.com/hpcaitech/ColossalAI-Examples/tree/main/features/amp" target="_blank" rel="noopener noreferrer">ColossalAI-Examples AMP</a></li></ul><p><strong>Related Paper</strong></p><ul><li><a href="https://arxiv.org/abs/0808.2794" target="_blank" rel="noopener noreferrer">Accelerating Scientific Computations with Mixed Precision Algorithms</a></li></ul><h2 class="anchor anchorWithHideOnScrollNavbar_3R7-" id="introduction">Introduction<a class="hash-link" href="#introduction" title="Direct link to heading">​</a></h2><p>AMP stands for automatic mixed precision training.
In Colossal-AI, we have incorporated different implementations of mixed precision training:</p><ol><li>torch.cuda.amp</li><li>apex.amp</li><li>naive amp</li></ol><table><thead><tr><th>Colossal-AI</th><th>support tensor parallel</th><th>support pipeline parallel</th><th>fp16 extent</th></tr></thead><tbody><tr><td>AMP_TYPE.TORCH</td><td>✅</td><td>❌</td><td>Model parameters, activation, gradients are downcast to fp16 during forward and backward propagation</td></tr><tr><td>AMP_TYPE.APEX</td><td>❌</td><td>❌</td><td>More fine-grained, we can choose opt_level O0, O1, O2, O3</td></tr><tr><td>AMP_TYPE.NAIVE</td><td>✅</td><td>✅</td><td>Model parameters, forward and backward operations are all downcast to fp16</td></tr></tbody></table><p>The first two rely on the original implementation of PyTorch (version 1.6 and above) and NVIDIA Apex.
The last method is similar to Apex O2 level.
Among these methods, apex AMP is not compatible with tensor parallelism.
This is because that tensors are split across devices in tensor parallelism, thus, it is required to communicate among different processes to check if inf or nan occurs in the whole model weights.
We modified the torch amp implementation so that it is compatible with tensor parallelism now.</p><blockquote><p>❌️ fp16 and zero configuration are not compatible</p><p>⚠️ Pipeline only support naive AMP currently</p></blockquote><p>We recommend you to use torch AMP as it generally gives better accuracy than naive AMP if no pipeline is used.</p><h2 class="anchor anchorWithHideOnScrollNavbar_3R7-" id="table-of-contents">Table of Contents<a class="hash-link" href="#table-of-contents" title="Direct link to heading">​</a></h2><p>In this tutorial we will cover:</p><ol><li>AMP introduction</li><li>AMP in Colossal-AI</li><li>Hands-on Practice</li></ol><h2 class="anchor anchorWithHideOnScrollNavbar_3R7-" id="amp-introduction">AMP Introduction<a class="hash-link" href="#amp-introduction" title="Direct link to heading">​</a></h2><p>Automatic Mixed Precision training is a mixture of FP16 and FP32 training.</p><p>Half-precision float point format (FP16) has lower arithmetic complexity and higher compute efficiency.
Besides, fp16 requires half of the storage needed by fp32 and saves memory &amp; network bandwidth, which makes more memory
available for large batch size and model size.</p><p>However, there are other operations, like reductions, which require the dynamic range of fp32 to avoid numeric overflow/underflow. That&#x27;s the reason why we introduce automatic mixed precision, attempting to match each operation to its appropriate data type, which can reduce the memory footprint and augment training efficiency.</p><figure style="text-align:center"><img src="https://s2.loli.net/2022/01/28/URzLJ3MPeDQbtck.png"><figcaption>Illustration of an ordinary AMP (figure from <a href="https://arxiv.org/abs/2108.05818" target="_blank" rel="noopener noreferrer">PatrickStar paper</a>)</figcaption></figure><h2 class="anchor anchorWithHideOnScrollNavbar_3R7-" id="amp-in-colossal-ai">AMP in Colossal-AI<a class="hash-link" href="#amp-in-colossal-ai" title="Direct link to heading">​</a></h2><p>We supported three AMP training methods and allowed the user to train with AMP with no code. You can just simply add <code>fp16</code>
configuration in your configuration file to use AMP.</p><div class="codeBlockContainer_K1bP language-python theme-code-block"><div class="codeBlockContent_hGly python"><pre tabindex="0" class="prism-code language-python codeBlock_23N8 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_39YC"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> colossalai</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">amp </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> AMP_TYPE</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># use Torch AMP</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">fp16</span><span class="token operator" style="color:#393A34">=</span><span class="token builtin">dict</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    mode </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> AMP_TYPE</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">TORCH</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># use naive AMP</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">fp16</span><span class="token operator" style="color:#393A34">=</span><span class="token builtin">dict</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    mode </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> AMP_TYPE</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">NAIVE</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># use NVIDIA Apex AMP</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">fp16</span><span class="token operator" style="color:#393A34">=</span><span class="token builtin">dict</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    mode </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> AMP_TYPE</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">APEX</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_Ue-o clean-btn">Copy</button></div></div><blockquote><p>These are the minimum configuration, full configuration are stated in the section later</p></blockquote><h3 class="anchor anchorWithHideOnScrollNavbar_3R7-" id="amp-modularity">AMP Modularity<a class="hash-link" href="#amp-modularity" title="Direct link to heading">​</a></h3><p>AMP module is designed to be completely modular and can be used independently.
If you wish to only use AMP in your code base without <code>colossalai.initialize</code>,
you can use <code>colossalai.amp.convert_to_amp</code>.</p><div class="codeBlockContainer_K1bP language-python theme-code-block"><div class="codeBlockContent_hGly python"><pre tabindex="0" class="prism-code language-python codeBlock_23N8 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_39YC"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> colossalai</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">amp </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> AMP_TYPE</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># exmaple of using torch amp</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">model</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> optimizer</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> criterion </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> colossalai</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">amp</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">convert_to_amp</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">model</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                                            optimizer</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                                            criterion</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                                            AMP_TYPE</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">TORCH</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_Ue-o clean-btn">Copy</button></div></div><h3 class="anchor anchorWithHideOnScrollNavbar_3R7-" id="torch-amp-configuration">Torch AMP Configuration<a class="hash-link" href="#torch-amp-configuration" title="Direct link to heading">​</a></h3><div class="codeBlockContainer_K1bP language-python theme-code-block"><div class="codeBlockContent_hGly python"><pre tabindex="0" class="prism-code language-python codeBlock_23N8 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_39YC"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> colossalai</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">amp </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> AMP_TYPE</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">fp16</span><span class="token operator" style="color:#393A34">=</span><span class="token builtin">dict</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    mode</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">AMP_TYPE</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">TORCH</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># below are default values for grad scaler</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    init_scale</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">2</span><span class="token punctuation" style="color:#393A34">.</span><span class="token operator" style="color:#393A34">**</span><span class="token number" style="color:#36acaa">16</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    growth_factor</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">2.0</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    backoff_factor</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">0.5</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    growth_interval</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">2000</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    enabled</span><span class="token operator" style="color:#393A34">=</span><span class="token boolean" style="color:#36acaa">True</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_Ue-o clean-btn">Copy</button></div></div><p>With optional arguments:</p><ul><li>init_scale(float, optional, default=2.**16): Initial scale factor</li><li>growth_factor(float, optional, default=2.0): Factor by which the scale is multiplied during <code>update</code> if no inf/NaN gradients occur for <code>growth_interval</code> consecutive iterations.</li><li>backoff_factor(float, optional, default=0.5): Factor by which the scale is multiplied during <code>update</code> if inf/NaN gradients occur in an iteration.</li><li>growth_interval(int, optional, default=2000): Number of consecutive iterations without inf/NaN gradients that must occur for the scale to be multiplied by <code>growth_factor</code>.</li><li>enabled(bool, optional, default=True): If <code>False</code>, disables gradient scaling. <code>step</code> simply invokes the underlying <code>optimizer.step()</code>, and other methods become no-ops.</li></ul><h3 class="anchor anchorWithHideOnScrollNavbar_3R7-" id="apex-amp-configuration">Apex AMP Configuration<a class="hash-link" href="#apex-amp-configuration" title="Direct link to heading">​</a></h3><p>For this mode, we rely on the Apex implementation for mixed precision training.
We support this plugin because it allows for finer control on the granularity of mixed precision.
For example, O2 level (optimization level 2) will keep batch normalization in fp32.</p><p>If you look for more details, please refer to <a href="https://nvidia.github.io/apex/" target="_blank" rel="noopener noreferrer">Apex Documentation</a>.</p><div class="codeBlockContainer_K1bP language-python theme-code-block"><div class="codeBlockContent_hGly python"><pre tabindex="0" class="prism-code language-python codeBlock_23N8 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_39YC"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> colossalai</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">amp </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> AMP_TYPE</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">fp16 </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token builtin">dict</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    mode</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">AMP_TYPE</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">APEX</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># below are the default values</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    enabled</span><span class="token operator" style="color:#393A34">=</span><span class="token boolean" style="color:#36acaa">True</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    opt_level</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">&#x27;O1&#x27;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    cast_model_type</span><span class="token operator" style="color:#393A34">=</span><span class="token boolean" style="color:#36acaa">None</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    patch_torch_functions</span><span class="token operator" style="color:#393A34">=</span><span class="token boolean" style="color:#36acaa">None</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    keep_batchnorm_fp32</span><span class="token operator" style="color:#393A34">=</span><span class="token boolean" style="color:#36acaa">None</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    master_weights</span><span class="token operator" style="color:#393A34">=</span><span class="token boolean" style="color:#36acaa">None</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    loss_scale</span><span class="token operator" style="color:#393A34">=</span><span class="token boolean" style="color:#36acaa">None</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    cast_model_outputs</span><span class="token operator" style="color:#393A34">=</span><span class="token boolean" style="color:#36acaa">None</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    num_losses</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    verbosity</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    min_loss_scale</span><span class="token operator" style="color:#393A34">=</span><span class="token boolean" style="color:#36acaa">None</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    max_loss_scale</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">16777216.0</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_Ue-o clean-btn">Copy</button></div></div><p>Parameters:</p><ul><li><p>enabled(bool, optional, default=True): If False, renders all AMP calls no-ops, so your script should run as if Amp were not present.</p></li><li><p>opt_level(str, optional, default=&quot;O1&quot; ): Pure or mixed precision optimization level.
Accepted values are “O0”, “O1”, “O2”, and “O3”, explained in detail above Apex AMP Documentation.</p></li><li><p>num_losses(int, optional, default=1): Option to tell AMP in advance how many losses/backward passes you plan to use.
When used in conjunction with the loss_id argument to <code>amp.scale_loss</code>, enables Amp to use a different loss scale per
loss/backward pass, which can improve stability. If num_losses is left to 1, Amp will still support multiple
losses/backward passes, but use a single global loss scale for all of them.</p></li><li><p>verbosity(int, default=1): Set to 0 to suppress Amp-related output.</p></li><li><p>min_loss_scale(float, default=None): Sets a floor for the loss scale values that can be chosen by dynamic loss scaling.
The default value of None means that no floor is imposed. If dynamic loss scaling is not used, min_loss_scale is ignored.</p></li><li><p>max_loss_scale(float, default=2.**24 ): Sets a ceiling for the loss scale values that can be chosen by dynamic loss
scaling. If dynamic loss scaling is not used, max_loss_scale is ignored.</p></li></ul><p>Currently, the under-the-hood properties that govern pure or mixed precision training are the following:
cast_model_type, patch_torch_functions, keep_batchnorm_fp32, master_weights, loss_scale.
They are optional properties override once opt_level is determined</p><ul><li>cast_model_type: Casts your model’s parameters and buffers to the desired type.</li><li>patch_torch_functions: Patch all Torch functions and Tensor methods to perform Tensor Core-friendly ops like GEMMs and convolutions in FP16, and any ops that benefit from FP32 precision in FP32.</li><li>keep_batchnorm_fp32: To enhance precision and enable cudnn batchnorm (which improves performance), it’s often beneficial to keep batchnorm weights in FP32 even if the rest of the model is FP16.</li><li>master_weights: Maintain FP32 master weights to accompany any FP16 model weights. FP32 master weights are stepped by the optimizer to enhance precision and capture small gradients.</li><li>loss_scale: If loss_scale is a float value, use this value as the static (fixed) loss scale. If loss_scale is the string &quot;dynamic&quot;, adaptively adjust the loss scale over time. Dynamic loss scale adjustments are performed by Amp automatically.</li></ul><h3 class="anchor anchorWithHideOnScrollNavbar_3R7-" id="naive-amp-configuration">Naive AMP Configuration<a class="hash-link" href="#naive-amp-configuration" title="Direct link to heading">​</a></h3><p>In Naive AMP mode, we achieved mixed precision training while maintaining compatibility with complex tensor and pipeline parallelism.
This AMP mode will cast all operations into fp16.
The following code block shows the <code>config.py</code> file for this mode.</p><div class="codeBlockContainer_K1bP language-python theme-code-block"><div class="codeBlockContent_hGly python"><pre tabindex="0" class="prism-code language-python codeBlock_23N8 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_39YC"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> colossalai</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">amp </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> AMP_TYPE</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">fp16 </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token builtin">dict</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    mode</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">AMP_TYPE</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">NAIVE</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># below are the default values</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    log_num_zeros_in_grad</span><span class="token operator" style="color:#393A34">=</span><span class="token boolean" style="color:#36acaa">False</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    initial_scale</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">2</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">**</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">32</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    min_scale</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    growth_factor</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">2</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    backoff_factor</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">0.5</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    growth_interval</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">1000</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    hysteresis</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">2</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_Ue-o clean-btn">Copy</button></div></div><p>The default parameters of Naive AMP:</p><ul><li>log_num_zeros_in_grad(bool): return number of zeros in the gradients.</li><li>initial_scale(int): initial scale of gradient scaler</li><li>growth_factor(int): the growth rate of loss scale</li><li>backoff_factor(float): the decrease rate of loss scale</li><li>hysterisis(int): delay shift in dynamic loss scaling</li><li>max_scale(int): maximum loss scale allowed</li><li>verbose(bool): if set to <code>True</code>, will print debug info</li></ul><p>When using <code>colossalai.initialize</code>, you are required to first instantiate a model, an optimizer and a criterion.
The output model is converted to AMP model of smaller memory consumption.
If your input model is already too large to fit in a GPU, please instantiate your model weights in <code>dtype=torch.float16</code>.
Otherwise, try smaller models or checkout more parallelization training techniques!</p><h2 class="anchor anchorWithHideOnScrollNavbar_3R7-" id="hands-on-practice">Hands-on Practice<a class="hash-link" href="#hands-on-practice" title="Direct link to heading">​</a></h2><p>We provide a <a href="https://github.com/hpcaitech/ColossalAI-Examples/tree/main/features/amp" target="_blank" rel="noopener noreferrer">runnable example</a> which demonstrates
the use of AMP with Colossal-AI. In this practice, we will use Torch AMP as an example, but do note that config files are provided for all AMP modes.</p><h3 class="anchor anchorWithHideOnScrollNavbar_3R7-" id="step-1-create-a-config-file">Step 1. Create a config file<a class="hash-link" href="#step-1-create-a-config-file" title="Direct link to heading">​</a></h3><p>Create a <code>config.py</code> and add the <code>fp16</code> configuration.</p><div class="codeBlockContainer_K1bP language-python theme-code-block"><div class="codeBlockContent_hGly python"><pre tabindex="0" class="prism-code language-python codeBlock_23N8 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_39YC"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># in config.py</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> colossalai</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">amp </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> AMP_TYPE</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">BATCH_SIZE </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">128</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">DROP_RATE </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0.1</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">NUM_EPOCHS </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">300</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">fp16 </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token builtin">dict</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    mode</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">AMP_TYPE</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">TORCH</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">clip_grad_norm </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1.0</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_Ue-o clean-btn">Copy</button></div></div><h3 class="anchor anchorWithHideOnScrollNavbar_3R7-" id="step-2-import-libraries-in-train_with_enginepy">Step 2. Import libraries in train_with_engine.py<a class="hash-link" href="#step-2-import-libraries-in-train_with_enginepy" title="Direct link to heading">​</a></h3><p>Create a <code>train_with_engine.py</code> and import the necessary dependencies. Remember to install <code>scipy</code> and <code>timm</code> by running
<code>pip install timm scipy</code>.</p><div class="codeBlockContainer_K1bP language-python theme-code-block"><div class="codeBlockContent_hGly python"><pre tabindex="0" class="prism-code language-python codeBlock_23N8 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_39YC"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> os</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> colossalai</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> torch</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> pathlib </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> Path</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> colossalai</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">core </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> global_context </span><span class="token keyword" style="color:#00009f">as</span><span class="token plain"> gpc</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> colossalai</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">logging </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> get_dist_logger</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> colossalai</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">utils </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> get_dataloader</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> colossalai</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">trainer </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> Trainer</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> hooks</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> colossalai</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">nn</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">lr_scheduler </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> LinearWarmupLR</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> timm</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">models </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> vit_base_patch16_224</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> torchvision </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> datasets</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> transforms</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_Ue-o clean-btn">Copy</button></div></div><h3 class="anchor anchorWithHideOnScrollNavbar_3R7-" id="step-3-initialize-distributed-environment">Step 3. Initialize Distributed Environment<a class="hash-link" href="#step-3-initialize-distributed-environment" title="Direct link to heading">​</a></h3><p>We then need to initialize distributed environment. For demo purpose, we uses <code>launch_from_torch</code>. You can refer to <a href="/docs/basics/launch_colossalai">Launch Colossal-AI</a>
for other initialization methods.</p><div class="codeBlockContainer_K1bP language-python theme-code-block"><div class="codeBlockContent_hGly python"><pre tabindex="0" class="prism-code language-python codeBlock_23N8 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_39YC"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># initialize distributed setting</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">parser </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> colossalai</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">get_default_parser</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">args </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> parser</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">parse_args</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># launch from torch</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">colossalai</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">launch_from_torch</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">config</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">args</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">config</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_Ue-o clean-btn">Copy</button></div></div><h3 class="anchor anchorWithHideOnScrollNavbar_3R7-" id="step-4-create-training-components">Step 4. Create training components<a class="hash-link" href="#step-4-create-training-components" title="Direct link to heading">​</a></h3><p>Build your model, optimizer, loss function, lr scheduler and dataloaders. Note that the root path of the dataset is
obtained from the environment varialbe <code>DATA</code>. You may <code>export DATA=/path/to/data</code> or change <code>Path(os.environ[&#x27;DATA&#x27;])</code>
to a path on your machine. Data will be automatically downloaded to the root path.</p><div class="codeBlockContainer_K1bP language-python theme-code-block"><div class="codeBlockContent_hGly python"><pre tabindex="0" class="prism-code language-python codeBlock_23N8 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_39YC"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># build model</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    model </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> vit_base_patch16_224</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">drop_rate</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">0.1</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># build dataloader</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    train_dataset </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> datasets</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">Caltech101</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        root</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">Path</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">os</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">environ</span><span class="token punctuation" style="color:#393A34">[</span><span class="token string" style="color:#e3116c">&#x27;DATA&#x27;</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        download</span><span class="token operator" style="color:#393A34">=</span><span class="token boolean" style="color:#36acaa">True</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        transform</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">transforms</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">Compose</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            transforms</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">Resize</span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">256</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            transforms</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">RandomResizedCrop</span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">224</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            transforms</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">RandomHorizontalFlip</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            transforms</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">ToTensor</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            Gray2RGB</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            transforms</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">Normalize</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">[</span><span class="token number" style="color:#36acaa">0.5</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0.5</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0.5</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                 </span><span class="token punctuation" style="color:#393A34">[</span><span class="token number" style="color:#36acaa">0.5</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0.5</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0.5</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    train_dataloader </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> get_dataloader</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">dataset</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">train_dataset</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                      shuffle</span><span class="token operator" style="color:#393A34">=</span><span class="token boolean" style="color:#36acaa">True</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                      batch_size</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">gpc</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">config</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">BATCH_SIZE</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                      num_workers</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                      pin_memory</span><span class="token operator" style="color:#393A34">=</span><span class="token boolean" style="color:#36acaa">True</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                      </span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># build optimizer</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    optimizer </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> torch</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">optim</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">SGD</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">model</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">parameters</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> lr</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">1e</span><span class="token operator" style="color:#393A34">-</span><span class="token number" style="color:#36acaa">2</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> weight_decay</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">0.1</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># build loss</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    criterion </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> torch</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">nn</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">CrossEntropyLoss</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># lr_scheduelr</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    lr_scheduler </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> LinearWarmupLR</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">optimizer</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> warmup_steps</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">50</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> total_steps</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">gpc</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">config</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">NUM_EPOCHS</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_Ue-o clean-btn">Copy</button></div></div><h3 class="anchor anchorWithHideOnScrollNavbar_3R7-" id="step-5-inject-amp-feature">Step 5. Inject AMP Feature<a class="hash-link" href="#step-5-inject-amp-feature" title="Direct link to heading">​</a></h3><p>Call <code>colossalai.initialize</code> to convert the training components to be running with FP16.</p><div class="codeBlockContainer_K1bP language-python theme-code-block"><div class="codeBlockContent_hGly python"><pre tabindex="0" class="prism-code language-python codeBlock_23N8 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_39YC"><span class="token-line" style="color:#393A34"><span class="token plain">engine</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> train_dataloader</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> _</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> _ </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> colossalai</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">initialize</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        model</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> optimizer</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> criterion</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> train_dataloader</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_Ue-o clean-btn">Copy</button></div></div><h3 class="anchor anchorWithHideOnScrollNavbar_3R7-" id="step-6-train-with-engine">Step 6. Train with Engine<a class="hash-link" href="#step-6-train-with-engine" title="Direct link to heading">​</a></h3><p>Use engine in a normal training loops.</p><div class="codeBlockContainer_K1bP language-python theme-code-block"><div class="codeBlockContent_hGly python"><pre tabindex="0" class="prism-code language-python codeBlock_23N8 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_39YC"><span class="token-line" style="color:#393A34"><span class="token plain">engine</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">train</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> epoch </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> </span><span class="token builtin">range</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">gpc</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">config</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">NUM_EPOCHS</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> img</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> label </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> </span><span class="token builtin">enumerate</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">train_dataloader</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        img </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> img</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">cuda</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        label </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> label</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">cuda</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        engine</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">zero_grad</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        output </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> engine</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">img</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        loss </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> engine</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">criterion</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">output</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> label</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        engine</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">backward</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">loss</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        engine</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">step</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        lr_scheduler</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">step</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_Ue-o clean-btn">Copy</button></div></div><h3 class="anchor anchorWithHideOnScrollNavbar_3R7-" id="step-7-invoke-training-scripts">Step 7. Invoke Training Scripts<a class="hash-link" href="#step-7-invoke-training-scripts" title="Direct link to heading">​</a></h3><p>Use the following command to start the training scripts. You can change <code>--nproc_per_node</code> to use a different number of GPUs.</p><div class="codeBlockContainer_K1bP language-python theme-code-block"><div class="codeBlockContent_hGly python"><pre tabindex="0" class="prism-code language-python codeBlock_23N8 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_39YC"><span class="token-line" style="color:#393A34"><span class="token plain">python </span><span class="token operator" style="color:#393A34">-</span><span class="token plain">m torch</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">distributed</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">launch </span><span class="token operator" style="color:#393A34">-</span><span class="token operator" style="color:#393A34">-</span><span class="token plain">nproc_per_node </span><span class="token number" style="color:#36acaa">4</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">-</span><span class="token operator" style="color:#393A34">-</span><span class="token plain">master_addr localhost </span><span class="token operator" style="color:#393A34">-</span><span class="token operator" style="color:#393A34">-</span><span class="token plain">master_port </span><span class="token number" style="color:#36acaa">29500</span><span class="token plain"> train_with_engine</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">py </span><span class="token operator" style="color:#393A34">-</span><span class="token operator" style="color:#393A34">-</span><span class="token plain">config config</span><span class="token operator" style="color:#393A34">/</span><span class="token plain">config_AMP_torch</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">py</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_Ue-o clean-btn">Copy</button></div></div></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><div class="pagination-nav__item"></div><div class="pagination-nav__item pagination-nav__item--next"></div></nav></div></div><div class="col col--3"><div class="tableOfContents_35-E thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#introduction" class="table-of-contents__link toc-highlight">Introduction</a></li><li><a href="#table-of-contents" class="table-of-contents__link toc-highlight">Table of Contents</a></li><li><a href="#amp-introduction" class="table-of-contents__link toc-highlight">AMP Introduction</a></li><li><a href="#amp-in-colossal-ai" class="table-of-contents__link toc-highlight">AMP in Colossal-AI</a><ul><li><a href="#amp-modularity" class="table-of-contents__link toc-highlight">AMP Modularity</a></li><li><a href="#torch-amp-configuration" class="table-of-contents__link toc-highlight">Torch AMP Configuration</a></li><li><a href="#apex-amp-configuration" class="table-of-contents__link toc-highlight">Apex AMP Configuration</a></li><li><a href="#naive-amp-configuration" class="table-of-contents__link toc-highlight">Naive AMP Configuration</a></li></ul></li><li><a href="#hands-on-practice" class="table-of-contents__link toc-highlight">Hands-on Practice</a><ul><li><a href="#step-1-create-a-config-file" class="table-of-contents__link toc-highlight">Step 1. Create a config file</a></li><li><a href="#step-2-import-libraries-in-train_with_enginepy" class="table-of-contents__link toc-highlight">Step 2. Import libraries in train_with_engine.py</a></li><li><a href="#step-3-initialize-distributed-environment" class="table-of-contents__link toc-highlight">Step 3. Initialize Distributed Environment</a></li><li><a href="#step-4-create-training-components" class="table-of-contents__link toc-highlight">Step 4. Create training components</a></li><li><a href="#step-5-inject-amp-feature" class="table-of-contents__link toc-highlight">Step 5. Inject AMP Feature</a></li><li><a href="#step-6-train-with-engine" class="table-of-contents__link toc-highlight">Step 6. Train with Engine</a></li><li><a href="#step-7-invoke-training-scripts" class="table-of-contents__link toc-highlight">Step 7. Invoke Training Scripts</a></li></ul></li></ul></div></div></div></div></main></div></div><footer class="footer Container_ZO7N"><div class="InnerContainer_kEOB"><div class="ContentContainer_Bd2W"><div class="FooterLeft_UU7Q"><div class="BrandContainer_37NB"><img class="BrandImage_1lbV" alt="AgileTs Logo" height="30" src="/img/logo.svg"></div><div class="Tagline_2AGk">An integrated large-scale model training system with efficient parallelization techniques.</div><button class="ButtonContainer_oPl2 GithubButton_1Y3L"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" class="GithubIcon_3htU" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg><div>GITHUB</div></button></div><div class="FooterRight_1NHD"><div class="SectionContainer_2QT6"><li class="LinkItemTitle_1y09">Resources</li><ul class="LinkItemContainer_2Lmc"><a class="LinkText_1B0E" href="/docs/get_started/installation" label="Tutorials" to="docs/get_started/installation">Tutorials</a></ul><ul class="LinkItemContainer_2Lmc"><a class="LinkText_1B0E" href="http://docs.colossalai.org" label="API Docs" to="http://docs.colossalai.org">API Docs</a></ul><ul class="LinkItemContainer_2Lmc"><a class="LinkText_1B0E" href="https://github.com/hpcaitech/ColossalAI/tree/main/examples" label="Examples" to="https://github.com/hpcaitech/ColossalAI/tree/main/examples">Examples</a></ul><ul class="LinkItemContainer_2Lmc"><a class="LinkText_1B0E" href="https://github.com/hpcaitech/ColossalAI/discussions" label="Forum" to="https://github.com/hpcaitech/ColossalAI/discussions">Forum</a></ul></div><div class="SectionContainer_2QT6"><li class="LinkItemTitle_1y09">Community</li><ul class="LinkItemContainer_2Lmc"><a class="LinkText_1B0E" href="https://github.com/hpcaitech/ColossalAI" rel="noopener noreferrer" target="_blank" label="GitHub">GitHub</a></ul><ul class="LinkItemContainer_2Lmc"><a class="LinkText_1B0E" href="https://www.hpc-ai.tech/blog" rel="noopener noreferrer" target="_blank" label="Blog">Blog</a></ul><ul class="LinkItemContainer_2Lmc"><a class="LinkText_1B0E" href="https://twitter.com/HPCAITech" rel="noopener noreferrer" target="_blank" label="Twitter">Twitter</a></ul></div></div></div><div class="BottomContainer_1let"><div class="CopyrightText_25Fq">Copyright © 2023 All Rights Reserved by HPC-AI Technology Inc.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.b6ce1973.js"></script>
<script src="/assets/js/main.aef369b8.js"></script>
</body>
</html>