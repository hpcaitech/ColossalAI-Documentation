<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-features/shardformer">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.3.1">
<title data-rh="true">Shardformer | Colossal-AI</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://colossalai.org/img/social-card.png"><meta data-rh="true" name="twitter:image" content="https://colossalai.org/img/social-card.png"><meta data-rh="true" property="og:url" content="https://colossalai.org/docs/features/shardformer"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Shardformer | Colossal-AI"><meta data-rh="true" name="description" content="Author: Baizhou Zhang, Bin Jia"><meta data-rh="true" property="og:description" content="Author: Baizhou Zhang, Bin Jia"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://colossalai.org/docs/features/shardformer"><link data-rh="true" rel="alternate" href="https://colossalai.org/docs/features/shardformer" hreflang="en"><link data-rh="true" rel="alternate" href="https://colossalai.org/zh-Hans/docs/features/shardformer" hreflang="zh-Hans"><link data-rh="true" rel="alternate" href="https://colossalai.org/docs/features/shardformer" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://XP2V2KAOVI-dsn.algolia.net" crossorigin="anonymous"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Colossal-AI RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Colossal-AI Atom Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-1XKZVCCKRZ"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-1XKZVCCKRZ",{anonymize_ip:!0})</script>


<link rel="search" type="application/opensearchdescription+xml" title="Colossal-AI" href="/opensearch.xml">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous">
<script src="https://js-eu1.hs-scripts.com/26563514.js" id="hs-script-loader" async defer="defer"></script><link rel="stylesheet" href="/assets/css/styles.d9a9b3d5.css">
<link rel="preload" href="/assets/js/runtime~main.00d66150.js" as="script">
<link rel="preload" href="/assets/js/main.b8498c50.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="project-logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/logo.svg" alt="project-logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">Colossal-AI</b></a><a class="navbar__item navbar__link" href="/docs/get_started/installation">Tutorials</a><a href="https://github.com/hpcaitech/ColossalAI/tree/main/examples" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Examples</a><a href="https://www.hpc-ai.tech/blog" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Blogs</a></div><div class="navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/docs/get_started/installation">Next</a><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_nlXk"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>English</a><ul class="dropdown__menu"><li><a href="/docs/features/shardformer" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="en">English</a></li><li><a href="/zh-Hans/docs/features/shardformer" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-Hans">简体中文</a></li></ul></div><a href="https://github.com/hpcaitech/ColossalAI" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-github-link"></a><div class="searchBox_ZlJk"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/get_started/installation">Get started</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/concepts/distributed_training">Concepts</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/basics/command_line_tool">Basics</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/docs/features/shardformer">Features</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/features/shardformer">Shardformer</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/features/mixed_precision_training_with_booster">Auto Mixed Precision Training</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/features/gradient_accumulation_with_booster">Gradient Accumulation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/features/gradient_clipping_with_booster">Gradient Clipping</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/features/zero_with_chunk">Zero Redundancy Optimizer with chunk-based memory management</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/features/1D_tensor_parallel">Tensor Parallel</a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/features/pipeline_parallel">Pipeline Parallel</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/features/nvme_offload">NVMe offload</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/features/cluster_utils">Cluster Utilities</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/advanced_tutorials/train_vit_using_pipeline_parallelism">Advanced Tutorials</a></div></li></ul></nav></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Features</span><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Shardformer</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>Shardformer</h1><p>Author: <a href="https://github.com/Fridge003" target="_blank" rel="noopener noreferrer">Baizhou Zhang</a>, <a href="https://github.com/FoolPlayer" target="_blank" rel="noopener noreferrer">Bin Jia</a></p><p><strong>Prerequisite</strong></p><ul><li><a href="/docs/concepts/paradigms_of_parallelism">Paradigms of Parallelism</a></li><li><a href="/docs/basics/booster_api">Booster API</a></li><li><a href="/docs/basics/booster_plugins">Booster Plugins</a></li></ul><p><strong>Example Code</strong></p><ul><li><a href="https://github.com/hpcaitech/ColossalAI/tree/main/colossalai/shardformer/examples" target="_blank" rel="noopener noreferrer">Tensor Parallelism with Shardformer</a></li><li><a href="https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/bert" target="_blank" rel="noopener noreferrer">Enabling Shardformer using HybridPrallelPlugin</a></li></ul><p><strong>Related Paper</strong></p><ul><li><a href="https://arxiv.org/abs/2104.04473" target="_blank" rel="noopener noreferrer">Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM</a></li><li><a href="https://arxiv.org/abs/1811.06965" target="_blank" rel="noopener noreferrer">GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism</a></li><li><a href="https://arxiv.org/abs/2307.08691" target="_blank" rel="noopener noreferrer">FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning</a></li><li><a href="https://arxiv.org/abs/2105.13120" target="_blank" rel="noopener noreferrer">Sequence Parallelism: Long Sequence Training from System Perspective</a></li><li><a href="https://arxiv.org/abs/2205.05198" target="_blank" rel="noopener noreferrer">Reducing Activation Recomputation in Large Transformer Models</a></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="introduction">Introduction<a href="#introduction" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2><p>When training large transformer models such as LLaMa-2 70B or OPT 175B, model parallelism methods that divide a huge model into smaller shards, including tensor parallelism or pipeline parallism, are essential so as to meet the limitation of GPU memory.
However, manually cutting model and rewriting its forward/backword logic could be difficult for users who are not familiar with distributed training.
Meanwhile, the Huggingface transformers library has gradually become users&#x27; first choice of model source, and most mainstream large models have been open-sourced in Huggingface transformers model library.</p><p>Out of this motivation, the ColossalAI team develops <strong>Shardformer</strong>, a feature that automatically does preparation of model parallelism (tensor parallelism/pipeline parallelism) for popular transformer models in HuggingFace.
This module aims to make parallelization hassle-free for users who are not from the system background.
Within a few lines of codes, users can turn a model into a state ready for distributed training.
Also, Shardformer contains various optimization tools for acceleration and memory saving during forward/backward pass.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="supporting-information">Supporting Information<a href="#supporting-information" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2><p>Model/Feature Compatibility Matrix:</p><table><tr><th nowrap="nowrap">Model/Feature</th><th nowrap="nowrap" title="Tensor Parallel">Tensor<br>Parallel</th><th nowrap="nowrap" align="center" title="Pipeline Parallel">Pipeline<br>Parallel</th><th nowrap="nowrap" align="center" title="Lazy Initialization">Lazy<br>Initialization</th><th nowrap="nowrap" align="center" title="xFormers">xFormers</th><th nowrap="nowrap" align="center" title="Flash Attention 2">Flash<br>Attention 2</th><th nowrap="nowrap" align="center" title="JIT Fused Operators">JIT Fused<br>Operators</th><th nowrap="nowrap" align="center" title="Fused LayerNorm">Fused<br>LayerNorm</th><th nowrap="nowrap" align="center" title="Sequence Parallel">Sequence<br>Parallel</th><th nowrap="nowrap" align="center" title="Sequence Overlap">Sequence<br>Overlap</th></tr><tr><td nowrap="nowrap">Llama V1/V2</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">❌</td><td nowrap="nowrap" align="center">❌</td></tr><tr><td nowrap="nowrap">OPT</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">❌</td><td nowrap="nowrap" align="center">❌</td></tr><tr><td nowrap="nowrap">BLOOM</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td></tr><tr><td nowrap="nowrap">ChatGLM 2</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td></tr><tr><td nowrap="nowrap">BERT</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td></tr><tr><td nowrap="nowrap">GPT 2</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td></tr><tr><td nowrap="nowrap">T5</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">❌</td><td nowrap="nowrap" align="center">❌</td></tr><tr><td nowrap="nowrap">ViT</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">❌</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">❌</td><td nowrap="nowrap" align="center">❌</td></tr><tr><td nowrap="nowrap">Whisper</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">❌</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">❌</td><td nowrap="nowrap" align="center">❌</td></tr><tr><td nowrap="nowrap">SAM</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">❌</td><td nowrap="nowrap" align="center">❌</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">❌</td><td nowrap="nowrap" align="center">❌</td></tr><tr><td nowrap="nowrap">Blip2</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">❌</td><td nowrap="nowrap" align="center">❌</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">❌</td><td nowrap="nowrap" align="center">❌</td></tr><tr><td colspan="39"></td></tr></table><p>List of model families we plan to support in the near future:</p><ul><li>RoBERTa</li><li>ALBERT</li><li>ERNIE</li><li>GPT Neo</li><li>GPT-J</li><li>BEiT</li><li>SwinTransformer V1/V2</li><li>qwen</li></ul><p>The support matrix will grow larger as more models and optimization tools emerge in the future. If you have any suggestions on the models/optimization we should support, please feel free to mention it in <a href="https://github.com/hpcaitech/ColossalAI/issues" target="_blank" rel="noopener noreferrer">Issues</a> section of our project.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="usage">Usage<a href="#usage" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="shardformer-configuration">Shardformer Configuration<a href="#shardformer-configuration" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3><p>The configuration of Shardformer is controlled by class <code>ShardConfig</code>:</p><div class="docstring-container"><div><div class="title-container"><div class="title-module"><h5>class</h5>  <h3>colossalai.shardformer.ShardConfig</h3></div><div class="title-source">&lt;<a href="https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/shardformer/shard/shard_config.py#L13" class="title-source">source</a>&gt;</div></div><div class="signature">(<!-- -->tensor_parallel_process_group: typing.Optional[torch.distributed.distributed_c10d.ProcessGroup] = None, pipeline_stage_manager: typing.Optional[colossalai.pipeline.stage_manager.PipelineStageManager] = None, enable_tensor_parallelism: bool = True, enable_fused_normalization: bool = False, enable_flash_attention: bool = False, enable_jit_fused: bool = False, enable_all_optimization: bool = False, inference_only: bool = False, inference_gptq: bool = False, enable_sequence_parallelism: bool = False, enable_sequence_overlap: bool = False<!-- -->)</div><div><div class="divider"><span class="divider-text">Parameters</span></div><ul>
<li><strong>tensor_parallel_process_group</strong> (Optional[ProcessGroup]) -- The process group of tensor parallelism, it&#x27;s necessary when using tensor parallel. Defaults to None, which is the global process group.</li>
<li><strong>pipeline_stage_manager</strong> (Optional[PipelineStageManager]) -- If using pipeline parallelism, it&#x27;s necessary to specify a pipeline stage manager for inter-process communication in pipeline parallelism. Defaults to None, which means not using pipeline parallelism.</li>
<li><strong>enable_tensor_parallelism</strong> (bool) -- Whether to use tensor parallelism. Defaults to True.</li>
<li><strong>enable_fused_normalization</strong> (bool) -- Whether to use fused layernorm. Defaults to False.</li>
<li><strong>enable_flash_attention</strong> (bool, optional) -- Whether to switch on flash attention. Defaults to False.</li>
<li><strong>enable_jit_fused</strong> (bool, optional) -- Whether to switch on JIT fused operators. Defaults to False.</li>
<li><strong>enable_sequence_parallelism</strong> (bool) -- Whether to turn on sequence parallelism, which partitions non-tensor-parallel regions along the sequence dimension. Defaults to False.</li>
<li><strong>enable_sequence_overlap</strong> (bool) -- Whether to turn on sequence overlap, wheich overlap the computation and communication in sequence parallelism. It can only be used when enable_sequence_parallelism is True. Defaults to False.</li>
<li><strong>enable_all_optimization</strong> (bool) -- Whether to turn on all optimization tools including &#x27;fused normalizaion&#x27;, &#x27;flash attention&#x27;, &#x27;JIT fused operators&#x27;, &#x27;sequence parallelism&#x27; and &#x27;sequence overlap&#x27;. Defaults to False.</li>
<li><strong>inference_only</strong> (bool) -- Whether only doing forward passing. Defaults to False.</li>
</ul></div></div><div><div class="divider"><span class="divider-text">Description</span></div><p>The config for sharding the huggingface model</p></div></div><p>If you want to enable Apex Fused Layernorm, please install <code>apex</code>.
If you want to enable the usage of flash attention, please install <code>flash_attn</code>.
In addition, xFormers&#x27;s <code>cutlass_op</code> can serve as a backup for flash attention.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="enabling-shardformer">Enabling Shardformer<a href="#enabling-shardformer" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3><h4 class="anchor anchorWithStickyNavbar_LWe7" id="1-enabling-shardformer-through-booster-recommended">1. Enabling Shardformer Through Booster (Recommended)<a href="#1-enabling-shardformer-through-booster-recommended" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h4><p>Enabling <code>Shardformer</code> through <code>Booster</code> initialized with <code>HybridParallelPlugin</code> is the recommended way to awaken the power of Shardformer.
The main reason is that pipeline parallelism cannot successfully work without the calling of <code>execute_pipeline</code> method of <code>Booster</code>. Besides, <code>HybridParallelPlugin</code> provides the capacity to combine the features of <code>Shardformer</code> with other useful features, such as mixed precision training or Zero.</p><p>More details about this usage can be found in chapter <a href="/docs/basics/booster_api">Booster API</a> and <a href="/docs/basics/booster_plugins">Booster Plugins</a>.</p><p><a href="https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/bert" target="_blank" rel="noopener noreferrer">Here</a> is an example on how to trigger <code>Shardformer</code> through <code>HybridParallelPlugin</code>. Please be aware that there&#x27;s a difference in the way of doing forward and backward between the situation of using pipeline and not using pipeline.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="2-enabling-shardformer-through-shardformer-apis-not-recommended">2. Enabling Shardformer Through Shardformer APIs (Not Recommended)<a href="#2-enabling-shardformer-through-shardformer-apis-not-recommended" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h4><p>You can also use Shardformer through manually calling Shardformer APIs. However, this usage is not recommended since pipeline parallelism can&#x27;t run without <code>Booster</code>.</p><p><a href="https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/shardformer/examples/convergence_benchmark.py" target="_blank" rel="noopener noreferrer">Here</a>
is an example on how to trigger <code>Shardformer</code> through calling Shardformer APIs.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="precautions">Precautions<a href="#precautions" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3><ol><li><p>When enabling pipeline parallel, please don&#x27;t do the forward/backward pass in the conventional way (<code>model(input)</code>, <code>loss.backward()</code>), which will cause unexpected errors. Rather, please do forward/backward pass through calling <code>booster.execute_pipeline</code> method.</p></li><li><p>When you use Shardformer to process classification models such as <code>GPT2ForSequenceClassification</code>, <code>ViTForImageClassification</code>, please ensure that the total number of labels should be integer multiple of tensor parallel size, otherwise Shardformer can&#x27;t process the classifier layer correctly. A simple fix could be appending dummy labels in transformers config. This bug will be fixed in future version of Shardformer.</p></li><li><p>The case of training ChatGLM-2 6B is a little special: since Huggingface transformers doesn&#x27;t officially support ChatGLM at present, please import the configuration/model classes through</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> colossalai</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">shardformer</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">modeling</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">chatglm2_6b</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">configuration_chatglm </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> ChatGLMConfig</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> colossalai</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">shardformer</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">modeling</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">chatglm2_6b</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">modeling_chatglm </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> ChatGLMForConditionalGeneration</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> ChatGLMModel</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>when training ChatGLM-2 with Shardformer, and initialize your model with these imported classes.</p></li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-shardformer-works">How Shardformer Works<a href="#how-shardformer-works" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2><p>Generally, Shardformer works through the following four kinds of <em>replacements</em>:</p><ol><li><p>Replacing original PyTorch module (e.g. <code>nn.Linear</code>, <code>nn.Embedding</code>) with a crafted distributed module.
The distributed module keeps the same attributes as the original module but replaces the original parameters with distributed parameters.
Also, new <code>forward</code> methods will replace original ones so as to execute distributed computation, such as linear layers&#x27; split /gather operations executed under tensor parallelism.
Each distributed module implements its <code>from_native_module</code> static method to convert the PyTorch module to its corresponding distributed module.</p></li><li><p>Replacing attributes of original Huggingface Transformers layers with appropriate attributes for distributed training.
For example, when training LlaMa-2 with tensor parallel size as 2, the attribute <code>num_heads</code> of <code>LlamaDecoderLayer</code> (the number of attention heads in each layer) should be replaced with <code>model.config.num_attention_heads // 2</code>.</p></li><li><p>Replacing the <code>forward</code> methods implemented by original Huggingface
Transformers libraries with our customized <code>forward</code> methods.
This replacement is essential for pipeline paralellism, where a customiozed function is needed to pass intermediate hidden states between different pipeline stages.
Also, optimization methods such as flash attention or sequence parallel can be injected into the <code>forward</code> process through our customized <code>forward</code> method.</p></li><li><p>Replacing the whole copy of model parameters and optimizer states with incomplete ones controlled by current device (this is why it&#x27;s called Shardformer).
By executing <code>ModelSharder.shard</code> method, current device will only keep the part of model parameters it&#x27;s supposed to take care of.
To be specific, they should be the assigned parameter shards when using tensor parallelism, or the parameters belonging to current pipeline stage when using pipeline parallelism, or both of them.
All other parameters are released so as to liberate memory usage.
As a result, the optimizer will only compute the states corresponding to these part of parameters, causing the usage of memory to be further saved.</p></li></ol><p>All of these replacements are implemented with manually written policies and forward functions.
If you want to delve deeper into the design of Shardformer or customize your own Shardformer policies, please refer to our <a href="https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/shardformer/README.md" target="_blank" rel="noopener noreferrer">Shardformer development document</a> and <a href="https://github.com/hpcaitech/ColossalAI/discussions/4050" target="_blank" rel="noopener noreferrer">pipeline parallelism design</a> for more details.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="sequence-parallelism">Sequence Parallelism<a href="#sequence-parallelism" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3><p>Sequence parallelism is a special optimization method supported by <code>Shardformer</code>. Sequence parallelism in <code>Shardformer</code> is a little different from <a href="https://colossalai.org/docs/basics/configure_parallelization/#sequence-parallel" target="_blank" rel="noopener noreferrer">this one</a> which focuses on ring attention. In <code>Shardformer</code>, sequence parallelism is only used along with 1D tensor parallelism to further reduce memory occupation of activation tensors during computation.</p><ol><li><p>In normal <a href="https://colossalai.org/docs/features/1D_tensor_parallel" target="_blank" rel="noopener noreferrer">1D tensor parallel</a>, there are 2 communication operations, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi></mrow><annotation encoding="application/x-tex">g</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em"></span><span class="mord mathnormal" style="margin-right:0.03588em">g</span></span></span></span></span> and <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>g</mi><mo>⃗</mo></mover></mrow><annotation encoding="application/x-tex">\vec{g}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9084399999999999em;vertical-align:-0.19444em"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.714em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">g</span></span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.20772em"><span class="overlay" style="height:0.714em;width:0.471em"><svg width="0.471em" height="0.714em" style="width:0.471em" viewBox="0 0 471 714" preserveAspectRatio="xMinYMin"><path d="M377 20c0-5.333 1.833-10 5.5-14S391 0 397 0c4.667 0 8.667 1.667 12 5
3.333 2.667 6.667 9 10 19 6.667 24.667 20.333 43.667 41 57 7.333 4.667 11
10.667 11 18 0 6-1 10-3 12s-6.667 5-14 9c-28.667 14.667-53.667 35.667-75 63
-1.333 1.333-3.167 3.5-5.5 6.5s-4 4.833-5 5.5c-1 .667-2.5 1.333-4.5 2s-4.333 1
-7 1c-4.667 0-9.167-1.833-13.5-5.5S337 184 337 178c0-12.667 15.667-32.333 47-59
H213l-171-1c-8.667-6-13-12.333-13-19 0-4.667 4.333-11.333 13-20h359
c-16-25.333-24-45-24-59z"></path></svg></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em"><span></span></span></span></span></span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi></mrow><annotation encoding="application/x-tex">g</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em"></span><span class="mord mathnormal" style="margin-right:0.03588em">g</span></span></span></span></span> will do one time All-Reduce in backward to get all gradients from all the devices and <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>g</mi><mo>⃗</mo></mover></mrow><annotation encoding="application/x-tex">\vec{g}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9084399999999999em;vertical-align:-0.19444em"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.714em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">g</span></span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.20772em"><span class="overlay" style="height:0.714em;width:0.471em"><svg width="0.471em" height="0.714em" style="width:0.471em" viewBox="0 0 471 714" preserveAspectRatio="xMinYMin"><path d="M377 20c0-5.333 1.833-10 5.5-14S391 0 397 0c4.667 0 8.667 1.667 12 5
3.333 2.667 6.667 9 10 19 6.667 24.667 20.333 43.667 41 57 7.333 4.667 11
10.667 11 18 0 6-1 10-3 12s-6.667 5-14 9c-28.667 14.667-53.667 35.667-75 63
-1.333 1.333-3.167 3.5-5.5 6.5s-4 4.833-5 5.5c-1 .667-2.5 1.333-4.5 2s-4.333 1
-7 1c-4.667 0-9.167-1.833-13.5-5.5S337 184 337 178c0-12.667 15.667-32.333 47-59
H213l-171-1c-8.667-6-13-12.333-13-19 0-4.667 4.333-11.333 13-20h359
c-16-25.333-24-45-24-59z"></path></svg></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em"><span></span></span></span></span></span></span></span></span></span> will do one time All-Reduce in forward to get whole outputs from all the devices.</p></li><li><p>When using sequence parallelism, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>g</mi><mo>⃗</mo></mover></mrow><annotation encoding="application/x-tex">\vec{g}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9084399999999999em;vertical-align:-0.19444em"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.714em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">g</span></span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.20772em"><span class="overlay" style="height:0.714em;width:0.471em"><svg width="0.471em" height="0.714em" style="width:0.471em" viewBox="0 0 471 714" preserveAspectRatio="xMinYMin"><path d="M377 20c0-5.333 1.833-10 5.5-14S391 0 397 0c4.667 0 8.667 1.667 12 5
3.333 2.667 6.667 9 10 19 6.667 24.667 20.333 43.667 41 57 7.333 4.667 11
10.667 11 18 0 6-1 10-3 12s-6.667 5-14 9c-28.667 14.667-53.667 35.667-75 63
-1.333 1.333-3.167 3.5-5.5 6.5s-4 4.833-5 5.5c-1 .667-2.5 1.333-4.5 2s-4.333 1
-7 1c-4.667 0-9.167-1.833-13.5-5.5S337 184 337 178c0-12.667 15.667-32.333 47-59
H213l-171-1c-8.667-6-13-12.333-13-19 0-4.667 4.333-11.333 13-20h359
c-16-25.333-24-45-24-59z"></path></svg></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em"><span></span></span></span></span></span></span></span></span></span> needs to do All-Gather to gather the inputs along sequence dimension during forward, and Reduce-Scatter to split the gradient during backward. <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>g</mi><mo>⃗</mo></mover></mrow><annotation encoding="application/x-tex">\vec{g}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9084399999999999em;vertical-align:-0.19444em"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.714em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">g</span></span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.20772em"><span class="overlay" style="height:0.714em;width:0.471em"><svg width="0.471em" height="0.714em" style="width:0.471em" viewBox="0 0 471 714" preserveAspectRatio="xMinYMin"><path d="M377 20c0-5.333 1.833-10 5.5-14S391 0 397 0c4.667 0 8.667 1.667 12 5
3.333 2.667 6.667 9 10 19 6.667 24.667 20.333 43.667 41 57 7.333 4.667 11
10.667 11 18 0 6-1 10-3 12s-6.667 5-14 9c-28.667 14.667-53.667 35.667-75 63
-1.333 1.333-3.167 3.5-5.5 6.5s-4 4.833-5 5.5c-1 .667-2.5 1.333-4.5 2s-4.333 1
-7 1c-4.667 0-9.167-1.833-13.5-5.5S337 184 337 178c0-12.667 15.667-32.333 47-59
H213l-171-1c-8.667-6-13-12.333-13-19 0-4.667 4.333-11.333 13-20h359
c-16-25.333-24-45-24-59z"></path></svg></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em"><span></span></span></span></span></span></span></span></span></span> needs to do Reduce-Scatter to split the output of <code>Row Linear</code> layer of tensor parallel to all devices along sequence dimension, and All-Gather to get the whole gradient during backward.</p></li><li><p>NCCL&#x27;s implementation of All-Reduce adopts the <code>Ring All-Reduce</code> approach, which consists of a Reduce-Scatter operation and an All-Gather operation with equal costs. Therefore, compared with sequence parallelism and tensor parallelism, it does not introduce additional communication overhead.</p></li><li><p>One important thing to note is that when using sequence parallelism along with <code>Column Linear</code> module of tensor parallelism, the complete input needs to be obtained during the backward computation of gradients. During the forward pass, only the portion of the input that is split along the sequence dimension is retained, in the shape of <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi><mo separator="true">,</mo><mi>s</mi><mi>e</mi><mi>q</mi><mi>u</mi><mi>e</mi><mi>n</mi><mi>c</mi><msub><mi>e</mi><mi>l</mi></msub><mi>e</mi><mi>n</mi><mi mathvariant="normal">/</mi><mi>k</mi><mo separator="true">,</mo><mi>h</mi><mi>i</mi><mi>d</mi><mi>d</mi><mi>e</mi><msub><mi>n</mi><mi>s</mi></msub><mi>t</mi><mi>a</mi><mi>t</mi><mi>e</mi><mi>s</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(batch, sequence_len/k, hidden_states)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">(</span><span class="mord mathnormal">b</span><span class="mord mathnormal">a</span><span class="mord mathnormal">t</span><span class="mord mathnormal">c</span><span class="mord mathnormal">h</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em"></span><span class="mord mathnormal">s</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.03588em">q</span><span class="mord mathnormal">u</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord mathnormal">c</span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord">/</span><span class="mord mathnormal" style="margin-right:0.03148em">k</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em"></span><span class="mord mathnormal">h</span><span class="mord mathnormal">i</span><span class="mord mathnormal">d</span><span class="mord mathnormal">d</span><span class="mord mathnormal">e</span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord mathnormal">t</span><span class="mord mathnormal">a</span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mord mathnormal">s</span><span class="mclose">)</span></span></span></span></span>. Therefore, an additional All-Gather operation is required to obtain the complete input for gradient computation. However, it is possible to overlap the gradient computation with the All-Gather communication operation in our implementation, which would not introduce additional communication overhead (corresponding to the <code>enable_sequence_overlap</code> parameter in <code>Shardformer</code>).</p></li></ol></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/features/shardformer.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/basics/booster_checkpoint"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Booster Checkpoint</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/features/mixed_precision_training_with_booster"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Auto Mixed Precision Training</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#introduction" class="table-of-contents__link toc-highlight">Introduction</a></li><li><a href="#supporting-information" class="table-of-contents__link toc-highlight">Supporting Information</a></li><li><a href="#usage" class="table-of-contents__link toc-highlight">Usage</a><ul><li><a href="#shardformer-configuration" class="table-of-contents__link toc-highlight">Shardformer Configuration</a></li><li><a href="#enabling-shardformer" class="table-of-contents__link toc-highlight">Enabling Shardformer</a></li><li><a href="#precautions" class="table-of-contents__link toc-highlight">Precautions</a></li></ul></li><li><a href="#how-shardformer-works" class="table-of-contents__link toc-highlight">How Shardformer Works</a><ul><li><a href="#sequence-parallelism" class="table-of-contents__link toc-highlight">Sequence Parallelism</a></li></ul></li></ul></div></div></div></div></main></div></div><footer class="footer"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Resources</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/get_started/installation">Tutorials</a></li><li class="footer__item"><a href="https://github.com/hpcaitech/ColossalAI/tree/main/examples" target="_blank" rel="noopener noreferrer" class="footer__link-item">Examples</a></li><li class="footer__item"><a href="https://github.com/hpcaitech/ColossalAI/discussions" target="_blank" rel="noopener noreferrer" class="footer__link-item">Forum</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/hpcaitech/ColossalAI" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub</a></li><li class="footer__item"><a href="https://www.hpc-ai.tech/blog" target="_blank" rel="noopener noreferrer" class="footer__link-item">Blog</a></li><li class="footer__item"><a href="https://twitter.com/HPCAITech" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter</a></li></ul></div><div class="col footer__col"><div class="footer__title">About</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://www.hpc-ai.tech/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Company</a></li><li class="footer__item"><a href="https://www.hpc-ai.tech/services" target="_blank" rel="noopener noreferrer" class="footer__link-item">Services</a></li><li class="footer__item"><a href="https://www.hpc-ai.tech/customers" target="_blank" rel="noopener noreferrer" class="footer__link-item">Customers</a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2023 All Rights Reserved by HPC-AI Technology Inc.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.00d66150.js"></script>
<script src="/assets/js/main.b8498c50.js"></script>
</body>
</html>