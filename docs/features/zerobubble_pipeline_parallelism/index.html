<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-features/zerobubble_pipeline_parallelism">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.3.1">
<title data-rh="true">ZeroBubble Pipeline Parallelism | Colossal-AI</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://colossalai.org/img/social-card.png"><meta data-rh="true" name="twitter:image" content="https://colossalai.org/img/social-card.png"><meta data-rh="true" property="og:url" content="https://colossalai.org/docs/features/zerobubble_pipeline_parallelism"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="ZeroBubble Pipeline Parallelism | Colossal-AI"><meta data-rh="true" name="description" content="Author: Junwen Duan, Hongxin Liu"><meta data-rh="true" property="og:description" content="Author: Junwen Duan, Hongxin Liu"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://colossalai.org/docs/features/zerobubble_pipeline_parallelism"><link data-rh="true" rel="alternate" href="https://colossalai.org/docs/features/zerobubble_pipeline_parallelism" hreflang="en"><link data-rh="true" rel="alternate" href="https://colossalai.org/zh-Hans/docs/features/zerobubble_pipeline_parallelism" hreflang="zh-Hans"><link data-rh="true" rel="alternate" href="https://colossalai.org/docs/features/zerobubble_pipeline_parallelism" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://XP2V2KAOVI-dsn.algolia.net" crossorigin="anonymous"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Colossal-AI RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Colossal-AI Atom Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-1XKZVCCKRZ"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-1XKZVCCKRZ",{anonymize_ip:!0})</script>


<link rel="search" type="application/opensearchdescription+xml" title="Colossal-AI" href="/opensearch.xml">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous">
<script src="https://js-eu1.hs-scripts.com/26563514.js" id="hs-script-loader" async defer="defer"></script><link rel="stylesheet" href="/assets/css/styles.d9a9b3d5.css">
<link rel="preload" href="/assets/js/runtime~main.b702bfd8.js" as="script">
<link rel="preload" href="/assets/js/main.4a17101c.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="project-logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/logo.svg" alt="project-logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">Colossal-AI</b></a><a class="navbar__item navbar__link" href="/docs/get_started/installation">Tutorials</a><a href="https://github.com/hpcaitech/ColossalAI/tree/main/examples" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Examples</a><a href="https://www.hpc-ai.tech/blog" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Blogs</a></div><div class="navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/docs/get_started/installation">Next</a><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_nlXk"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>English</a><ul class="dropdown__menu"><li><a href="/docs/features/zerobubble_pipeline_parallelism" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="en">English</a></li><li><a href="/zh-Hans/docs/features/zerobubble_pipeline_parallelism" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-Hans">简体中文</a></li></ul></div><a href="https://github.com/hpcaitech/ColossalAI" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-github-link"></a><div class="searchBox_ZlJk"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><main class="docMainContainer_gTbr docMainContainerEnhanced_Uz_u"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>ZeroBubble Pipeline Parallelism</h1><p>Author: <a href="https://github.com/duanjunwen" target="_blank" rel="noopener noreferrer">Junwen Duan</a>, <a href="https://github.com/ver217" target="_blank" rel="noopener noreferrer">Hongxin Liu</a></p><p><strong>Related Paper</strong></p><ul><li><a href="https://arxiv.org/abs/2401.10241" target="_blank" rel="noopener noreferrer">Zero Bubble Pipeline Parallelism</a></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="introduction">Introduction<a href="#introduction" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2><p>ZeroBubble (V Schedule):
Crucially, splitting B into two stages (also known as an activation gradient and a weight gradient) and a scheme like 1F1B1W can further reduce the bubble compared to the 1F1B scheme in earlier work.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="hands-on-practice">Hands-On Practice<a href="#hands-on-practice" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2><p>We now demonstrate how to use ZeroBubble with booster API with 4 GPUs.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="step-1-import-libraries">step 1. Import libraries<a href="#step-1-import-libraries" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> torch</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> torch</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">distributed </span><span class="token keyword" style="color:#00009f">as</span><span class="token plain"> dist</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> torch</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">nn </span><span class="token keyword" style="color:#00009f">as</span><span class="token plain"> nn</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> torch</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">testing </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> assert_close</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> transformers</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">models</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">llama</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">configuration_llama </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> LlamaConfig</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> transformers</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">models</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">llama</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">modeling_llama </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> LlamaModel</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> colossalai</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> colossalai</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">booster</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">booster </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> Booster</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> colossalai</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">booster</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">plugin</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">moe_hybrid_parallel_plugin </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> HybridParallelPlugin</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> colossalai</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">pipeline</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">schedule</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">zero_bubble_pp </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> ZeroBubbleVPipeScheduler</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="step-2-initialize-distributed-environment-and-parallism-group">step 2. Initialize Distributed Environment and Parallism Group<a href="#step-2-initialize-distributed-environment-and-parallism-group" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">colossalai</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">launch</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">rank</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">rank</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> world_size</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">world_size</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> host</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">&quot;localhost&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> port</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">port</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> backend</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">&quot;nccl&quot;</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="step-3-initialize-module-optimizer-and-pipeline-schedule">step 3. Initialize Module, Optimizer, and Pipeline Schedule<a href="#step-3-initialize-module-optimizer-and-pipeline-schedule" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3><p>Build our model and Optimizer. We created a Llama with 8 Decoder-Layer. Then, inite the PipelineGraph and Pipeline schedule by get_v_schedule() function.</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># Global Param</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">NUM_BATCH </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">8</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">NUM_TOK_PER_BATCH </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">4</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">NUM_LAYERS </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">8</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">HIDDEN_SIZE_PER_HEAD </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">4</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">NUM_HEADS </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">4</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># Init Llama from huggingface</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">configuration </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> LlamaConfig</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    hidden_size</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">HIDDEN_SIZE_PER_HEAD </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> NUM_HEADS</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    intermediate_size</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">HIDDEN_SIZE_PER_HEAD </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> NUM_HEADS </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">2</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    num_hidden_layers</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">NUM_LAYERS</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    num_attention_heads</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">NUM_HEADS</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    num_key_value_heads</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">NUM_HEADS</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    attn_implementation</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">&quot;flash_attention_2&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">model </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> LlamaModel</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">configuration</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">cuda</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">optimizer </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> torch</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">optim</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">Adam</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">torch_model</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">parameters</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> lr</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="step-4-initialize-module-optimizer-and-pipeline-schedul">step 4. Initialize Module, Optimizer, and Pipeline Schedul<a href="#step-4-initialize-module-optimizer-and-pipeline-schedul" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3><p>Then, we need to create the PipelineGraph and PipelineSchedule using the get_v_schedule() function. We need to initialise the PipelineGraph with the following parameters.
x_cost represents the runtime consumed by operation x of each model chunk.
x_mem represents the amount of memory consumed by the operation x of each model chunk.
These parameters are estimated and filled in before the pipeline starts. In fact, better results can be obtained based on the runtime and memory cost during the real computation of the model.
In the following example, we assume that the computation times for the model&#x27;s forward, reverse B, and reverse W are 1, 1, 1, respectively, and the p2p communication time is 1.</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># Init schedule</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">h</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> a</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> s </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> config</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">hidden_size</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> config</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">num_attention_heads</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1024</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">mem_f </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">34</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> h </span><span class="token operator" style="color:#393A34">+</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">5</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> a </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> s</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">mem_w </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">-</span><span class="token number" style="color:#36acaa">32</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> h</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">mem_b </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">-</span><span class="token plain">mem_w </span><span class="token operator" style="color:#393A34">-</span><span class="token plain"> mem_f</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">graph </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> PipelineGraph</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    n_stage</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">pp_size</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    n_micro</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">num_microbatches</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    f_cost</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    b_cost</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    w_cost</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    c_cost</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    f_mem</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">mem_f</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    b_mem</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">mem_b</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    w_mem</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">mem_w</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">zbv_schedule </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> graph</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">get_v_schedule</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="step-5init-booster">step 5.Init Booster<a href="#step-5init-booster" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3><p>Pass pp_style=&quot;zbv&quot; when initialising the Plugin to use the ZeroBubble Pipeline.</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">plugin </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> HybridParallelPlugin</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    pp_size</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">4</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    num_microbatches</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">4</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    tp_size</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    sp_size</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    zero_stage</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    initial_scale</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    find_unused_parameters</span><span class="token operator" style="color:#393A34">=</span><span class="token boolean" style="color:#36acaa">True</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    pp_style</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">&quot;zbv&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    scheduler_nodes</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">zbv_schedule</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    num_model_chunks</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">2</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">dp_size </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> plugin</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">dp_size</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">booster </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> Booster</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">plugin</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">plugin</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="step-6train-your-model">step 6.Train Your Model<a href="#step-6train-your-model" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">steps </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">10</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> step </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> </span><span class="token builtin">range</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">steps</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    input_embeddings </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> torch</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">rand</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        NUM_BATCH</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> NUM_TOK_PER_BATCH</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> HIDDEN_SIZE_PER_HEAD </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> NUM_HEADS</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> requires_grad</span><span class="token operator" style="color:#393A34">=</span><span class="token boolean" style="color:#36acaa">True</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">cuda</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    dist</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">all_reduce</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        input_embeddings</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> group</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">plugin</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">pp_group</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    data_iter </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token builtin">iter</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">[</span><span class="token punctuation" style="color:#393A34">{</span><span class="token string" style="color:#e3116c">&quot;inputs_embeds&quot;</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> input_embeddings</span><span class="token punctuation" style="color:#393A34">}</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    output </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> booster</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">execute_pipeline</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        data_iter</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        model</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token keyword" style="color:#00009f">lambda</span><span class="token plain"> x</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> y</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> x</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">last_hidden_state</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">mean</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        optimizer</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        return_loss</span><span class="token operator" style="color:#393A34">=</span><span class="token boolean" style="color:#36acaa">True</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        return_outputs</span><span class="token operator" style="color:#393A34">=</span><span class="token boolean" style="color:#36acaa">True</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    optimizer</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">step</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    optimizer</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">zero_grad</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h2 class="anchor anchorWithStickyNavbar_LWe7" id="advanced-practice">Advanced Practice<a href="#advanced-practice" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2><p>In ColossalAI, you can get better training performance by using MetaCache and HybridParallel with ZeroBubble.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="1use-metacache-with-zerobubble">1.Use MetaCache with ZeroBubble<a href="#1use-metacache-with-zerobubble" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3><p>Pass &quot;enable_metadata_cache=True&quot; when initialising the Plugin to use the Meta Cache with ZeroBubble Pipeline.</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">plugin </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> HybridParallelPlugin</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    pp_size</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">2</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    num_microbatches</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">4</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    tp_size</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">2</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    sp_size</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">2</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    zero_stage</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    initial_scale</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    enable_metadata_cache</span><span class="token operator" style="color:#393A34">=</span><span class="token boolean" style="color:#36acaa">True</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    find_unused_parameters</span><span class="token operator" style="color:#393A34">=</span><span class="token boolean" style="color:#36acaa">True</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    pp_style</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">&quot;zbv&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    scheduler_nodes</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">zbv_schedule</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    num_model_chunks</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">2</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="2hybridparallel-with-zerobubble">2.HybridParallel with ZeroBubble<a href="#2hybridparallel-with-zerobubble" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3><p>Pass pp_size, tp_size, sp_size when initialising the Plugin to use the HybridParallel with ZeroBubble Pipeline.</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">plugin </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> HybridParallelPlugin</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    pp_size</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">2</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    num_microbatches</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">2</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    tp_size</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">2</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    sp_size</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">2</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    zero_stage</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    initial_scale</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    find_unused_parameters</span><span class="token operator" style="color:#393A34">=</span><span class="token boolean" style="color:#36acaa">True</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    pp_style</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">&quot;zbv&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    scheduler_nodes</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">zbv_schedule</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    num_model_chunks</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">2</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Performance Benchmark</p><table><tr><th nowrap="nowrap">HybridParallel Strategy</th><th nowrap="nowrap" align="center">Pipeline Parallel</th><th nowrap="nowrap" align="center">Sequence Parallel + Pipeline Parallel</th><th nowrap="nowrap" align="center">Data Parallel + Pipeline Parallel</th></tr><tr><td nowrap="nowrap" align="center" title="1F1B">With 1F1B</td><td nowrap="nowrap" align="center">15.27 samples/sec</td><td nowrap="nowrap" align="center">17.22 samples/sec</td><td nowrap="nowrap" align="center">14.06 samples/sec</td></tr><tr><td nowrap="nowrap" align="center" title="Zero Bubble">With Zero Bubble</td><td nowrap="nowrap" align="center">17.36 samples/sec</td><td nowrap="nowrap" align="center">18.38 samples/sec</td><td nowrap="nowrap" align="center">14.44 samples/sec</td></tr><tr><td colspan="39"></td></tr></table><h3 class="anchor anchorWithStickyNavbar_LWe7" id="3fine-tuning-scheduler-parameters">3.Fine-tuning Scheduler parameters<a href="#3fine-tuning-scheduler-parameters" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h2 class="anchor anchorWithStickyNavbar_LWe7" id="model-compatibility">Model compatibility<a href="#model-compatibility" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2><table><tr><th nowrap="nowrap">Shardformer/Model</th><th nowrap="nowrap" align="center">Bert</th><th nowrap="nowrap" align="center">Blip2</th><th nowrap="nowrap" align="center">Bloom</th><th nowrap="nowrap" align="center">Chatglm2</th><th nowrap="nowrap" align="center">Command</th><th nowrap="nowrap" align="center">Deepseek</th><th nowrap="nowrap" align="center">Falcon</th><th nowrap="nowrap" align="center">GPT2</th><th nowrap="nowrap" align="center">Gptj</th><th nowrap="nowrap" align="center">Llama</th><th nowrap="nowrap" align="center">Mistral</th><th nowrap="nowrap" align="center">Opt</th><th nowrap="nowrap" align="center">Qwen2</th><th nowrap="nowrap" align="center">Sam</th><th nowrap="nowrap" align="center">T5</th><th nowrap="nowrap" align="center">Vit</th><th nowrap="nowrap" align="center">Whisper</th></tr><tr><td nowrap="nowrap" align="center" title="ZeroBubble">ZeroBubble</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td></tr><tr><td colspan="39"></td></tr></table><h2 class="anchor anchorWithStickyNavbar_LWe7" id="api-reference">API Reference<a href="#api-reference" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2><div class="docstring-container"><div><div class="title-container"><div class="title-module"><h5>class</h5>  <h3>colossalai.pipeline.ZeroBubbleVPipeScheduler</h3></div><div class="title-source">&lt;<a href="https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/pipeline/schedule/zero_bubble_pp.py#L40" class="title-source">source</a>&gt;</div></div><div class="signature">(<!-- -->stage_manager: PipelineStageManager, schedule: typing.List[colossalai.pipeline.schedule.v_schedule.ScheduledNode], num_model_chunks: int, num_microbatch: typing.Optional[int] = None, microbatch_size: typing.Optional[int] = None, enable_metadata_cache: bool = True, overlap_p2p: bool = True<!-- -->)</div><div><div class="divider"><span class="divider-text">Parameters</span></div><ul>
<li><strong>stage_manager</strong> (PipelineStageManager) -- If using pipeline parallelism, it&#x27;s necessary to specify a pipeline stage manager for inter-process communication in pipeline parallelism. Defaults to None, which means not using pipeline parallelism.</li>
<li><strong>schedule</strong> (List[ScheduledNode]) -- Schedule for ZeroBubbleVPipe.</li>
<li><strong>num_model_chunks</strong> (int)  -- The number of model chunk in a device.</li>
<li><strong>num_microbatch</strong> (Optional[int]) -- The number of microbatch.</li>
<li><strong>microbatch_size</strong> (Optional[int]) -- The size per microbatch.</li>
<li><strong>enable_metadata_cache</strong> (bool) -- whether to enable metadata cache to acclerate communication.</li>
<li><strong>overlap_p2p</strong> (bool) -- whether to use overlap_p2p.</li>
</ul></div></div><div><div class="divider"><span class="divider-text">Description</span></div><p>ZeroBubbleVPipeScheduler</p></div><div class="docstring-container"><div><div class="title-container"><div class="title-module"><h5>function</h5>  <h3>backward_b_step</h3></div><div class="title-source">&lt;<a href="https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/pipeline/schedule/zero_bubble_pp.py#L516" class="title-source">source</a>&gt;</div></div><div class="signature">(<!-- -->model_chunk: typing.Union[torch.nn.modules.container.ModuleList, torch.nn.modules.module.Module], model_chunk_id: int, optimizer: OptimizerWrapper, input_obj: typing.Optional[dict], output_obj: typing.Union[dict, torch.Tensor], output_obj_grad: typing.Optional[dict]<!-- -->)</div><div><div class="divider"><span class="divider-text">Parameters</span></div><ul>
<li><strong>model_chunk</strong> (ModuleList or Module) -- Model Chunk to be run;</li>
<li><strong>model_chunk_id</strong> (int) -- The current model chunk idx;</li>
<li><strong>optimizer</strong> (OptimizerWrapper) -- Optimizer to update the model</li>
<li><strong>input_obj</strong> (Optional[Tuple(dict)]) -- x. (microbatch, input_obj)</li>
<li><strong>output_obj</strong> (Union[dict, torch.Tensor]) -- y.</li>
<li><strong>output_obj_grad</strong> (dict) -- dy.</li>
</ul></div><div><div class="divider"><span class="divider-text">Returns</span></div><p>Optional[dict]: dx.</p></div></div><div><div class="divider"><span class="divider-text">Description</span></div>Backward dx step of the pipeline; we calculate &quot;dx = w*dy&quot; here;</div></div><div class="docstring-container"><div><div class="title-container"><div class="title-module"><h5>function</h5>  <h3>backward_w_step</h3></div><div class="title-source">&lt;<a href="https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/pipeline/schedule/zero_bubble_pp.py#L591" class="title-source">source</a>&gt;</div></div><div class="signature">(<!-- -->model_chunk: typing.Union[torch.nn.modules.container.ModuleList, torch.nn.modules.module.Module], model_chunk_id: int, optimizer: OptimizerWrapper, output_obj: typing.Union[dict, torch.Tensor], output_obj_grad: typing.Optional[dict]<!-- -->)</div><div><div class="divider"><span class="divider-text">Parameters</span></div><ul>
<li><strong>model_chunk</strong> (ModuleList or Module) -- Model Chunk to be run;</li>
<li><strong>model_chunk_id</strong> (int) -- The current model chunk idx;</li>
<li><strong>optimizer</strong> (OptimizerWrapper) -- Optimizer to update the model</li>
<li><strong>output_obj</strong> (Union[dict, torch.Tensor]) -- y.</li>
<li><strong>output_obj_grad</strong> (dict) -- dy.</li>
</ul></div><div><div class="divider"><span class="divider-text">Returns</span></div><p>: Nothing need to return; we only calculate dw then update w;</p></div></div><div><div class="divider"><span class="divider-text">Description</span></div>Backward dw step of the pipeline; we calculate &quot;dw = x*dy&quot; here;</div></div><div class="docstring-container"><div><div class="title-container"><div class="title-module"><h5>function</h5>  <h3>forward_backward_step</h3></div><div class="title-source">&lt;<a href="https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/pipeline/schedule/zero_bubble_pp.py#L938" class="title-source">source</a>&gt;</div></div><div class="signature">(<!-- -->model_chunk: typing.Union[torch.nn.modules.container.ModuleList, torch.nn.modules.module.Module], data_iter: typing.Iterable, criterion: typing.Callable[..., typing.Any], optimizer: typing.Optional[colossalai.interface.optimizer.OptimizerWrapper] = None, return_loss: bool = False, return_outputs: bool = False<!-- -->)</div><div><div class="divider"><span class="divider-text">Parameters</span></div><ul>
<li><strong>model_chunk</strong> (ModuleList or Module) -- Model Chunk to be trained. Original interleaved uses a module list whereas shardformer uses entire model + layer specification</li>
<li><strong>data_iter</strong> (Iterable) -- Data iterator.</li>
<li><strong>criterion</strong> (Callable[[Any, Any], Tensor]) -- Criterion to be used. It should take two arguments: model outputs and inputs, and returns loss tensor.</li>
<li><strong>optimizer</strong> (OptimizerWrapper, optional) -- Optimizer to be used. Can be None when only forward is executed. Defaults to None.</li>
<li><strong>return_loss</strong> (bool, optional) -- Whether to return loss. Defaults to False. Whether to return loss.</li>
<li><strong>return_outputs</strong> (bool, optional) -- Whether to return model outputs. Defaults to False. Whether to return model outputs.</li>
</ul></div><div><div class="divider"><span class="divider-text">Returns</span></div><p>dict: A dict with keys: &#x27;loss&#x27; and &#x27;outputs&#x27;.</p></div></div><div><div class="divider"><span class="divider-text">Description</span></div></div></div><div class="docstring-container"><div><div class="title-container"><div class="title-module"><h5>function</h5>  <h3>forward_step</h3></div><div class="title-source">&lt;<a href="https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/pipeline/schedule/zero_bubble_pp.py#L474" class="title-source">source</a>&gt;</div></div><div class="signature">(<!-- -->model_chunk: typing.Union[torch.nn.modules.container.ModuleList, torch.nn.modules.module.Module], model_chunk_id: int, micro_batch: typing.Optional[dict], input_obj: typing.Optional[dict], criterion: typing.Callable, accum_loss: typing.Optional[torch.Tensor] = None, outputs: typing.Optional[typing.List[typing.Any]] = None<!-- -->)</div><div><div class="divider"><span class="divider-text">Parameters</span></div><ul>
<li><strong>model_chunk</strong> (ModuleList or Module) -- Model Chunk to be run;</li>
<li><strong>model_chunk_id</strong> (int) -- The current model chunk idx;</li>
<li><strong>input_obj</strong> (Optional[dict]) -- x;</li>
<li><strong>criterion</strong> (Callable) -- loss function;</li>
<li><strong>accum_loss</strong> (Optional[torch.Tensor], optional) -- Accumulated loss. Defaults to None.</li>
<li><strong>outputs</strong> (Optional[List[Any]], optional) -- List to store the output of the last stage (final output). Defaults to None.</li>
</ul></div><div><div class="divider"><span class="divider-text">Returns</span></div><p>Union[torch.Tensor, dict]: The intermediate output (dict) of the current stage. If it is the last stage, the output is the loss (Tensor).</p></div></div><div><div class="divider"><span class="divider-text">Description</span></div>Forward one step of the pipeline</div></div><div class="docstring-container"><div><div class="title-container"><div class="title-module"><h5>function</h5>  <h3>get_model_chunk_id</h3></div><div class="title-source">&lt;<a href="https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/pipeline/schedule/zero_bubble_pp.py#L219" class="title-source">source</a>&gt;</div></div><div class="signature">(<!-- -->microbatch_id: int, is_forward: bool<!-- -->)</div><div><div class="divider"><span class="divider-text">Parameters</span></div><ul>
<li><strong>microbatch_id</strong> (int) -- the current microbatch idx</li>
<li><strong>forward</strong> (bool) -- if is the forward process</li>
</ul></div><div><div class="divider"><span class="divider-text">Returns</span></div><p>int: The model chunk idx of the input microbatch_id</p></div></div><div><div class="divider"><span class="divider-text">Description</span></div>Helper method to get the model chunk ID given the iteration number.</div></div><div class="docstring-container"><div><div class="title-container"><div class="title-module"><h5>function</h5>  <h3>load_batch</h3></div><div class="title-source">&lt;<a href="https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/pipeline/schedule/zero_bubble_pp.py#L170" class="title-source">source</a>&gt;</div></div><div class="signature">(<!-- -->data_iter: typing.Iterable, device: typing.Optional[torch.device] = None<!-- -->)</div><div><div class="divider"><span class="divider-text">Parameters</span></div><ul>
<li><strong>data_iter</strong> (Iterable) -- Data iterator.</li>
<li><strong>device</strong> (Optional[torch.device], optional) -- Target device. Defaults to None.</li>
</ul></div></div><div><div class="divider"><span class="divider-text">Description</span></div>Load a batch from data iterator.</div></div><div class="docstring-container"><div><div class="title-container"><div class="title-module"><h5>function</h5>  <h3>load_micro_batch</h3></div><div class="title-source">&lt;<a href="https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/pipeline/schedule/zero_bubble_pp.py#L205" class="title-source">source</a>&gt;</div></div><div class="signature">(<!-- -->model_chunk_id: int<!-- -->)</div><div><div class="divider"><span class="divider-text">Parameters</span></div><ul>
<li><strong>microbatch_id</strong> (int) -- the current model chunk idx.</li>
</ul></div><div><div class="divider"><span class="divider-text">Returns</span></div><p>Any: Micro batch.</p></div></div><div><div class="divider"><span class="divider-text">Description</span></div>Load a micro batch from the current batch.</div></div><div class="docstring-container"><div><div class="title-container"><div class="title-module"><h5>function</h5>  <h3>recv_backward</h3></div><div class="title-source">&lt;<a href="https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/pipeline/schedule/zero_bubble_pp.py#L297" class="title-source">source</a>&gt;</div></div><div class="signature">(<!-- -->model_chunk_id: int, next_rank: int = None<!-- -->)</div><div><div class="divider"><span class="divider-text">Parameters</span></div><ul>
<li><strong>model_chunk_id</strong> (int) -- The current model chunk idx.</li>
<li><strong>next_rank</strong> (int, optional) -- The rank of the source of the tensor.</li>
</ul></div><div><div class="divider"><span class="divider-text">Returns</span></div><p>Any: The input gradient tensor or gradient tensor list.
Any: The wait handles for the communication.</p></div></div><div><div class="divider"><span class="divider-text">Description</span></div>Copy the gradient tensor from the next stage in pipeline as the input gradient of this stage. For ZBV.</div></div><div class="docstring-container"><div><div class="title-container"><div class="title-module"><h5>function</h5>  <h3>recv_forward</h3></div><div class="title-source">&lt;<a href="https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/pipeline/schedule/zero_bubble_pp.py#L239" class="title-source">source</a>&gt;</div></div><div class="signature">(<!-- -->model_chunk_id: int, prev_rank: int = None<!-- -->)</div><div><div class="divider"><span class="divider-text">Parameters</span></div><ul>
<li><strong>model_chunk_id</strong> (int) -- The current model chunk idx.</li>
<li><strong>prev_rank</strong> (int, optional) -- The rank of the source of the tensor.</li>
</ul></div><div><div class="divider"><span class="divider-text">Returns</span></div><p>Any: The input tensor or input tensor list.
Any: The wait handles for the communication.</p></div></div><div><div class="divider"><span class="divider-text">Description</span></div>Copy the forward output from the previous stage in pipeline as the input tensor of this stage. For ZBV.</div></div><div class="docstring-container"><div><div class="title-container"><div class="title-module"><h5>function</h5>  <h3>run_forward_backward</h3></div><div class="title-source">&lt;<a href="https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/pipeline/schedule/zero_bubble_pp.py#L871" class="title-source">source</a>&gt;</div></div><div class="signature">(<!-- -->model_chunk: typing.Union[torch.nn.modules.container.ModuleList, torch.nn.modules.module.Module], data_iter: typing.Iterable, criterion: typing.Callable[..., typing.Any], optimizer: typing.Optional[colossalai.interface.optimizer.OptimizerWrapper] = None, return_loss: bool = False, return_outputs: bool = False<!-- -->)</div></div><div><div class="divider"><span class="divider-text">Description</span></div><p>Runs Zerobubble schedule, with communication between pipeline stages.</p></div></div><div class="docstring-container"><div><div class="title-container"><div class="title-module"><h5>function</h5>  <h3>schedule_b</h3></div><div class="title-source">&lt;<a href="https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/pipeline/schedule/zero_bubble_pp.py#L741" class="title-source">source</a>&gt;</div></div><div class="signature">(<!-- -->scheduled_node, model_chunk: typing.Union[torch.nn.modules.container.ModuleList, torch.nn.modules.module.Module], model_chunk_id: int, optimizer: OptimizerWrapper<!-- -->)</div><div><div class="divider"><span class="divider-text">Parameters</span></div><p>scheduled_node --</p>
<ul>
<li><strong>model_chunk</strong> (ModuleList or Module) -- Model Chunk to be run;</li>
<li><strong>model_chunk_id</strong> (int) -- The current model chunk idx;</li>
</ul></div><div><div class="divider"><span class="divider-text">Returns</span></div><p>: Nothing.</p></div></div><div><div class="divider"><span class="divider-text">Description</span></div>A complete backward b schedule; Include recv bwd --&gt; cal bwd step --&gt; send bwd;</div></div><div class="docstring-container"><div><div class="title-container"><div class="title-module"><h5>function</h5>  <h3>schedule_f</h3></div><div class="title-source">&lt;<a href="https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/pipeline/schedule/zero_bubble_pp.py#L636" class="title-source">source</a>&gt;</div></div><div class="signature">(<!-- -->scheduled_node, model_chunk: ModuleList, model_chunk_id: int, criterion: typing.Callable, accum_loss: typing.Optional[torch.Tensor] = None, outputs: typing.Optional[typing.List[typing.Any]] = None<!-- -->)</div><div><div class="divider"><span class="divider-text">Parameters</span></div><p>scheduled_node --</p>
<ul>
<li><strong>model_chunk</strong> (ModuleList or Module) -- Model Chunk to be run;</li>
<li><strong>model_chunk_id</strong> (int) -- The current model chunk idx;</li>
<li><strong>criterion</strong> (Callable) -- loss function;</li>
<li><strong>accum_loss</strong> (Optional[torch.Tensor], optional) -- Accumulated loss. Defaults to None.</li>
<li><strong>outputs</strong> (Optional[List[Any]], optional) -- List to store the output of the last stage (final output). Defaults to None.</li>
</ul></div><div><div class="divider"><span class="divider-text">Returns</span></div><p>: Nothing.</p></div></div><div><div class="divider"><span class="divider-text">Description</span></div>A complete forward schedule; Include recv fwd --&gt; cal fwd --&gt; send fwd;</div></div><div class="docstring-container"><div><div class="title-container"><div class="title-module"><h5>function</h5>  <h3>schedule_w</h3></div><div class="title-source">&lt;<a href="https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/pipeline/schedule/zero_bubble_pp.py#L809" class="title-source">source</a>&gt;</div></div><div class="signature">(<!-- -->scheduled_node, model_chunk: typing.Union[torch.nn.modules.container.ModuleList, torch.nn.modules.module.Module], model_chunk_id: int, optimizer: OptimizerWrapper<!-- -->)</div><div><div class="divider"><span class="divider-text">Parameters</span></div><p>scheduled_node --</p>
<ul>
<li><strong>model_chunk</strong> (ModuleList or Module) -- Model Chunk to be run;</li>
<li><strong>model_chunk_id</strong> (int) -- The current model chunk idx;</li>
</ul></div><div><div class="divider"><span class="divider-text">Returns</span></div><p>: Nothing.</p></div></div><div><div class="divider"><span class="divider-text">Description</span></div>A complete backward w schedule; Include get y &amp; dy from buffer --&gt; cal bwd w step(cal dw &amp; update w);</div></div><div class="docstring-container"><div><div class="title-container"><div class="title-module"><h5>function</h5>  <h3>send_backward</h3></div><div class="title-source">&lt;<a href="https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/pipeline/schedule/zero_bubble_pp.py#L415" class="title-source">source</a>&gt;</div></div><div class="signature">(<!-- -->model_chunk_id: int, prev_rank: int = None<!-- -->)</div><div><div class="divider"><span class="divider-text">Parameters</span></div><ul>
<li><strong>model_chunk_id</strong> (int) -- The current model chunk idx.</li>
<li><strong>prev_rank</strong> (int, optional) -- The rank of the recipient of the tensor</li>
</ul></div><div><div class="divider"><span class="divider-text">Returns</span></div><p>Any: The wait handles for the communication.</p></div></div><div><div class="divider"><span class="divider-text">Description</span></div>Sends the gradient tensor to the previous stage in pipeline. For ZBV.</div></div><div class="docstring-container"><div><div class="title-container"><div class="title-module"><h5>function</h5>  <h3>send_forward</h3></div><div class="title-source">&lt;<a href="https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/pipeline/schedule/zero_bubble_pp.py#L356" class="title-source">source</a>&gt;</div></div><div class="signature">(<!-- -->model_chunk_id: int, next_rank: int = None<!-- -->)</div><div><div class="divider"><span class="divider-text">Parameters</span></div><ul>
<li><strong>model_chunk_id</strong> (int) -- The current model chunk idx.</li>
<li><strong>next_rank</strong> (int, optional) -- The rank of the recipient of the tensor.</li>
</ul></div><div><div class="divider"><span class="divider-text">Returns</span></div><p>Any: The wait handles for the communication.</p></div></div><div><div class="divider"><span class="divider-text">Description</span></div>Sends the input tensor to the next stage in pipeline. For ZBV.</div></div></div></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/features/zerobubble_pipeline_parallelism.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#introduction" class="table-of-contents__link toc-highlight">Introduction</a></li><li><a href="#hands-on-practice" class="table-of-contents__link toc-highlight">Hands-On Practice</a><ul><li><a href="#step-1-import-libraries" class="table-of-contents__link toc-highlight">step 1. Import libraries</a></li><li><a href="#step-2-initialize-distributed-environment-and-parallism-group" class="table-of-contents__link toc-highlight">step 2. Initialize Distributed Environment and Parallism Group</a></li><li><a href="#step-3-initialize-module-optimizer-and-pipeline-schedule" class="table-of-contents__link toc-highlight">step 3. Initialize Module, Optimizer, and Pipeline Schedule</a></li><li><a href="#step-4-initialize-module-optimizer-and-pipeline-schedul" class="table-of-contents__link toc-highlight">step 4. Initialize Module, Optimizer, and Pipeline Schedul</a></li><li><a href="#step-5init-booster" class="table-of-contents__link toc-highlight">step 5.Init Booster</a></li><li><a href="#step-6train-your-model" class="table-of-contents__link toc-highlight">step 6.Train Your Model</a></li></ul></li><li><a href="#advanced-practice" class="table-of-contents__link toc-highlight">Advanced Practice</a><ul><li><a href="#1use-metacache-with-zerobubble" class="table-of-contents__link toc-highlight">1.Use MetaCache with ZeroBubble</a></li><li><a href="#2hybridparallel-with-zerobubble" class="table-of-contents__link toc-highlight">2.HybridParallel with ZeroBubble</a></li><li><a href="#3fine-tuning-scheduler-parameters" class="table-of-contents__link toc-highlight">3.Fine-tuning Scheduler parameters</a></li></ul></li><li><a href="#model-compatibility" class="table-of-contents__link toc-highlight">Model compatibility</a></li><li><a href="#api-reference" class="table-of-contents__link toc-highlight">API Reference</a></li></ul></div></div></div></div></main></div></div><footer class="footer"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Resources</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/get_started/installation">Tutorials</a></li><li class="footer__item"><a href="https://github.com/hpcaitech/ColossalAI/tree/main/examples" target="_blank" rel="noopener noreferrer" class="footer__link-item">Examples</a></li><li class="footer__item"><a href="https://github.com/hpcaitech/ColossalAI/discussions" target="_blank" rel="noopener noreferrer" class="footer__link-item">Forum</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/hpcaitech/ColossalAI" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub</a></li><li class="footer__item"><a href="https://www.hpc-ai.tech/blog" target="_blank" rel="noopener noreferrer" class="footer__link-item">Blog</a></li><li class="footer__item"><a href="https://twitter.com/HPCAITech" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter</a></li></ul></div><div class="col footer__col"><div class="footer__title">About</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://www.hpc-ai.tech/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Company</a></li><li class="footer__item"><a href="https://www.hpc-ai.tech/services" target="_blank" rel="noopener noreferrer" class="footer__link-item">Services</a></li><li class="footer__item"><a href="https://www.hpc-ai.tech/customers" target="_blank" rel="noopener noreferrer" class="footer__link-item">Customers</a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 All Rights Reserved by HPC-AI Technology Inc.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.b702bfd8.js"></script>
<script src="/assets/js/main.4a17101c.js"></script>
</body>
</html>