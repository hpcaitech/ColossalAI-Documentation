<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-features/mixed_precision_training_with_booster">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.3.1">
<title data-rh="true">Auto Mixed Precision Training (Latest) | Colossal-AI</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://colossalai.org/img/social-card.png"><meta data-rh="true" name="twitter:image" content="https://colossalai.org/img/social-card.png"><meta data-rh="true" property="og:url" content="https://colossalai.org/docs/features/mixed_precision_training_with_booster"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Auto Mixed Precision Training (Latest) | Colossal-AI"><meta data-rh="true" name="description" content="Author: Mingyan Jiang"><meta data-rh="true" property="og:description" content="Author: Mingyan Jiang"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://colossalai.org/docs/features/mixed_precision_training_with_booster"><link data-rh="true" rel="alternate" href="https://colossalai.org/docs/features/mixed_precision_training_with_booster" hreflang="en"><link data-rh="true" rel="alternate" href="https://colossalai.org/zh-Hans/docs/features/mixed_precision_training_with_booster" hreflang="zh-Hans"><link data-rh="true" rel="alternate" href="https://colossalai.org/docs/features/mixed_precision_training_with_booster" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://XP2V2KAOVI-dsn.algolia.net" crossorigin="anonymous"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Colossal-AI RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Colossal-AI Atom Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-1XKZVCCKRZ"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-1XKZVCCKRZ",{anonymize_ip:!0})</script>


<link rel="search" type="application/opensearchdescription+xml" title="Colossal-AI" href="/opensearch.xml">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous">
<script src="https://js-eu1.hs-scripts.com/26563514.js" id="hs-script-loader" async defer="defer"></script><link rel="stylesheet" href="/assets/css/styles.d9a9b3d5.css">
<link rel="preload" href="/assets/js/runtime~main.2b5603a8.js" as="script">
<link rel="preload" href="/assets/js/main.ba26ab7e.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="project-logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/logo.svg" alt="project-logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">Colossal-AI</b></a><a class="navbar__item navbar__link" href="/docs/get_started/installation">Tutorials</a><a href="https://github.com/hpcaitech/ColossalAI/tree/main/examples" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Examples</a><a href="https://www.hpc-ai.tech/blog" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Blogs</a></div><div class="navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/docs/get_started/installation">Next</a><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_nlXk"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>English</a><ul class="dropdown__menu"><li><a href="/docs/features/mixed_precision_training_with_booster" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="en">English</a></li><li><a href="/zh-Hans/docs/features/mixed_precision_training_with_booster" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-Hans">简体中文</a></li></ul></div><a href="https://github.com/hpcaitech/ColossalAI" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-github-link"></a><div class="searchBox_ZlJk"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/get_started/installation">Get started</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/concepts/distributed_training">Concepts</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/basics/command_line_tool">Basics</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/docs/features/mixed_precision_training_with_booster">Features</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/features/mixed_precision_training_with_booster">Auto Mixed Precision Training (Latest)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/features/mixed_precision_training">Auto Mixed Precision Training (Outdated)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/features/gradient_accumulation_with_booster">Gradient Accumulation (Latest)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/features/gradient_accumulation">Gradient Accumulation (Outdated)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/features/gradient_clipping_with_booster">Gradient Clipping (Latest)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/features/gradient_clipping">Gradient Clipping (Outdated)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/features/gradient_handler">Gradient Handler</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/features/zero_with_chunk">Zero Redundancy Optimizer with chunk-based memory management</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/features/1D_tensor_parallel">Tensor Parallel</a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/features/pipeline_parallel">Pipeline Parallel</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/features/nvme_offload">NVMe offload</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/features/cluster_utils">Cluster Utilities</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/advanced_tutorials/train_vit_using_pipeline_parallelism">Advanced Tutorials</a></div></li></ul></nav></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Features</span><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Auto Mixed Precision Training (Latest)</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>Auto Mixed Precision Training (Latest)</h1><p>Author: <a href="https://github.com/jiangmingyan" target="_blank" rel="noopener noreferrer">Mingyan Jiang</a></p><p><strong>Prerequisite</strong></p><ul><li><a href="/docs/basics/define_your_config">Define Your Configuration</a></li><li><a href="/docs/basics/booster_api">Training Booster</a></li></ul><p><strong>Related Paper</strong></p><ul><li><a href="https://arxiv.org/abs/0808.2794" target="_blank" rel="noopener noreferrer">Accelerating Scientific Computations with Mixed Precision Algorithms</a></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="introduction">Introduction<a href="#introduction" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2><p>AMP stands for automatic mixed precision training.
In Colossal-AI, we have incorporated different implementations of mixed precision training:</p><ol><li>torch.cuda.amp</li><li>apex.amp</li><li>naive amp</li></ol><table><thead><tr><th>Colossal-AI</th><th>support tensor parallel</th><th>support pipeline parallel</th><th>fp16 extent</th></tr></thead><tbody><tr><td>AMP_TYPE.TORCH</td><td>✅</td><td>❌</td><td>Model parameters, activation, gradients are downcast to fp16 during forward and backward propagation</td></tr><tr><td>AMP_TYPE.APEX</td><td>❌</td><td>❌</td><td>More fine-grained, we can choose opt_level O0, O1, O2, O3</td></tr><tr><td>AMP_TYPE.NAIVE</td><td>✅</td><td>✅</td><td>Model parameters, forward and backward operations are all downcast to fp16</td></tr></tbody></table><p>The first two rely on the original implementation of PyTorch (version 1.6 and above) and NVIDIA Apex.
The last method is similar to Apex O2 level.
Among these methods, apex AMP is not compatible with tensor parallelism.
This is because that tensors are split across devices in tensor parallelism, thus, it is required to communicate among different processes to check if inf or nan occurs in the whole model weights.
We modified the torch amp implementation so that it is compatible with tensor parallelism now.</p><blockquote><p>❌️ fp16 and zero are not compatible</p><p>⚠️ Pipeline only support naive AMP currently</p></blockquote><p>We recommend you to use torch AMP as it generally gives better accuracy than naive AMP if no pipeline is used.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="table-of-contents">Table of Contents<a href="#table-of-contents" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2><p>In this tutorial we will cover:</p><ol><li><a href="#amp-introduction">AMP introduction</a></li><li><a href="#amp-in-colossal-ai">AMP in Colossal-AI</a></li><li><a href="#hands-on-practice">Hands-on Practice</a></li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="amp-introduction">AMP Introduction<a href="#amp-introduction" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2><p>Automatic Mixed Precision training is a mixture of FP16 and FP32 training.</p><p>Half-precision float point format (FP16) has lower arithmetic complexity and higher compute efficiency. Besides, fp16 requires half of the storage needed by fp32 and saves memory &amp; network bandwidth, which makes more memory available for large batch size and model size.</p><p>However, there are other operations, like reductions, which require the dynamic range of fp32 to avoid numeric overflow/underflow. That&#x27;s the reason why we introduce automatic mixed precision, attempting to match each operation to its appropriate data type, which can reduce the memory footprint and augment training efficiency.</p><figure style="text-align:center"><img loading="lazy" src="https://s2.loli.net/2022/01/28/URzLJ3MPeDQbtck.png" class="img_ev3q"><figcaption>Illustration of an ordinary AMP (figure from <a href="https://arxiv.org/abs/2108.05818" target="_blank" rel="noopener noreferrer">PatrickStar paper</a>)</figcaption></figure><h2 class="anchor anchorWithStickyNavbar_LWe7" id="amp-in-colossal-ai">AMP in Colossal-AI<a href="#amp-in-colossal-ai" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2><p>We supported three AMP training methods and allowed the user to train with AMP with no code. If you want to train with amp, just assign <code>mixed_precision</code> with <code>fp16</code> when you instantiate the <code>Booster</code>. Now booster support torch amp, the other two(apex amp, naive amp) are still started by <code>colossalai.initialize</code>, if needed, please refer to <a href="/docs/features/mixed_precision_training">this</a>. Next we will support <code>bf16</code>, <code>fp8</code>.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="start-with-booster">Start with Booster<a href="#start-with-booster" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3><p>instantiate <code>Booster</code> with <code>mixed_precision=&quot;fp16&quot;</code>, then you can train with torch amp.</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">&quot;&quot;&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">    Mapping:</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">    &#x27;fp16&#x27;: torch amp</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">    &#x27;fp16_apex&#x27;: apex amp,</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">    &#x27;bf16&#x27;: bf16,</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">    &#x27;fp8&#x27;: fp8,</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">    &#x27;fp16_naive&#x27;: naive amp</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">&quot;&quot;&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> colossalai </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> Booster</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">booster </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> Booster</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">mixed_precision</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">&#x27;fp16&#x27;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token punctuation" style="color:#393A34">.</span><span class="token punctuation" style="color:#393A34">.</span><span class="token punctuation" style="color:#393A34">.</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>or you can create a <code>FP16TorchMixedPrecision</code> object, such as:</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> colossalai</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">mixed_precision </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> FP16TorchMixedPrecision</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">mixed_precision </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> FP16TorchMixedPrecision</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    init_scale</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">2.</span><span class="token operator" style="color:#393A34">**</span><span class="token number" style="color:#36acaa">16</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    growth_factor</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">2.0</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    backoff_factor</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">0.5</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    growth_interval</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">2000</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">booster </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> Booster</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">mixed_precision</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">mixed_precision</span><span class="token punctuation" style="color:#393A34">,</span><span class="token punctuation" style="color:#393A34">.</span><span class="token punctuation" style="color:#393A34">.</span><span class="token punctuation" style="color:#393A34">.</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>The same goes for other types of amps.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="torch-amp-configuration">Torch AMP Configuration<a href="#torch-amp-configuration" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3><div class="docstring-container"><div><div class="title-container"><div class="title-module"><h5>class</h5>  <h3>colossalai.booster.mixed_precision.FP16TorchMixedPrecision</h3></div><div class="title-source">&lt;<a href="https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/booster/mixed_precision/fp16_torch.py#L89" class="title-source">source</a>&gt;</div></div><div class="signature">(<!-- -->init_scale: float = 65536.0, growth_factor: float = 2.0, backoff_factor: float = 0.5, growth_interval: int = 2000<!-- -->)</div><div><div class="divider"><span class="divider-text">Parameters</span></div><ul>
<li><strong>init_scale</strong> (float) -- Initial scale factor. Default: 2**16.</li>
<li><strong>growth_factor</strong> (float) -- Factor by which the scale is multiplied during
[<code>torch.cuda.amp.GradScaler.step</code>] if gradients were found to be finite
this iteration. Default: 2.0.</li>
<li><strong>backoff_factor</strong> (float) -- Factor by which the scale is multiplied during
[<code>torch.cuda.amp.GradScaler.step</code>] if gradients were found to be infinite
this iteration. Default: 0.5.</li>
<li><strong>growth_interval</strong> (int) -- Number of iterations between [<code>torch.cuda.amp.GradScaler.step</code>]
calls that may cause the scale to increase. Default: 2000.</li>
</ul></div></div><div><div class="divider"><span class="divider-text">Description</span></div><p>Precision for mixed precision training in FP16 using PyTorch AMP.</p></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="apex-amp-configuration">Apex AMP Configuration<a href="#apex-amp-configuration" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3><p>For this mode, we rely on the Apex implementation for mixed precision training.
We support this plugin because it allows for finer control on the granularity of mixed precision.
For example, O2 level (optimization level 2) will keep batch normalization in fp32.</p><p>If you look for more details, please refer to <a href="https://nvidia.github.io/apex/" target="_blank" rel="noopener noreferrer">Apex Documentation</a>.</p><div class="docstring-container"><div><div class="title-container"><div class="title-module"><h5>class</h5>  <h3>colossalai.booster.mixed_precision.FP16ApexMixedPrecision</h3></div><div class="title-source">&lt;<a href="https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/booster/mixed_precision/fp16_apex.py#L8" class="title-source">source</a>&gt;</div></div><div class="signature">(<!-- -->opt_level: typing.Optional[str] = &#x27;O1&#x27;, cast_model_type: dtype = None, patch_torch_functions: bool = None, keep_batchnorm_fp32: typing.Union[bool, str] = None, master_weights: bool = None, loss_scale: typing.Union[float, str] = None, cast_model_outputs: typing.Any = None, num_losses: typing.Optional[int] = 1, verbosity: int = 1, min_loss_scale: float = None, max_loss_scale: float = 16777216.0<!-- -->)</div><div><div class="divider"><span class="divider-text">Parameters</span></div><ul>
<li><strong>opt_level(str,</strong> optional, default=&quot;O1&quot; ) -- Pure or mixed precision optimization level. Accepted values are “O0”, “O1”, “O2”, and “O3”, explained in detail above Apex AMP Documentation.</li>
<li><strong>cast_model_type</strong> (torch.dtype, optional, default=None) -- Casts your model’s parameters and buffers to the desired type.</li>
<li><strong>patch_torch_functions</strong> (bool, optional, default=None) -- Patch all Torch functions and Tensor methods to perform Tensor Core-friendly ops like GEMMs and convolutions in FP16, and any ops that benefit from FP32 precision in FP32.</li>
<li><strong>keep_batchnorm_fp32</strong> (bool or str, optional, default=None) -- To enhance precision and enable cudnn batchnorm (which improves performance), it’s often beneficial to keep batchnorm weights in FP32 even if the rest of the model is FP16.</li>
<li><strong>master_weights</strong> (bool, optional, default=None) -- Maintain FP32 master weights to accompany any FP16 model weights. FP32 master weights are stepped by the optimizer to enhance precision and capture small gradients.</li>
<li><strong>loss_scale</strong> (float or str, optional, default=None) -- If loss_scale is a float value, use this value as the static (fixed) loss scale. If loss_scale is the string &quot;dynamic&quot;, adaptively adjust the loss scale over time. Dynamic loss scale adjustments are performed by Amp automatically.</li>
<li><strong>cast_model_outputs</strong> (torch.dpython --type, optional, default=None): Option to ensure that the outputs of your model(s) are always cast to a particular type regardless of opt_level.</li>
<li><strong>num_losses(int,</strong> optional, default=1) -- Option to tell AMP in advance how many losses/backward passes you plan to use. When used in conjunction with the loss_id argument to <code>amp.scale_loss</code>, enables Amp to use a different loss scale per loss/backward pass, which can improve stability. If num_losses is left to 1, Amp will still support multiple losses/backward passes, but use a single global loss scale for all of them.</li>
<li><strong>verbosity(int,</strong> default=1) -- Set to 0 to suppress Amp-related output.</li>
<li><strong>min_loss_scale(float,</strong> default=None) -- Sets a floor for the loss scale values that can be chosen by dynamic loss scaling. The default value of None means that no floor is imposed. If dynamic loss scaling is not used, min_loss_scale is ignored.</li>
<li><strong>max_loss_scale(float,</strong> default=2.**24 ) -- Sets a ceiling for the loss scale values that can be chosen by dynamic loss scaling. If dynamic loss scaling is not used, max_loss_scale is ignored.</li>
</ul></div></div><div><div class="divider"><span class="divider-text">Description</span></div><p>Precision for mixed precision training in FP16 using apex AMP.</p></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="naive-amp-configuration">Naive AMP Configuration<a href="#naive-amp-configuration" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3><p>In Naive AMP mode, we achieved mixed precision training while maintaining compatibility with complex tensor and pipeline parallelism.
This AMP mode will cast all operations into fp16.
The following code block shows the mixed precision api for this mode.</p><div class="docstring-container"><div><div class="title-container"><div class="title-module"><h5>class</h5>  <h3>colossalai.booster.mixed_precision.FP16NaiveMixedPrecision</h3></div><div class="title-source">&lt;<a href="https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/booster/mixed_precision/fp16_naive.py#L4" class="title-source">source</a>&gt;</div></div><div class="signature">(<!-- -->log_num_zeros_in_grad: bool, initial_scale: int, growth_factor: int, backoff_factor: float, hysteresis: int, max_scale: int, verbose: bool = None<!-- -->)</div><div><div class="divider"><span class="divider-text">Parameters</span></div><p>log_num_zeros_in_grad(bool) -- return number of zeros in the gradients.
initial_scale(int) -- initial scale of gradient scaler.
growth_factor(int) -- the growth rate of loss scale.
backoff_factor(float) -- the decrease rate of loss scale.
hysteresis(int) -- delay shift in dynamic loss scaling.
max_scale(int) -- maximum loss scale allowed.
verbose(bool) -- if set to <code>True</code>, will print debug info.</p></div></div><div><div class="divider"><span class="divider-text">Description</span></div><p>Precision for mixed precision training in FP16 using naive AMP.</p></div></div><p>When using <code>colossalai.booster</code>, you are required to first instantiate a model, an optimizer and a criterion.
The output model is converted to AMP model of smaller memory consumption.
If your input model is already too large to fit in a GPU, please instantiate your model weights in <code>dtype=torch.float16</code>.
Otherwise, try smaller models or checkout more parallelization training techniques!</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="hands-on-practice">Hands-on Practice<a href="#hands-on-practice" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2><p>Now we will introduce the use of AMP with Colossal-AI. In this practice, we will use Torch AMP as an example.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="step-1-import-libraries-in-trainpy">Step 1. Import libraries in train.py<a href="#step-1-import-libraries-in-trainpy" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3><p>Create a <code>train.py</code> and import the necessary dependencies. Remember to install <code>scipy</code> and <code>timm</code> by running
<code>pip install timm scipy</code>.</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> os</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> pathlib </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> Path</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> torch</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> timm</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">models </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> vit_base_patch16_224</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> titans</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">utils </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> barrier_context</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> torchvision </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> datasets</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> transforms</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> colossalai</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> colossalai</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">booster </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> Booster</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> colossalai</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">booster</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">plugin </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> TorchDDPPlugin</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> colossalai</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">logging </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> get_dist_logger</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> colossalai</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">nn</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">lr_scheduler </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> LinearWarmupLR</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="step-2-initialize-distributed-environment">Step 2. Initialize Distributed Environment<a href="#step-2-initialize-distributed-environment" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3><p>We then need to initialize distributed environment. For demo purpose, we uses <code>launch_from_torch</code>. You can refer to <a href="/docs/basics/launch_colossalai">Launch Colossal-AI</a>
for other initialization methods.</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># initialize distributed setting</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">parser </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> colossalai</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">get_default_parser</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">args </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> parser</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">parse_args</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># launch from torch</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">colossalai</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">launch_from_torch</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">config</span><span class="token operator" style="color:#393A34">=</span><span class="token builtin">dict</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="step-3-create-training-components">Step 3. Create training components<a href="#step-3-create-training-components" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3><p>Build your model, optimizer, loss function, lr scheduler and dataloaders. Note that the root path of the dataset is
obtained from the environment variable <code>DATA</code>. You may <code>export DATA=/path/to/data</code> or change <code>Path(os.environ[&#x27;DATA&#x27;])</code>
to a path on your machine. Data will be automatically downloaded to the root path.</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># define the constants</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">NUM_EPOCHS </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">2</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">BATCH_SIZE </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">128</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># build model</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">model </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> vit_base_patch16_224</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">drop_rate</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">0.1</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># build dataloader</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">train_dataset </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> datasets</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">Caltech101</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    root</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">Path</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">os</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">environ</span><span class="token punctuation" style="color:#393A34">[</span><span class="token string" style="color:#e3116c">&#x27;DATA&#x27;</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    download</span><span class="token operator" style="color:#393A34">=</span><span class="token boolean" style="color:#36acaa">True</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    transform</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">transforms</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">Compose</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        transforms</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">Resize</span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">256</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        transforms</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">RandomResizedCrop</span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">224</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        transforms</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">RandomHorizontalFlip</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        transforms</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">ToTensor</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        Gray2RGB</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        transforms</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">Normalize</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">[</span><span class="token number" style="color:#36acaa">0.5</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0.5</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0.5</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                </span><span class="token punctuation" style="color:#393A34">[</span><span class="token number" style="color:#36acaa">0.5</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0.5</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0.5</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># build optimizer</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">optimizer </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> torch</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">optim</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">SGD</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">model</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">parameters</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> lr</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">1e-2</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> weight_decay</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">0.1</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># build loss</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">criterion </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> torch</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">nn</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">CrossEntropyLoss</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># lr_scheduler</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">lr_scheduler </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> LinearWarmupLR</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">optimizer</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> warmup_steps</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">50</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> total_steps</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">NUM_EPOCHS</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="step-4-inject-amp-feature">Step 4. Inject AMP Feature<a href="#step-4-inject-amp-feature" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3><p>Create a <code>MixedPrecision</code>(if needed) and <code>TorchDDPPlugin</code> object, call <code>colossalai.boost</code> convert the training components to be running with FP16.</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">plugin </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> TorchDDPPlugin</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">train_dataloader </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> plugin</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">prepare_dataloader</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">train_dataset</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> batch_size</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">BATCH_SIZE</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> shuffle</span><span class="token operator" style="color:#393A34">=</span><span class="token boolean" style="color:#36acaa">True</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> drop_last</span><span class="token operator" style="color:#393A34">=</span><span class="token boolean" style="color:#36acaa">True</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">booster </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> Booster</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">mixed_precision</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">&#x27;fp16&#x27;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> plugin</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">plugin</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># if you need to customize the config, do like this</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># &gt;&gt;&gt; from colossalai.mixed_precision import FP16TorchMixedPrecision</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># &gt;&gt;&gt; mixed_precision = FP16TorchMixedPrecision(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># &gt;&gt;&gt;     init_scale=2.**16,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># &gt;&gt;&gt;     growth_factor=2.0,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># &gt;&gt;&gt;     backoff_factor=0.5,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># &gt;&gt;&gt;     growth_interval=2000)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># &gt;&gt;&gt; plugin = TorchDDPPlugin()</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># &gt;&gt;&gt; booster = Booster(mixed_precision=mixed_precision, plugin=plugin)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># boost model, optimizer, criterion, dataloader, lr_scheduler</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">model</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> optimizer</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> criterion</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> dataloader</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> lr_scheduler </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> booster</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">boost</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">model</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> optimizer</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> criterion</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> dataloader</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> lr_scheduler</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="step-5-train-with-booster">Step 5. Train with Booster<a href="#step-5-train-with-booster" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3><p>Use booster in a normal training loops.</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">model</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">train</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> epoch </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> </span><span class="token builtin">range</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">NUM_EPOCHS</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> img</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> label </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> </span><span class="token builtin">enumerate</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">train_dataloader</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        img </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> img</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">cuda</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        label </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> label</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">cuda</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        optimizer</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">zero_grad</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        output </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> model</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">img</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        loss </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> criterion</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">output</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> label</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        booster</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">backward</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">loss</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> optimizer</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        optimizer</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">step</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    lr_scheduler</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">step</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="step-6-invoke-training-scripts">Step 6. Invoke Training Scripts<a href="#step-6-invoke-training-scripts" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3><p>Use the following command to start the training scripts. You can change <code>--nproc_per_node</code> to use a different number of GPUs.</p><div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">colossalai run --nproc_per_node </span><span class="token number" style="color:#36acaa">1</span><span class="token plain"> train.py</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/features/mixed_precision_training_with_booster.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/basics/colotensor_concept"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">ColoTensor Concepts</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/features/mixed_precision_training"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Auto Mixed Precision Training (Outdated)</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#introduction" class="table-of-contents__link toc-highlight">Introduction</a></li><li><a href="#table-of-contents" class="table-of-contents__link toc-highlight">Table of Contents</a></li><li><a href="#amp-introduction" class="table-of-contents__link toc-highlight">AMP Introduction</a></li><li><a href="#amp-in-colossal-ai" class="table-of-contents__link toc-highlight">AMP in Colossal-AI</a><ul><li><a href="#start-with-booster" class="table-of-contents__link toc-highlight">Start with Booster</a></li><li><a href="#torch-amp-configuration" class="table-of-contents__link toc-highlight">Torch AMP Configuration</a></li><li><a href="#apex-amp-configuration" class="table-of-contents__link toc-highlight">Apex AMP Configuration</a></li><li><a href="#naive-amp-configuration" class="table-of-contents__link toc-highlight">Naive AMP Configuration</a></li></ul></li><li><a href="#hands-on-practice" class="table-of-contents__link toc-highlight">Hands-on Practice</a><ul><li><a href="#step-1-import-libraries-in-trainpy" class="table-of-contents__link toc-highlight">Step 1. Import libraries in train.py</a></li><li><a href="#step-2-initialize-distributed-environment" class="table-of-contents__link toc-highlight">Step 2. Initialize Distributed Environment</a></li><li><a href="#step-3-create-training-components" class="table-of-contents__link toc-highlight">Step 3. Create training components</a></li><li><a href="#step-4-inject-amp-feature" class="table-of-contents__link toc-highlight">Step 4. Inject AMP Feature</a></li><li><a href="#step-5-train-with-booster" class="table-of-contents__link toc-highlight">Step 5. Train with Booster</a></li><li><a href="#step-6-invoke-training-scripts" class="table-of-contents__link toc-highlight">Step 6. Invoke Training Scripts</a></li></ul></li></ul></div></div></div></div></main></div></div><footer class="footer"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Resources</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/get_started/installation">Tutorials</a></li><li class="footer__item"><a href="https://github.com/hpcaitech/ColossalAI/tree/main/examples" target="_blank" rel="noopener noreferrer" class="footer__link-item">Examples</a></li><li class="footer__item"><a href="https://github.com/hpcaitech/ColossalAI/discussions" target="_blank" rel="noopener noreferrer" class="footer__link-item">Forum</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/hpcaitech/ColossalAI" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub</a></li><li class="footer__item"><a href="https://www.hpc-ai.tech/blog" target="_blank" rel="noopener noreferrer" class="footer__link-item">Blog</a></li><li class="footer__item"><a href="https://twitter.com/HPCAITech" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter</a></li></ul></div><div class="col footer__col"><div class="footer__title">About</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://www.hpc-ai.tech/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Company</a></li><li class="footer__item"><a href="https://www.hpc-ai.tech/services" target="_blank" rel="noopener noreferrer" class="footer__link-item">Services</a></li><li class="footer__item"><a href="https://www.hpc-ai.tech/customers" target="_blank" rel="noopener noreferrer" class="footer__link-item">Customers</a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2023 All Rights Reserved by HPC-AI Technology Inc.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.2b5603a8.js"></script>
<script src="/assets/js/main.ba26ab7e.js"></script>
</body>
</html>