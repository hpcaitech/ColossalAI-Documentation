<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-concepts/paradigms_of_parallelism">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.3.1">
<title data-rh="true">Paradigms of Parallelism | Colossal-AI</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://colossalai.org/img/social-card.png"><meta data-rh="true" name="twitter:image" content="https://colossalai.org/img/social-card.png"><meta data-rh="true" property="og:url" content="https://colossalai.org/docs/concepts/paradigms_of_parallelism"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Paradigms of Parallelism | Colossal-AI"><meta data-rh="true" name="description" content="Author: Shenggui Li, Siqi Mai"><meta data-rh="true" property="og:description" content="Author: Shenggui Li, Siqi Mai"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://colossalai.org/docs/concepts/paradigms_of_parallelism"><link data-rh="true" rel="alternate" href="https://colossalai.org/docs/concepts/paradigms_of_parallelism" hreflang="en"><link data-rh="true" rel="alternate" href="https://colossalai.org/zh-Hans/docs/concepts/paradigms_of_parallelism" hreflang="zh-Hans"><link data-rh="true" rel="alternate" href="https://colossalai.org/docs/concepts/paradigms_of_parallelism" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://XP2V2KAOVI-dsn.algolia.net" crossorigin="anonymous"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Colossal-AI RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Colossal-AI Atom Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-1XKZVCCKRZ"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-1XKZVCCKRZ",{anonymize_ip:!0})</script>


<link rel="search" type="application/opensearchdescription+xml" title="Colossal-AI" href="/opensearch.xml">

<script src="https://js-eu1.hs-scripts.com/26563514.js" id="hs-script-loader" async defer="defer"></script><link rel="stylesheet" href="/assets/css/styles.d9a9b3d5.css">
<link rel="preload" href="/assets/js/runtime~main.291922f9.js" as="script">
<link rel="preload" href="/assets/js/main.5cfc44a3.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="project-logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/logo.svg" alt="project-logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">Colossal-AI</b></a><a class="navbar__item navbar__link" href="/docs/get_started/installation">Tutorials</a><a href="https://github.com/hpcaitech/ColossalAI/tree/main/examples" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Examples</a><a href="https://www.hpc-ai.tech/blog" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Blogs</a></div><div class="navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/docs/get_started/installation">Next</a><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_nlXk"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>English</a><ul class="dropdown__menu"><li><a href="/docs/concepts/paradigms_of_parallelism" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="en">English</a></li><li><a href="/zh-Hans/docs/concepts/paradigms_of_parallelism" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-Hans">简体中文</a></li></ul></div><a href="https://github.com/hpcaitech/ColossalAI" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-github-link"></a><div class="searchBox_ZlJk"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/get_started/installation">Get started</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/docs/concepts/distributed_training">Concepts</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/concepts/distributed_training">Distributed Training</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/concepts/paradigms_of_parallelism">Paradigms of Parallelism</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/concepts/colossalai_overview">Colossal-AI Overview</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/basics/command_line_tool">Basics</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/features/mixed_precision_training_with_booster">Features</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/advanced_tutorials/train_vit_using_pipeline_parallelism">Advanced Tutorials</a></div></li></ul></nav></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Concepts</span><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Paradigms of Parallelism</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>Paradigms of Parallelism</h1><p>Author: Shenggui Li, Siqi Mai</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="introduction">Introduction<a href="#introduction" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2><p>With the development of deep learning, there is an increasing demand for parallel training. This is because that model
and datasets are getting larger and larger and training time becomes a nightmare if we stick to single-GPU training. In
this section, we will provide a brief overview of existing methods to parallelize training. If you wish to add on to this
post, you may create a discussion in the <a href="https://github.com/hpcaitech/ColossalAI/discussions" target="_blank" rel="noopener noreferrer">GitHub forum</a>.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="data-parallel">Data Parallel<a href="#data-parallel" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2><p>Data parallel is the most common form of parallelism due to its simplicity. In data parallel training, the dataset is split
into several shards, each shard is allocated to a device. This is equivalent to parallelize the training process along the
batch dimension. Each device will hold a full copy of the model replica and trains on the dataset shard allocated. After
back-propagation, the gradients of the model will be all-reduced so that the model parameters on different devices can stay
synchronized.</p><figure style="text-align:center"><img loading="lazy" src="https://s2.loli.net/2022/01/28/WSAensMqjwHdOlR.png" class="img_ev3q"><figcaption>Data parallel illustration</figcaption></figure><h2 class="anchor anchorWithStickyNavbar_LWe7" id="model-parallel">Model Parallel<a href="#model-parallel" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2><p>In data parallel training, one prominent feature is that each GPU holds a copy of the whole model weights. This brings
redundancy issue. Another paradigm of parallelism is model parallelism, where model is split and distributed over an array
of devices. There are generally two types of parallelism: tensor parallelism and pipeline parallelism. Tensor parallelism is
to parallelize computation within an operation such as matrix-matrix multiplication. Pipeline parallelism is to parallelize
computation between layers. Thus, from another point of view, tensor parallelism can be seen as intra-layer parallelism and
pipeline parallelism can be seen as inter-layer parallelism.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="tensor-parallel">Tensor Parallel<a href="#tensor-parallel" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3><p>Tensor parallel training is to split a tensor into <code>N</code> chunks along a specific dimension and each device only holds <code>1/N</code>
of the whole tensor while not affecting the correctness of the computation graph. This requires additional communication
to make sure that the result is correct.</p><p>Taking a general matrix multiplication as an example, let&#x27;s say we have C = AB. We can split B along the column dimension
into <code>[B0 B1 B2 ... Bn]</code> and each device holds a column. We then multiply <code>A</code> with each column in <code>B</code> on each device, we
will get <code>[AB0 AB1 AB2 ... ABn]</code>. At this moment, each device still holds partial results, e.g. device rank 0 holds <code>AB0</code>.
To make sure the result is correct, we need to all-gather the partial result and concatenate the tensor along the column
dimension. In this way, we are able to distribute the tensor over devices while making sure the computation flow remains
correct.</p><figure style="text-align:center"><img loading="lazy" src="https://s2.loli.net/2022/01/28/2ZwyPDvXANW4tMG.png" class="img_ev3q"><figcaption>Tensor parallel illustration</figcaption></figure><p>In Colossal-AI, we provide an array of tensor parallelism methods, namely 1D, 2D, 2.5D and 3D tensor parallelism. We will
talk about them in detail in <code>advanced tutorials</code>.</p><p>Related paper:</p><ul><li><a href="https://arxiv.org/abs/2006.16668" target="_blank" rel="noopener noreferrer">GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding</a></li><li><a href="https://arxiv.org/abs/1909.08053" target="_blank" rel="noopener noreferrer">Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism</a></li><li><a href="https://arxiv.org/abs/2104.05343" target="_blank" rel="noopener noreferrer">An Efficient 2D Method for Training Super-Large Deep Learning Models</a></li><li><a href="https://arxiv.org/abs/2105.14500" target="_blank" rel="noopener noreferrer">2.5-dimensional distributed model training</a></li><li><a href="https://arxiv.org/abs/2105.14450" target="_blank" rel="noopener noreferrer">Maximizing Parallelism in Distributed Training for Huge Neural Networks</a></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="pipeline-parallel">Pipeline Parallel<a href="#pipeline-parallel" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3><p>Pipeline parallelism is generally easy to understand. If you recall your computer architecture course, this indeed exists
in the CPU design.</p><figure style="text-align:center"><img loading="lazy" src="https://s2.loli.net/2022/01/28/at3eDv7kKBusxbd.png" class="img_ev3q"><figcaption>Pipeline parallel illustration</figcaption></figure><p>The core idea of pipeline parallelism is that the model is split by layer into several chunks, each chunk is
given to a device. During the forward pass, each device passes the intermediate activation to the next stage. During the backward pass,
each device passes the gradient of the input tensor back to the previous pipeline stage. This allows devices to compute simultaneously,
and increases the training throughput. One drawback of pipeline parallel training is that there will be some bubble time where
some devices are engaged in computation, leading to waste of computational resources.</p><figure style="text-align:center"><img loading="lazy" src="https://s2.loli.net/2022/01/28/sDNq51PS3Gxbw7F.png" class="img_ev3q"><figcaption>Source: <a href="https://arxiv.org/abs/1811.06965" target="_blank" rel="noopener noreferrer">GPipe</a></figcaption></figure><p>Related paper:</p><ul><li><a href="https://arxiv.org/abs/1806.03377" target="_blank" rel="noopener noreferrer">PipeDream: Fast and Efficient Pipeline Parallel DNN Training</a></li><li><a href="https://arxiv.org/abs/1811.06965" target="_blank" rel="noopener noreferrer">GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism</a></li><li><a href="https://arxiv.org/abs/1909.08053" target="_blank" rel="noopener noreferrer">Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism</a></li><li><a href="https://arxiv.org/abs/2107.06925" target="_blank" rel="noopener noreferrer">Chimera: Efficiently Training Large-Scale Neural Networks with Bidirectional Pipelines</a></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="optimizer-level-parallel">Optimizer-Level Parallel<a href="#optimizer-level-parallel" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2><p>Another paradigm works at the optimizer level, and the current most famous method of this paradigm is ZeRO which stands
for <a href="https://arxiv.org/abs/1910.02054" target="_blank" rel="noopener noreferrer">zero redundancy optimizer</a>. ZeRO works at three levels to remove memory redundancy
(fp16 training is required for ZeRO):</p><ul><li>Level 1: The optimizer states are partitioned across the processes</li><li>Level 2: The reduced 32-bit gradients for updating the model weights are also partitioned such that each process
only stores the gradients corresponding to its partition of the optimizer states.</li><li>Level 3: The 16-bit model parameters are partitioned across the processes</li></ul><p>Related paper:</p><ul><li><a href="https://arxiv.org/abs/1910.02054" target="_blank" rel="noopener noreferrer">ZeRO: Memory Optimizations Toward Training Trillion Parameter Models</a></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="parallelism-on-heterogeneous-system">Parallelism on Heterogeneous System<a href="#parallelism-on-heterogeneous-system" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2><p>The methods mentioned above generally require a large number of GPU to train a large model. However, it is often neglected
that CPU has a much larger memory compared to GPU. On a typical server, CPU can easily have several hundred GB RAM while each GPU
typically only has 16 or 32 GB RAM. This prompts the community to think why CPU memory is not utilized for distributed training.</p><p>Recent advances rely on CPU and even NVMe disk to train large models. The main idea is to offload tensors back to CPU memory
or NVMe disk when they are not used. By using the heterogeneous system architecture, it is possible to accommodate a huge
model on a single machine.</p><figure style="text-align:center"><img loading="lazy" src="https://s2.loli.net/2022/01/28/qLHD5lk97hXQdbv.png" class="img_ev3q"><figcaption>Heterogenous system illustration</figcaption></figure><p>Related paper:</p><ul><li><a href="https://arxiv.org/abs/2101.06840" target="_blank" rel="noopener noreferrer">ZeRO-Offload: Democratizing Billion-Scale Model Training</a></li><li><a href="https://arxiv.org/abs/2104.07857" target="_blank" rel="noopener noreferrer">ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning</a></li><li><a href="https://arxiv.org/abs/2108.05818" target="_blank" rel="noopener noreferrer">PatrickStar: Parallel Training of Pre-trained Models via Chunk-based Memory Management</a></li></ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/concepts/paradigms_of_parallelism.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/concepts/distributed_training"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Distributed Training</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/concepts/colossalai_overview"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Colossal-AI Overview</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#introduction" class="table-of-contents__link toc-highlight">Introduction</a></li><li><a href="#data-parallel" class="table-of-contents__link toc-highlight">Data Parallel</a></li><li><a href="#model-parallel" class="table-of-contents__link toc-highlight">Model Parallel</a><ul><li><a href="#tensor-parallel" class="table-of-contents__link toc-highlight">Tensor Parallel</a></li><li><a href="#pipeline-parallel" class="table-of-contents__link toc-highlight">Pipeline Parallel</a></li></ul></li><li><a href="#optimizer-level-parallel" class="table-of-contents__link toc-highlight">Optimizer-Level Parallel</a></li><li><a href="#parallelism-on-heterogeneous-system" class="table-of-contents__link toc-highlight">Parallelism on Heterogeneous System</a></li></ul></div></div></div></div></main></div></div><footer class="footer"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Resources</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/get_started/installation">Tutorials</a></li><li class="footer__item"><a href="https://github.com/hpcaitech/ColossalAI/tree/main/examples" target="_blank" rel="noopener noreferrer" class="footer__link-item">Examples</a></li><li class="footer__item"><a href="https://github.com/hpcaitech/ColossalAI/discussions" target="_blank" rel="noopener noreferrer" class="footer__link-item">Forum</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/hpcaitech/ColossalAI" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub</a></li><li class="footer__item"><a href="https://www.hpc-ai.tech/blog" target="_blank" rel="noopener noreferrer" class="footer__link-item">Blog</a></li><li class="footer__item"><a href="https://twitter.com/HPCAITech" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter</a></li></ul></div><div class="col footer__col"><div class="footer__title">About</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://www.hpc-ai.tech/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Company</a></li><li class="footer__item"><a href="https://www.hpc-ai.tech/services" target="_blank" rel="noopener noreferrer" class="footer__link-item">Services</a></li><li class="footer__item"><a href="https://www.hpc-ai.tech/customers" target="_blank" rel="noopener noreferrer" class="footer__link-item">Customers</a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2023 All Rights Reserved by HPC-AI Technology Inc.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.291922f9.js"></script>
<script src="/assets/js/main.5cfc44a3.js"></script>
</body>
</html>