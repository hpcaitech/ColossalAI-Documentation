"use strict";(self.webpackChunkdemo=self.webpackChunkdemo||[]).push([[2567],{3905:(e,t,n)=>{n.d(t,{Zo:()=>u,kt:()=>h});var r=n(7294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function l(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function o(e,t){if(null==e)return{};var n,r,a=function(e,t){if(null==e)return{};var n,r,a={},i=Object.keys(e);for(r=0;r<i.length;r++)n=i[r],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)n=i[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var s=r.createContext({}),d=function(e){var t=r.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):l(l({},t),e)),n},u=function(e){var t=d(e.components);return r.createElement(s.Provider,{value:t},e.children)},p="mdxType",c={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},m=r.forwardRef((function(e,t){var n=e.components,a=e.mdxType,i=e.originalType,s=e.parentName,u=o(e,["components","mdxType","originalType","parentName"]),p=d(n),m=a,h=p["".concat(s,".").concat(m)]||p[m]||c[m]||i;return n?r.createElement(h,l(l({ref:t},u),{},{components:n})):r.createElement(h,l({ref:t},u))}));function h(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var i=n.length,l=new Array(i);l[0]=m;var o={};for(var s in t)hasOwnProperty.call(t,s)&&(o[s]=t[s]);o.originalType=e,o[p]="string"==typeof e?e:a,l[1]=o;for(var d=2;d<i;d++)l[d]=n[d];return r.createElement.apply(null,l)}return r.createElement.apply(null,n)}m.displayName="MDXCreateElement"},502:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>l,default:()=>c,frontMatter:()=>i,metadata:()=>o,toc:()=>d});var r=n(7462),a=(n(7294),n(3905));const i={},l="Gradient Handler",o={unversionedId:"features/gradient_handler",id:"version-v0.2.4/features/gradient_handler",title:"Gradient Handler",description:"Author: Shenggui Li, Yongbin Li",source:"@site/i18n/en/docusaurus-plugin-content-docs/version-v0.2.4/features/gradient_handler.md",sourceDirName:"features",slug:"/features/gradient_handler",permalink:"/docs/features/gradient_handler",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/versioned_docs/version-v0.2.4/features/gradient_handler.md",tags:[],version:"v0.2.4",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Gradient Clipping",permalink:"/docs/features/gradient_clipping"},next:{title:"Zero Redundancy Optimizer with chunk-based memory management",permalink:"/docs/features/zero_with_chunk"}},s={},d=[{value:"Introduction",id:"introduction",level:2},{value:"Customize Your Gradient Handlers",id:"customize-your-gradient-handlers",level:2},{value:"Usage",id:"usage",level:2},{value:"Hands-On Practice",id:"hands-on-practice",level:3}],u={toc:d},p="wrapper";function c(e){let{components:t,...n}=e;return(0,a.kt)(p,(0,r.Z)({},u,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"gradient-handler"},"Gradient Handler"),(0,a.kt)("p",null,"Author: Shenggui Li, Yongbin Li"),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"Prerequisite")),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"/docs/basics/define_your_config"},"Define Your Configuration")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"/docs/basics/engine_trainer"},"Use Engine and Trainer in Training"))),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"Example Code")),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"https://github.com/hpcaitech/ColossalAI-Examples/tree/main/features/gradient_handler"},"ColossalAI-Examples Gradient Handler"))),(0,a.kt)("h2",{id:"introduction"},"Introduction"),(0,a.kt)("p",null,"In distributed training, gradient synchronization is required at the end of each iteration. This is important because we\nneed to make sure the parameters are updated with the same gradients in different machines so that the resulting parameters\nare the same. This is often seen in data parallel as the model is replicated across data parallel ranks."),(0,a.kt)("p",null,"In Colossal-AI, we provide an interface for users to customize how they want to handle the synchronization. This brings\nflexibility in cases such as implementing a new parallelism method."),(0,a.kt)("p",null,"When gradient handlers are used, PyTorch ",(0,a.kt)("inlineCode",{parentName:"p"},"DistributedDataParallel")," will not be used as it will synchronize automatically."),(0,a.kt)("h2",{id:"customize-your-gradient-handlers"},"Customize Your Gradient Handlers"),(0,a.kt)("p",null,"To implement a customized gradient handler, you need to follow these steps."),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},"inherit ",(0,a.kt)("inlineCode",{parentName:"li"},"BaseGradientHandler")," in Colossal-AI."),(0,a.kt)("li",{parentName:"ol"},"register the gradient handler into the ",(0,a.kt)("inlineCode",{parentName:"li"},"GRADIENT_HANDLER"),"."),(0,a.kt)("li",{parentName:"ol"},"implement ",(0,a.kt)("inlineCode",{parentName:"li"},"handle_gradient")," method.")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"from colossalai.registry import GRADIENT_HANDLER\nfrom colossalai.engine.gradient_handler import BaseGradientHandler\n\n\n@GRADIENT_HANDLER.register_module\nclass MyGradientHandler(BaseGradientHandler):\n\n    def handle_gradient(self):\n        do_something()\n\n\n")),(0,a.kt)("h2",{id:"usage"},"Usage"),(0,a.kt)("p",null,"To use a gradient handler, you need to specify your gradient handler in the config file. The gradient handler\nwill be automatically built and attached to the engine."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"gradient_handler = [dict(type='MyGradientHandler')]\n")),(0,a.kt)("h3",{id:"hands-on-practice"},"Hands-On Practice"),(0,a.kt)("p",null,"We provide a ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/hpcaitech/ColossalAI-Examples/tree/main/features/gradient_handler"},"runnable example"),"\nto demonstrate the use of gradient handler. In this example, we used ",(0,a.kt)("inlineCode",{parentName:"p"},"DataParallelGradientHandler")," instead of PyTorch\n",(0,a.kt)("inlineCode",{parentName:"p"},"DistributedDataParallel")," for data parallel training."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell"},"python -m torch.distributed.launch --nproc_per_node 4 --master_addr localhost --master_port 29500  train_with_engine.py\n")))}c.isMDXComponent=!0}}]);