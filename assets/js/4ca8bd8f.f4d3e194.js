"use strict";(self.webpackChunkagile_docs=self.webpackChunkagile_docs||[]).push([[584],{3905:function(e,t,n){n.d(t,{Zo:function(){return c},kt:function(){return m}});var a=n(7294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function l(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?l(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):l(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function o(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},l=Object.keys(e);for(a=0;a<l.length;a++)n=l[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);for(a=0;a<l.length;a++)n=l[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var s=a.createContext({}),p=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},c=function(e){var t=p(e.components);return a.createElement(s.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},d=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,l=e.originalType,s=e.parentName,c=o(e,["components","mdxType","originalType","parentName"]),d=p(n),m=r,h=d["".concat(s,".").concat(m)]||d[m]||u[m]||l;return n?a.createElement(h,i(i({ref:t},c),{},{components:n})):a.createElement(h,i({ref:t},c))}));function m(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var l=n.length,i=new Array(l);i[0]=d;var o={};for(var s in t)hasOwnProperty.call(t,s)&&(o[s]=t[s]);o.originalType=e,o.mdxType="string"==typeof e?e:r,i[1]=o;for(var p=2;p<l;p++)i[p]=n[p];return a.createElement.apply(null,i)}return a.createElement.apply(null,n)}d.displayName="MDXCreateElement"},3469:function(e,t,n){n.r(t),n.d(t,{frontMatter:function(){return o},contentTitle:function(){return s},metadata:function(){return p},toc:function(){return c},default:function(){return d}});var a=n(3117),r=n(102),l=(n(7294),n(3905)),i=["components"],o={},s="Quick Demo",p={unversionedId:"get_started/run_demo",id:"version-v0.2.2/get_started/run_demo",title:"Quick Demo",description:"Colossal-AI is an integrated large-scale deep learning system with efficient parallelization techniques. The system can",source:"@site/i18n/en/docusaurus-plugin-content-docs/version-v0.2.2/get_started/run_demo.md",sourceDirName:"get_started",slug:"/get_started/run_demo",permalink:"/docs/get_started/run_demo",tags:[],version:"v0.2.2",frontMatter:{}},c=[{value:"Single GPU",id:"single-gpu",children:[],level:2},{value:"Multiple GPUs",id:"multiple-gpus",children:[{value:"1. data parallel",id:"1-data-parallel",children:[],level:4},{value:"2. hybrid parallel",id:"2-hybrid-parallel",children:[],level:4},{value:"3. MoE parallel",id:"3-moe-parallel",children:[],level:4},{value:"4. sequence parallel",id:"4-sequence-parallel",children:[],level:4}],level:2}],u={toc:c};function d(e){var t=e.components,n=(0,r.Z)(e,i);return(0,l.kt)("wrapper",(0,a.Z)({},u,n,{components:t,mdxType:"MDXLayout"}),(0,l.kt)("h1",{id:"quick-demo"},"Quick Demo"),(0,l.kt)("p",null,"Colossal-AI is an integrated large-scale deep learning system with efficient parallelization techniques. The system can\naccelerate model training on distributed systems with multiple GPUs by applying parallelization techniques. The system\ncan also run on systems with only one GPU. Quick demos showing how to use Colossal-AI are given below."),(0,l.kt)("h2",{id:"single-gpu"},"Single GPU"),(0,l.kt)("p",null,"Colossal-AI can be used to train deep learning models on systems with only one GPU and achieve baseline\nperformances. We provided an example to ",(0,l.kt)("a",{parentName:"p",href:"https://github.com/hpcaitech/ColossalAI-Examples/tree/main/image/resnet"},"train ResNet on CIFAR10 dataset"),"\nwith only one GPU. You can find the example in ",(0,l.kt)("a",{parentName:"p",href:"https://github.com/hpcaitech/ColossalAI-Examples"},"ColossalAI-Examples"),".\nDetailed instructions can be found in its ",(0,l.kt)("inlineCode",{parentName:"p"},"README.md"),"."),(0,l.kt)("h2",{id:"multiple-gpus"},"Multiple GPUs"),(0,l.kt)("p",null,"Colossal-AI can be used to train deep learning models on distributed systems with multiple GPUs and accelerate the\ntraining process drastically by applying efficient parallelization techniques. When we have several parallelism for you\nto try out."),(0,l.kt)("h4",{id:"1-data-parallel"},"1. data parallel"),(0,l.kt)("p",null,"You can use the same ",(0,l.kt)("a",{parentName:"p",href:"https://github.com/hpcaitech/ColossalAI-Examples/tree/main/image/resnet"},"ResNet example")," as the\nsingle-GPU demo above. By setting ",(0,l.kt)("inlineCode",{parentName:"p"},"--nproc_per_node")," to be the number of GPUs you have on your machine, the example\nis turned into a data parallel example."),(0,l.kt)("h4",{id:"2-hybrid-parallel"},"2. hybrid parallel"),(0,l.kt)("p",null,"Hybrid parallel includes data, tensor, and pipeline parallelism. In Colossal-AI, we support different types of tensor\nparallelism (i.e. 1D, 2D, 2.5D and 3D). You can switch between different tensor parallelism by simply changing the configuration\nin the ",(0,l.kt)("inlineCode",{parentName:"p"},"config.py"),". You can follow the ",(0,l.kt)("a",{parentName:"p",href:"https://github.com/hpcaitech/ColossalAI-Examples/tree/main/language/gpt"},"GPT example"),".\nDetailed instructions can be found in its ",(0,l.kt)("inlineCode",{parentName:"p"},"README.md"),"."),(0,l.kt)("h4",{id:"3-moe-parallel"},"3. MoE parallel"),(0,l.kt)("p",null,"We provided ",(0,l.kt)("a",{parentName:"p",href:"https://github.com/hpcaitech/ColossalAI-Examples/tree/main/image/widenet"},"an example of WideNet")," to demonstrate\nMoE parallelism. WideNet uses mixture of experts (MoE) to achieve better performance. More details can be found in\n",(0,l.kt)("a",{parentName:"p",href:"/docs/advanced_tutorials/integrate_mixture_of_experts_into_your_model"},"Tutorial: Integrate Mixture-of-Experts Into Your Model")),(0,l.kt)("h4",{id:"4-sequence-parallel"},"4. sequence parallel"),(0,l.kt)("p",null,"Sequence parallel is designed to tackle memory efficiency and sequence length limit problems in NLP tasks. We provided\n",(0,l.kt)("a",{parentName:"p",href:"https://github.com/hpcaitech/ColossalAI-Examples/tree/main/language/bert/sequene_parallel"},"an example of BERT")," in\n",(0,l.kt)("a",{parentName:"p",href:"https://github.com/hpcaitech/ColossalAI-Examples"},"ColossalAI-Examples"),". You can follow the ",(0,l.kt)("inlineCode",{parentName:"p"},"README.md")," to execute the code."))}d.isMDXComponent=!0}}]);