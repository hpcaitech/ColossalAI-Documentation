"use strict";(self.webpackChunkagile_docs=self.webpackChunkagile_docs||[]).push([[6194],{3905:function(e,n,r){r.d(n,{Zo:function(){return u},kt:function(){return m}});var a=r(7294);function t(e,n,r){return n in e?Object.defineProperty(e,n,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[n]=r,e}function i(e,n){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),r.push.apply(r,a)}return r}function l(e){for(var n=1;n<arguments.length;n++){var r=null!=arguments[n]?arguments[n]:{};n%2?i(Object(r),!0).forEach((function(n){t(e,n,r[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):i(Object(r)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(r,n))}))}return e}function o(e,n){if(null==e)return{};var r,a,t=function(e,n){if(null==e)return{};var r,a,t={},i=Object.keys(e);for(a=0;a<i.length;a++)r=i[a],n.indexOf(r)>=0||(t[r]=e[r]);return t}(e,n);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)r=i[a],n.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(t[r]=e[r])}return t}var s=a.createContext({}),d=function(e){var n=a.useContext(s),r=n;return e&&(r="function"==typeof e?e(n):l(l({},n),e)),r},u=function(e){var n=d(e.components);return a.createElement(s.Provider,{value:n},e.children)},p={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},c=a.forwardRef((function(e,n){var r=e.components,t=e.mdxType,i=e.originalType,s=e.parentName,u=o(e,["components","mdxType","originalType","parentName"]),c=d(r),m=t,f=c["".concat(s,".").concat(m)]||c[m]||p[m]||i;return r?a.createElement(f,l(l({ref:n},u),{},{components:r})):a.createElement(f,l({ref:n},u))}));function m(e,n){var r=arguments,t=n&&n.mdxType;if("string"==typeof e||t){var i=r.length,l=new Array(i);l[0]=c;var o={};for(var s in n)hasOwnProperty.call(n,s)&&(o[s]=n[s]);o.originalType=e,o.mdxType="string"==typeof e?e:t,l[1]=o;for(var d=2;d<i;d++)l[d]=r[d];return a.createElement.apply(null,l)}return a.createElement.apply(null,r)}c.displayName="MDXCreateElement"},3738:function(e,n,r){r.r(n),r.d(n,{frontMatter:function(){return o},contentTitle:function(){return s},metadata:function(){return d},toc:function(){return u},default:function(){return c}});var a=r(3117),t=r(102),i=(r(7294),r(3905)),l=["components"],o={},s="Add Your Own Parallel Mode",d={unversionedId:"advanced_tutorials/add_your_parallel",id:"version-v0.2.2/advanced_tutorials/add_your_parallel",title:"Add Your Own Parallel Mode",description:"Author: Shenggui Li, Yongbin Li",source:"@site/i18n/en/docusaurus-plugin-content-docs/version-v0.2.2/advanced_tutorials/add_your_parallel.md",sourceDirName:"advanced_tutorials",slug:"/advanced_tutorials/add_your_parallel",permalink:"/docs/advanced_tutorials/add_your_parallel",tags:[],version:"v0.2.2",frontMatter:{}},u=[{value:"Introduction",id:"introduction",children:[],level:2},{value:"Process Group Initializer",id:"process-group-initializer",children:[],level:2},{value:"Gradient Handler",id:"gradient-handler",children:[],level:2},{value:"Schedule",id:"schedule",children:[],level:2}],p={toc:u};function c(e){var n=e.components,r=(0,t.Z)(e,l);return(0,i.kt)("wrapper",(0,a.Z)({},p,r,{components:n,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"add-your-own-parallel-mode"},"Add Your Own Parallel Mode"),(0,i.kt)("p",null,"Author: Shenggui Li, Yongbin Li"),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Prerequisite:")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"/docs/basics/define_your_config"},"Define Your Configuration")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"/docs/basics/configure_parallelization"},"Configure Parallelization"))),(0,i.kt)("h2",{id:"introduction"},"Introduction"),(0,i.kt)("p",null,"To enable researchers and engineers to extend our system to other novel large-scale distributed training algorithm\nwith less effort, we have decoupled various components in the training lifecycle. You can implement your own\nparallelism by simply inheriting from the base class."),(0,i.kt)("p",null,"The main components are:"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("inlineCode",{parentName:"li"},"ProcessGroupInitializer")),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("inlineCode",{parentName:"li"},"GradientHandler")),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("inlineCode",{parentName:"li"},"Schedule"))),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"This currently requires some code to the source code, thus we recommend that you install from source with the ",(0,i.kt)("inlineCode",{parentName:"strong"},"-e")," flag.\n",(0,i.kt)("inlineCode",{parentName:"strong"},"-e")," flag makes the installation editable, thus, your code change will be reflected in your Python runtime.\nWe will work on this to avoid change to source code in future releases.")),(0,i.kt)("h2",{id:"process-group-initializer"},"Process Group Initializer"),(0,i.kt)("p",null,"Parallelism is often managed by process groups where processes involved in the same parallel algorithm are placed in the same\nprocess group. For different parallel algorithms, different process groups need to be created. Colossal-AI provides a\nglobal context for users to easily manage their process groups. If you wish to add new process group, you can easily\ndefine a new class and set it in your configuration file. To define your own way of creating process groups, you can\nfollow the steps below to create a new distributed initialization."),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"Add your parallel mode in ",(0,i.kt)("inlineCode",{parentName:"p"},"colossalai.context.parallel_mode.ParallelMode"),"."),(0,i.kt)("pre",{parentName:"li"},(0,i.kt)("code",{parentName:"pre",className:"language-python"},"class ParallelMode(Enum):\n    GLOBAL = 'global'\n    DATA = 'data'\n    PIPELINE = 'pipe'\n    ...\n\n    NEW_MODE = 'new_mode'  # define your mode here\n"))),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"Create a ",(0,i.kt)("inlineCode",{parentName:"p"},"ProcessGroupInitializer"),". You can refer to examples given in ",(0,i.kt)("inlineCode",{parentName:"p"},"colossalai.context.dist_group_initializer"),". The\nfirst six arguments are fixed. ",(0,i.kt)("inlineCode",{parentName:"p"},"ParallelContext")," will pass in these arguments for you. If you need to set other\narguments, you can add it behind like the ",(0,i.kt)("inlineCode",{parentName:"p"},"arg1, arg2")," in the example below. Lastly, register your initializer to the\nregistry by adding the decorator ",(0,i.kt)("inlineCode",{parentName:"p"},"@DIST_GROUP_INITIALIZER.register_module"),"."),(0,i.kt)("pre",{parentName:"li"},(0,i.kt)("code",{parentName:"pre",className:"language-python"},"# sample initializer class\n@DIST_GROUP_INITIALIZER.register_module\nclass MyParallelInitializer(ProcessGroupInitializer):\n\n    def __init__(self,\n                rank: int,\n                world_size: int,\n                config: Config,\n                data_parallel_size: int,\n                pipeline_parlalel_size: int,\n                tensor_parallel_size: int,\n                arg1,\n                arg2):\n        super().__init__(rank, world_size, config)\n        self.arg1 = arg1\n        self.arg2 = arg2\n        # ... your variable init\n\n    def init_parallel_groups(self):\n        # initialize your process groups\n        pass\n\n")),(0,i.kt)("p",{parentName:"li"}," Then, you can insert your new initializer to the current mode-to-initialize mapping\nin ",(0,i.kt)("inlineCode",{parentName:"p"},"colossalai.constants.INITIALIZER_MAPPING"),". You can modify the file or insert new key-value pair dynamically."),(0,i.kt)("pre",{parentName:"li"},(0,i.kt)("code",{parentName:"pre",className:"language-python"},"colossalai.constants.INITIALIZER_MAPPING['new_mode'] = 'MyParallelInitializer'\n"))),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"Set your initializer in your config file. You can pass in your own arguments if there is any. This allows\nthe ",(0,i.kt)("inlineCode",{parentName:"p"},"ParallelContext")," to create your initializer and initialize your desired process groups."),(0,i.kt)("pre",{parentName:"li"},(0,i.kt)("code",{parentName:"pre",className:"language-python"},"parallel = dict(\n    pipeline=dict(size=1),\n    tensor=dict(size=x, mode='new_mode')  # this is where you enable your new parallel mode\n)\n")))),(0,i.kt)("h2",{id:"gradient-handler"},"Gradient Handler"),(0,i.kt)("p",null,"Gradient handlers are objects which execute the all-reduce operations on parameters' gradients. As different all-reduce\nstrategies may be executed for different kinds of parallelism, users can\ninherit ",(0,i.kt)("inlineCode",{parentName:"p"},"colossalai.engine.gradient_handler.BaseGradientHandler")," to implement their strategies. Currently, the library\nuses the normal data parallel gradient handler which all-reduces the gradients across data parallel ranks. The data\nparallel gradient handler is added to the engine automatically if data parallel is detected. You can add your own\ngradient handler like below:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"from colossalai.registry import GRADIENT_HANDLER\nfrom colossalai.engine import BaseGradientHandler\n\n@GRADIENT_HANDLER.register_module\nclass YourGradientHandler(BaseGradientHandler):\n\n    def handle_gradient(self):\n        do_something()\n\n")),(0,i.kt)("p",null,"Afterwards, you can specify the gradient handler you want to use in your configuration file."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"gradient_handlers = [\n    dict(type='YourGradientHandler'),\n]\n")),(0,i.kt)("h2",{id:"schedule"},"Schedule"),(0,i.kt)("p",null,"Schedule entails how to execute a forward and backward pass. Currently, Colossal-AI provides pipeline and non-pipeline\nschedules. If you want to modify how the forward and backward passes are executed, you can\ninherit ",(0,i.kt)("inlineCode",{parentName:"p"},"colossalai.engine.schedule.BaseSchedule")," and implement the ",(0,i.kt)("inlineCode",{parentName:"p"},"forward_back_step")," function."))}c.isMDXComponent=!0}}]);