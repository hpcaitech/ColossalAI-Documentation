"use strict";(self.webpackChunkagile_docs=self.webpackChunkagile_docs||[]).push([[6198],{3905:function(e,t,n){n.d(t,{Zo:function(){return s},kt:function(){return m}});var r=n(7294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function c(e,t){if(null==e)return{};var n,r,a=function(e,t){if(null==e)return{};var n,r,a={},i=Object.keys(e);for(r=0;r<i.length;r++)n=i[r],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)n=i[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var u=r.createContext({}),l=function(e){var t=r.useContext(u),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},s=function(e){var t=l(e.components);return r.createElement(u.Provider,{value:t},e.children)},p={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},d=r.forwardRef((function(e,t){var n=e.components,a=e.mdxType,i=e.originalType,u=e.parentName,s=c(e,["components","mdxType","originalType","parentName"]),d=l(n),m=a,f=d["".concat(u,".").concat(m)]||d[m]||p[m]||i;return n?r.createElement(f,o(o({ref:t},s),{},{components:n})):r.createElement(f,o({ref:t},s))}));function m(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var i=n.length,o=new Array(i);o[0]=d;var c={};for(var u in t)hasOwnProperty.call(t,u)&&(c[u]=t[u]);c.originalType=e,c.mdxType="string"==typeof e?e:a,o[1]=c;for(var l=2;l<i;l++)o[l]=n[l];return r.createElement.apply(null,o)}return r.createElement.apply(null,n)}d.displayName="MDXCreateElement"},8404:function(e,t,n){n.r(t),n.d(t,{frontMatter:function(){return c},contentTitle:function(){return u},metadata:function(){return l},toc:function(){return s},default:function(){return d}});var r=n(3117),a=n(102),i=(n(7294),n(3905)),o=["components"],c={},u="Gradient Accumulation",l={unversionedId:"features/gradient_accumulation",id:"version-v0.2.2/features/gradient_accumulation",title:"Gradient Accumulation",description:"Author: Shenggui Li, Yongbin Li",source:"@site/i18n/en/docusaurus-plugin-content-docs/version-v0.2.2/features/gradient_accumulation.md",sourceDirName:"features",slug:"/features/gradient_accumulation",permalink:"/docs/features/gradient_accumulation",tags:[],version:"v0.2.2",frontMatter:{}},s=[{value:"Introduction",id:"introduction",children:[],level:2},{value:"Usage",id:"usage",children:[],level:2},{value:"Hands-on Practice",id:"hands-on-practice",children:[],level:2}],p={toc:s};function d(e){var t=e.components,n=(0,a.Z)(e,o);return(0,i.kt)("wrapper",(0,r.Z)({},p,n,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"gradient-accumulation"},"Gradient Accumulation"),(0,i.kt)("p",null,"Author: Shenggui Li, Yongbin Li"),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Prerequisite")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"/docs/basics/define_your_config"},"Define Your Configuration")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"/docs/basics/engine_trainer"},"Use Engine and Trainer in Training"))),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Example Code")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://github.com/hpcaitech/ColossalAI-Examples/tree/main/features/gradient_accumulation"},"ColossalAI-Examples Gradient Accumulation"))),(0,i.kt)("h2",{id:"introduction"},"Introduction"),(0,i.kt)("p",null,"Gradient accumulation is a common way to enlarge your batch size for training.\nWhen training large-scale models, memory can easily become the bottleneck and the batch size can be very small, (e.g. 2),\nleading to unsatisfactory convergence. Gradient accumulation works by adding up the gradients calculated in multiple iterations,\nand only update the parameters in the preset iteration."),(0,i.kt)("h2",{id:"usage"},"Usage"),(0,i.kt)("p",null,"It is simple to use gradient accumulation in Colossal-AI. Just add this following configuration into your config file.\nThe integer represents the number of iterations to accumulate gradients."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"gradient_accumulation = <int>\n")),(0,i.kt)("h2",{id:"hands-on-practice"},"Hands-on Practice"),(0,i.kt)("p",null,"We provide a ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/hpcaitech/ColossalAI-Examples/tree/main/features/gradient_accumulation"},"runnable example"),"\nto demonstrate gradient accumulation. In this example, we set the gradinet accumulation size to be 4. You can run the script using this command:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-shell"},"python -m torch.distributed.launch --nproc_per_node 1 --master_addr localhost --master_port 29500  run_resnet_cifar10_with_engine.py\n")),(0,i.kt)("p",null,"You will see output similar to the text below. This shows gradient is indeed accumulated as the parameter is not updated\nin the first 3 steps, but only updated in the last step."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-text"},"iteration 0, first 10 elements of param: tensor([-0.0208,  0.0189,  0.0234,  0.0047,  0.0116, -0.0283,  0.0071, -0.0359, -0.0267, -0.0006], device='cuda:0', grad_fn=<SliceBackward0>)\niteration 1, first 10 elements of param: tensor([-0.0208,  0.0189,  0.0234,  0.0047,  0.0116, -0.0283,  0.0071, -0.0359, -0.0267, -0.0006], device='cuda:0', grad_fn=<SliceBackward0>)\niteration 2, first 10 elements of param: tensor([-0.0208,  0.0189,  0.0234,  0.0047,  0.0116, -0.0283,  0.0071, -0.0359, -0.0267, -0.0006], device='cuda:0', grad_fn=<SliceBackward0>)\niteration 3, first 10 elements of param: tensor([-0.0141,  0.0464,  0.0507,  0.0321,  0.0356, -0.0150,  0.0172, -0.0118, 0.0222,  0.0473], device='cuda:0', grad_fn=<SliceBackward0>)\n")))}d.isMDXComponent=!0}}]);