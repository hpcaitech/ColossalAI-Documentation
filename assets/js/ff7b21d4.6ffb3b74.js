"use strict";(self.webpackChunkdemo=self.webpackChunkdemo||[]).push([[8332],{6999:(e,t,a)=>{a.d(t,{Cl:()=>n,Dx:()=>d,Pc:()=>o,aE:()=>l,e_:()=>u,iz:()=>s,nT:()=>p});var i=a(7294),r=a(398);a(814);function n(e){return i.createElement("div",{className:"docstring-container"},e.children)}function o(e){return i.createElement("div",{className:"signature"},"(",e.children,")")}function s(e){return i.createElement("div",{class:"divider"},i.createElement("span",{class:"divider-text"},e.name))}function l(e){return i.createElement("div",null,i.createElement(s,{name:"Parameters"}),i.createElement(r.D,null,e.children))}function p(e){return i.createElement("div",null,i.createElement(s,{name:"Returns"}),i.createElement(r.D,null,`${e.name}: ${e.desc}`))}function d(e){return i.createElement("div",{className:"title-container"},i.createElement("div",{className:"title-module"},i.createElement("h5",null,e.type),"\xa0 ",i.createElement("h3",null,e.name)),i.createElement("div",{className:"title-source"},"<",i.createElement("a",{href:e.source,className:"title-source"},"source"),">"))}function u(e){return i.createElement("div",null,i.createElement(s,{name:"Example"}),i.createElement(r.D,null,e.code))}},9530:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>p,contentTitle:()=>s,default:()=>c,frontMatter:()=>o,metadata:()=>l,toc:()=>d});var i=a(7462),r=(a(7294),a(3905)),n=a(6999);const o={},s="Distributed Optimizers",l={unversionedId:"features/distributed_optimizers",id:"features/distributed_optimizers",title:"Distributed Optimizers",description:"Author: Wenxuan Tan, Junwen Duan, Renjie Mao",source:"@site/i18n/en/docusaurus-plugin-content-docs/current/features/distributed_optimizers.md",sourceDirName:"features",slug:"/features/distributed_optimizers",permalink:"/docs/features/distributed_optimizers",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/features/distributed_optimizers.md",tags:[],version:"current",frontMatter:{}},p={},d=[{value:"Introduction",id:"introduction",level:2},{value:"Optimizers",id:"optimizers",level:2},{value:"API Reference",id:"api-reference",level:2},{value:"Hands-On Practice",id:"hands-on-practice",level:2},{value:"step 1. Import libraries",id:"step-1-import-libraries",level:3},{value:"step 2. Initialize Distributed Environment and Parallism Group",id:"step-2-initialize-distributed-environment-and-parallism-group",level:3},{value:"step 3. Initialize Module and Optimizer",id:"step-3-initialize-module-and-optimizer",level:3},{value:"step 4.Init Booster",id:"step-4init-booster",level:3},{value:"step 5.Train Your Model",id:"step-5train-your-model",level:3},{value:"GaLore special handling",id:"galore-special-handling",level:3},{value:"Plugin compatibility",id:"plugin-compatibility",level:2}],u={toc:d},m="wrapper";function c(e){let{components:t,...a}=e;return(0,r.kt)(m,(0,i.Z)({},u,a,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"distributed-optimizers"},"Distributed Optimizers"),(0,r.kt)("p",null,"Author: ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/Edenzzzz"},"Wenxuan Tan"),", ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/duanjunwen"},"Junwen Duan"),", ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/chongqichuizi875"},"Renjie Mao")),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Related Paper")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/1804.04235"},"Adafactor: Adaptive Learning Rates with Sublinear Memory Cost")),(0,r.kt)("li",{parentName:"ul"},"[CAME: Confidence-guided Adaptive Memory Efficient Optimization]"," (",(0,r.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2307.02047"},"https://arxiv.org/abs/2307.02047"),")"),(0,r.kt)("li",{parentName:"ul"},"[GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection]"," (",(0,r.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2403.03507"},"https://arxiv.org/abs/2403.03507"),")"),(0,r.kt)("li",{parentName:"ul"},"[Large Batch Optimization for Deep Learning: Training BERT in 76 minutes]"," (",(0,r.kt)("a",{parentName:"li",href:"https://arxiv.org/pdf/1904.00962"},"https://arxiv.org/pdf/1904.00962"),")")),(0,r.kt)("h2",{id:"introduction"},"Introduction"),(0,r.kt)("p",null,"Apart from the widely adopted Adam and SGD, many modern optimizers require layer-wise statistics to efficiently update parameters, and are thus not directly applicable to parallel settings where model layers are sharded across multiple devices. We provide optimized distributed implementations with minimal extra communications, and seamless integrations with Tensor Parallel, DDP and ZeRO using plugins."),(0,r.kt)("h2",{id:"optimizers"},"Optimizers"),(0,r.kt)("p",null,"Adafactor is a first-order Adam variant using Non-negative Matrix Factorization(NMF) to reduce memory footprint. CAME improves by introducting a confidence matrix to correct NMF. GaLore further reduces memory by projecting gradients into a low-rank space and 8-bit block-wise quantization. Lamb allows huge batch sizes without lossing accuracy via layer-wise adaptive update bounded by the inverse of its Lipschiz constant."),(0,r.kt)("h2",{id:"api-reference"},"API Reference"),(0,r.kt)(n.Cl,{mdxType:"DocStringContainer"},(0,r.kt)("div",null,(0,r.kt)(n.Dx,{type:"class",name:"colossalai.nn.DistributedAdaFactor",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/nn/optimizer/distributed_adafactor.py#L15",mdxType:"Title"}),(0,r.kt)(n.Pc,{mdxType:"Signature"},"params, lr = None, eps = (1e-30, 0.001), clip_threshold = 1.0, decay_rate = -0.8, beta1 = None, weight_decay = 0.0, scale_parameter = True, relative_step = True, warmup_init = False")),(0,r.kt)("div",null,(0,r.kt)(n.iz,{name:"Description",mdxType:"Divider"})),(0,r.kt)(n.Cl,{mdxType:"DocStringContainer"},(0,r.kt)("div",null,(0,r.kt)(n.Dx,{type:"function",name:"setup_distributed",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/nn/optimizer/distributed_adafactor.py#L60",mdxType:"Title"}),(0,r.kt)(n.Pc,{mdxType:"Signature"},"tp_group: ProcessGroup = None, dp_group: ProcessGroup = None, shard_to_working_param: typing.Dict = {}, padding_map = None, use_zero: bool = True"),(0,r.kt)(n.aE,{mdxType:"Parameters"},"tp_group -- The devices group for tensor parallel;\ndp_group -- The devices group for data parallel;\n- **shard_to_working_param** (Dict) -- ZeRO 2 feeds the optimizer a sharded param view as grads are sharded.\n  This maps from id(view) to working params used in forward & backward.\n  padding_map -- An empty interface placeholder;\n  use_zero -- Whether or not to use zero;")),(0,r.kt)("div",null,(0,r.kt)(n.iz,{name:"Description",mdxType:"Divider"}),"Setup process groups for TP and ZeRO 2. Inject features to the Optimizer")),(0,r.kt)(n.Cl,{mdxType:"DocStringContainer"},(0,r.kt)("div",null,(0,r.kt)(n.Dx,{type:"function",name:"step",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/nn/optimizer/distributed_adafactor.py#L285",mdxType:"Title"}),(0,r.kt)(n.Pc,{mdxType:"Signature"},"closure = None"),(0,r.kt)(n.aE,{mdxType:"Parameters"},"- **closure** (callable, optional) -- A closure that reevaluates the model\n  and returns the loss.")),(0,r.kt)("div",null,(0,r.kt)(n.iz,{name:"Description",mdxType:"Divider"}),(0,r.kt)("p",null,"Performs a single optimization steps")))),(0,r.kt)(n.Cl,{mdxType:"DocStringContainer"},(0,r.kt)("div",null,(0,r.kt)(n.Dx,{type:"class",name:"colossalai.nn.DistributedLamb",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/nn/optimizer/distributed_lamb.py#L15",mdxType:"Title"}),(0,r.kt)(n.Pc,{mdxType:"Signature"},"params, lr = 0.001, betas = (0.9, 0.999), eps = 1e-06, weight_decay = 0, bias_correction = True"),(0,r.kt)(n.aE,{mdxType:"Parameters"},"- **params** (iterable) -- iterable of parameters to optimize or dicts defining\n  parameter groups\n- **lr** (float, optional) -- learning rate (default: 1e-3)\n- **betas** (Tuple[float, float], optional) -- coefficients used for computing\n  running averages of gradient and its square (default: (0.9, 0.999))\n- **eps** (float, optional) -- term added to the denominator to improve\n  numerical stability (default: 1e-8)\n- **weight_decay** (float, optional) -- weight decay (L2 penalty) (default: 0)")),(0,r.kt)("div",null,(0,r.kt)(n.iz,{name:"Description",mdxType:"Divider"}),"Implements the Lamb algorithm, with extra support for ZeRO 2 and Tensor Parallel. Proposed in `Large Batch Optimization for Deep Learning: Training BERT in 76 minutes`_. It's recommended to use this with HybridParallelPlugin/ZeRO plugin and booster, which will take care of setup_distributed. Example with 4 devices: >>> optim = DistributedLamb(model.parameters(), lr=1e-3) >>> proc_mesh = ProcessGroupMesh(tp_size, zero_size) >>> tp_group = proc_mesh.get_group_along_axis(0) >>> dp_group = proc_mesh.get_group_along_axis(1) >>> optim.setup_distributed(tp_group, dp_group)",(0,r.kt)("p",null,".. _Large Batch Optimization for Deep Learning: Training BERT in 76 minutes:\n",(0,r.kt)("a",{parentName:"p",href:"https://arxiv.org/abs/1904.00962"},"https://arxiv.org/abs/1904.00962"))),(0,r.kt)(n.Cl,{mdxType:"DocStringContainer"},(0,r.kt)("div",null,(0,r.kt)(n.Dx,{type:"function",name:"setup_distributed",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/nn/optimizer/distributed_lamb.py#L65",mdxType:"Title"}),(0,r.kt)(n.Pc,{mdxType:"Signature"},"tp_group: typing.Optional[torch.distributed.distributed_c10d.ProcessGroup] = None, dp_group: typing.Optional[torch.distributed.distributed_c10d.ProcessGroup] = None, shard_to_working_param: typing.Optional[typing.Dict] = {}, padding_map = None, is_zero: typing.Optional[bool] = False"),(0,r.kt)(n.aE,{mdxType:"Parameters"},"- **tp_group** (dist.ProcessGroup) -- Tensor Parallel process group\n- **dp_group** (dist.ProcessGroup) -- ZeRO 2 process group\n- **shard_to_working_param** (Dict) -- ZeRO 2 feeds the optimizer a sharded param view as grads are sharded.\n  This maps from id(view) to working params used in forward & backward.\n  padding_map -- An empty interface placeholder\n- **is_zero** (bool) -- Whether to use ZeRO 2.")),(0,r.kt)("div",null,(0,r.kt)(n.iz,{name:"Description",mdxType:"Divider"}),"Assign process groups for TP and ZeRO 2.")),(0,r.kt)(n.Cl,{mdxType:"DocStringContainer"},(0,r.kt)("div",null,(0,r.kt)(n.Dx,{type:"function",name:"step",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/nn/optimizer/distributed_lamb.py#L103",mdxType:"Title"}),(0,r.kt)(n.Pc,{mdxType:"Signature"},"closure = None"),(0,r.kt)(n.aE,{mdxType:"Parameters"},"- **closure** (callable, optional) -- A closure that reevaluates the model\n  and returns the loss.")),(0,r.kt)("div",null,(0,r.kt)(n.iz,{name:"Description",mdxType:"Divider"}),"Performs a single optimization step."))),(0,r.kt)(n.Cl,{mdxType:"DocStringContainer"},(0,r.kt)("div",null,(0,r.kt)(n.Dx,{type:"class",name:"colossalai.nn.DistGaloreAwamW",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/nn/optimizer/distributed_galore.py#L21",mdxType:"Title"}),(0,r.kt)(n.Pc,{mdxType:"Signature"},"params, lr = 0.001, betas = (0.9, 0.999), eps = 1e-08, weight_decay = 0.01, nbits = 8, min_8bit_size = 4096, percentile_clipping = 100, block_wise = True, is_paged = False"),(0,r.kt)(n.aE,{mdxType:"Parameters"},"- **params** (iterable) -- iterable of parameters to optimize or dicts defining\n  parameter groups.\n- **lr** (float, optional) -- learning rate. (default: 1e-3)\n- **betas** (Tuple[float, float], optional) -- coefficients used for computing\n  running averages of gradient and its norm. (default: (0.9, 0.999))\n- **eps** (float, optional) -- term added to the denominator to improve\n  numerical stability. (default: 1e-6)\n- **weight_decay** (float, optional) -- weight decay (L2 penalty) (default: 0.01)\n  nbits -- Number of bits for quantization optim states. Only 32 and 8 are supported.\n- **min_8bit_size** (`int`, defaults to 4096) --\n  The minimum number of elements of the parameter tensors for 8-bit optimization.\n- **percentile_clipping** (`int`, defaults to 100) --\n  Adapts clipping threshold automatically by tracking the last 100 gradient norms and clipping the gradient at a certain percentile to improve stability.\n- **block_wise** (`bool`, defaults to `True`) --\n  Whether to independently quantize each block of tensors to reduce outlier effects and improve stability.\n- **is_paged** (`bool`, defaults to `False`) --\n  Whether the optimizer is a paged optimizer (handle memory spike via CPU-GPU transfer) or not.")),(0,r.kt)("div",null,(0,r.kt)(n.iz,{name:"Description",mdxType:"Divider"}),"Implements Galore, a optimizer-agonistic gradient compression technique on 8-bit AdamW. It largely compresses gradient via low-rank projection and is claimed to be insensitive to hyperparams like lr. Supports Tensor Parallel and ZeRO stage 1 and 2 via booster and plugin. Proposed in `GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection` https://arxiv.org/abs/2403.03507"),(0,r.kt)(n.Cl,{mdxType:"DocStringContainer"},(0,r.kt)("div",null,(0,r.kt)(n.Dx,{type:"function",name:"setup_distributed",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/nn/optimizer/distributed_galore.py#L91",mdxType:"Title"}),(0,r.kt)(n.Pc,{mdxType:"Signature"},"tp_group: typing.Optional[torch.distributed.distributed_c10d.ProcessGroup] = None, dp_group: typing.Optional[torch.distributed.distributed_c10d.ProcessGroup] = None, shard_to_working_param: typing.Optional[typing.Dict] = {}, padding_map: typing.Optional[typing.Dict] = defaultdict(<class 'int'>, {}), is_zero: typing.Optional[bool] = False"),(0,r.kt)(n.aE,{mdxType:"Parameters"},"- **tp_group** (dist.ProcessGroup) -- Tensor Parallel process group\n- **dp_group** (dist.ProcessGroup) -- ZeRO 2 process group\n- **shard_to_working_param** (Dict) -- ZeRO 2 feeds the optimizer a sharded param view as grads are sharded.\n  This maps from id(view) to working params used in forward & backward.\n- **padding_map** (Dict) -- Padding size of each param from ZeRO's param store. Required if ZeRO is used.\n- **is_zero** (bool) -- Whether to use ZeRO 2.")),(0,r.kt)("div",null,(0,r.kt)(n.iz,{name:"Description",mdxType:"Divider"}),"Setup process groups for TP and ZeRO 2.")),(0,r.kt)(n.Cl,{mdxType:"DocStringContainer"},(0,r.kt)("div",null,(0,r.kt)(n.Dx,{type:"function",name:"step",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/nn/optimizer/distributed_galore.py#L139",mdxType:"Title"}),(0,r.kt)(n.Pc,{mdxType:"Signature"},"closure = None"),(0,r.kt)(n.aE,{mdxType:"Parameters"},"- **closure** (callable, optional) -- A closure that reevaluates the model\n  and returns the loss.")),(0,r.kt)("div",null,(0,r.kt)(n.iz,{name:"Description",mdxType:"Divider"}),"Performs a single optimization step.")),(0,r.kt)(n.Cl,{mdxType:"DocStringContainer"},(0,r.kt)("div",null,(0,r.kt)(n.Dx,{type:"function",name:"to_master_shape",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/nn/optimizer/distributed_galore.py#L265",mdxType:"Title"}),(0,r.kt)(n.Pc,{mdxType:"Signature"},"data, padding")),(0,r.kt)("div",null,(0,r.kt)(n.iz,{name:"Description",mdxType:"Divider"}),"Pad to master (optimizer) param shape"))),(0,r.kt)(n.Cl,{mdxType:"DocStringContainer"},(0,r.kt)("div",null,(0,r.kt)(n.Dx,{type:"class",name:"colossalai.nn.DistributedCAME",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/nn/optimizer/distributed_came.py#L11",mdxType:"Title"}),(0,r.kt)(n.Pc,{mdxType:"Signature"},"params, lr = None, eps = (1e-30, 1e-16), clip_threshold = 1.0, betas = (0.9, 0.999, 0.9999), weight_decay = 0.0"),(0,r.kt)(n.aE,{mdxType:"Parameters"},"- **params** (iterable) -- iterable of parameters to optimize or dicts defining\n  parameter groups\n- **lr** (float, optional) -- external learning rate (default: None)\n- **eps** (tuple[float, float]) -- regularization constants for square gradient\n  and instability respectively (default: (1e-30, 1e-16))\n- **clip_threshold** (float) -- threshold of root-mean-square of\n  final gradient update (default: 1.0)\n- **betas** (tuple[float, float, float]) -- coefficient used for computing running averages of\n- **update,** square gradient and instability (default -- (0.9, 0.999, 0.9999)))\n- **weight_decay** (float, optional) -- weight decay (L2 penalty) (default: 0)")),(0,r.kt)("div",null,(0,r.kt)(n.iz,{name:"Description",mdxType:"Divider"}),"Implements CAME algorithm. This implementation is based on: `CAME: Confidence-guided Adaptive Memory Efficient Optimization`"),(0,r.kt)(n.Cl,{mdxType:"DocStringContainer"},(0,r.kt)("div",null,(0,r.kt)(n.Dx,{type:"function",name:"setup_distributed",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/nn/optimizer/distributed_came.py#L71",mdxType:"Title"}),(0,r.kt)(n.Pc,{mdxType:"Signature"},"tp_group: ProcessGroup = None, dp_group: ProcessGroup = None, shard_to_working_param: typing.Dict = {}, padding_map = None, use_zero: bool = True"),(0,r.kt)(n.aE,{mdxType:"Parameters"},"tp_group -- The devices group for tensor parallel;\ndp_group -- The devices group for data parallel;\n- **shard_to_working_param** (Dict) -- ZeRO 2 feeds the optimizer a sharded param view as grads are sharded.\n  This maps from id(view) to working params used in forward & backward.\n  padding_map -- Interface placeholder\n  use_zero -- Whether or not to use zero;")),(0,r.kt)("div",null,(0,r.kt)(n.iz,{name:"Description",mdxType:"Divider"}),(0,r.kt)("p",null,"Inject features to the Optimizer"))),(0,r.kt)(n.Cl,{mdxType:"DocStringContainer"},(0,r.kt)("div",null,(0,r.kt)(n.Dx,{type:"function",name:"step",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/nn/optimizer/distributed_came.py#L335",mdxType:"Title"}),(0,r.kt)(n.Pc,{mdxType:"Signature"},"closure = None"),(0,r.kt)(n.aE,{mdxType:"Parameters"},"- **closure** (callable, optional) -- A closure that reevaluates the model\n  and returns the loss.")),(0,r.kt)("div",null,(0,r.kt)(n.iz,{name:"Description",mdxType:"Divider"}),"Performs a single optimization step."))),(0,r.kt)("h2",{id:"hands-on-practice"},"Hands-On Practice"),(0,r.kt)("p",null,"We now demonstrate how to use Distributed Adafactor with booster API combining Tensor Parallel and ZeRO 2 with 4 GPUs."),(0,r.kt)("h3",{id:"step-1-import-libraries"},"step 1. Import libraries"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"from transformers import LlamaModel, LlamaConfig\nfrom colossalai.nn.optimizer.distributed_adafactor import DistributedAdaFactor\nfrom colossalai.booster import Booster\nfrom colossalai.booster.plugin import HybridParallelPlugin\nimport colossalai\nimport torch\n")),(0,r.kt)("h3",{id:"step-2-initialize-distributed-environment-and-parallism-group"},"step 2. Initialize Distributed Environment and Parallism Group"),(0,r.kt)("p",null,"We need to initialize distributed environment. For demo purpose, we use ",(0,r.kt)("inlineCode",{parentName:"p"},"colossal run --nproc_per_node 4"),". You can refer to ",(0,r.kt)("a",{parentName:"p",href:"/docs/basics/launch_colossalai"},"Launch Colossal-AI")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"colossalai.launch_from_torch()\n")),(0,r.kt)("h3",{id:"step-3-initialize-module-and-optimizer"},"step 3. Initialize Module and Optimizer"),(0,r.kt)("p",null,"Build our model. We created an MLP using two Linear Layer."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"# Init Llama from huggingface\nconfiguration = LlamaConfig()\nmodel = LlamaModel(configuration).cuda()\ncriterion = lambda x: x.mean()\ndist_optim = DistributedAdaFactor(model.parameters())\n\n")),(0,r.kt)("h3",{id:"step-4init-booster"},"step 4.Init Booster"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"plugin = HybridParallelPlugin(tp_size=2, zero_stage=2, pp_size=1, enable_all_optimization=True)\nbooster = Booster(plugin=plugin)\n# You should also pass in your own dataset.\nmodel, dist_optim, criterion, dataloader, _ = booster.boost(model, dist_optim, criterion)\n")),(0,r.kt)("h3",{id:"step-5train-your-model"},"step 5.Train Your Model"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'steps = 10\nfor step in range(steps):\n    input_ids = torch.ones(1, 100, device="cuda", dtype=torch.int)\n    attention_mask = input_ids.clone()\n    outputs = model(input_ids.cuda(), attention_mask.cuda())\n    loss = criterion(outputs.last_hidden_state)\n    booster.backward(loss, dist_optim)\n    dist_optim.step()\n    dist_optim.zero_grad()\n')),(0,r.kt)("h3",{id:"galore-special-handling"},"GaLore special handling"),(0,r.kt)("p",null,"For GaLore, we need to specify projection rank for each parameter group and quantization & paged optimizer params. Please refer to bitandbytes for quantization details. Support for ZeRO is underway."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"from colossalai.nn.optimizer.galore import get_galore_param_groups\nfrom colossalai.nn.optimizer import DistGaloreAwamW\noptim = DistGaloreAwamW(\n    get_galore_param_groups(model, decay=1e-2, rank=8),\n    lr=lr,\n    betas=(beta1, beta2),\n    eps=eps,\n    nbits=8,\n    percentile_clipping=100,\n    block_wise=True,\n    min_8bit_size=4096,\n)\n")),(0,r.kt)("h2",{id:"plugin-compatibility"},"Plugin compatibility"),(0,r.kt)("table",null,(0,r.kt)("tr",null,(0,r.kt)("th",{nowrap:"nowrap"},"Model/Feature"),(0,r.kt)("th",{nowrap:"nowrap",align:"center",title:"Lamb"},"Lamb"),(0,r.kt)("th",{nowrap:"nowrap",align:"center",title:"GaLore"},"GaLore"),(0,r.kt)("th",{nowrap:"nowrap",align:"center",title:"Adafactor"},"Adafactor"),(0,r.kt)("th",{nowrap:"nowrap",align:"center",title:"CAME"},"CAME")),(0,r.kt)("tr",null,(0,r.kt)("td",{nowrap:"nowrap"},"Hybrid Parallel",(0,r.kt)("br",null),"Plugin"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f")),(0,r.kt)("tr",null,(0,r.kt)("td",{nowrap:"nowrap"},"Low Level Zero",(0,r.kt)("br",null),"Plugin"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u274c"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f")),(0,r.kt)("tr",null,(0,r.kt)("td",{nowrap:"nowrap"},"Torch DDP",(0,r.kt)("br",null),"Plugin"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f")),(0,r.kt)("tr",null,(0,r.kt)("td",{nowrap:"nowrap"},"Gemini",(0,r.kt)("br",null),"Plugin"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u274c"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u274c"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u274c"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u274c")),(0,r.kt)("tr",null,(0,r.kt)("td",{nowrap:"nowrap"},"Moe Hybrid",(0,r.kt)("br",null),"Plugin"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u274c"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u274c"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u274c"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u274c")),(0,r.kt)("tr",null,(0,r.kt)("td",{colspan:"39"}))))}c.isMDXComponent=!0}}]);