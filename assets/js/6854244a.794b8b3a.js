"use strict";(self.webpackChunkdemo=self.webpackChunkdemo||[]).push([[1906],{6999:(e,t,n)=>{n.d(t,{Cl:()=>o,Dx:()=>c,Pc:()=>r,aE:()=>s,e_:()=>m,iz:()=>l,nT:()=>p});var i=n(7294),a=n(398);n(814);function o(e){return i.createElement("div",{className:"docstring-container"},e.children)}function r(e){return i.createElement("div",{className:"signature"},"(",e.children,")")}function l(e){return i.createElement("div",{class:"divider"},i.createElement("span",{class:"divider-text"},e.name))}function s(e){return i.createElement("div",null,i.createElement(l,{name:"Parameters"}),i.createElement(a.D,null,e.children))}function p(e){return i.createElement("div",null,i.createElement(l,{name:"Returns"}),i.createElement(a.D,null,`${e.name}: ${e.desc}`))}function c(e){return i.createElement("div",{className:"title-container"},i.createElement("div",{className:"title-module"},i.createElement("h5",null,e.type),"\xa0 ",i.createElement("h3",null,e.name)),i.createElement("div",{className:"title-source"},"<",i.createElement("a",{href:e.source,className:"title-source"},"source"),">"))}function m(e){return i.createElement("div",null,i.createElement(l,{name:"Example"}),i.createElement(a.D,null,e.code))}},6321:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>p,contentTitle:()=>l,default:()=>u,frontMatter:()=>r,metadata:()=>s,toc:()=>c});var i=n(7462),a=(n(7294),n(3905)),o=n(6999);const r={},l="Auto Mixed Precision Training",s={unversionedId:"features/mixed_precision_training_with_booster",id:"features/mixed_precision_training_with_booster",title:"Auto Mixed Precision Training",description:"Author: Mingyan Jiang",source:"@site/i18n/en/docusaurus-plugin-content-docs/current/features/mixed_precision_training_with_booster.md",sourceDirName:"features",slug:"/features/mixed_precision_training_with_booster",permalink:"/docs/features/mixed_precision_training_with_booster",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/features/mixed_precision_training_with_booster.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Shardformer",permalink:"/docs/features/shardformer"},next:{title:"Gradient Accumulation",permalink:"/docs/features/gradient_accumulation_with_booster"}},p={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Table of Contents",id:"table-of-contents",level:2},{value:"AMP Introduction",id:"amp-introduction",level:2},{value:"AMP in Colossal-AI",id:"amp-in-colossal-ai",level:2},{value:"Start with Booster",id:"start-with-booster",level:3},{value:"Torch AMP Configuration",id:"torch-amp-configuration",level:3},{value:"Apex AMP Configuration",id:"apex-amp-configuration",level:3},{value:"Naive AMP Configuration",id:"naive-amp-configuration",level:3},{value:"Hands-on Practice",id:"hands-on-practice",level:2},{value:"Step 1. Import libraries in train.py",id:"step-1-import-libraries-in-trainpy",level:3},{value:"Step 2. Initialize Distributed Environment",id:"step-2-initialize-distributed-environment",level:3},{value:"Step 3. Create training components",id:"step-3-create-training-components",level:3},{value:"Step 4. Inject AMP Feature",id:"step-4-inject-amp-feature",level:3},{value:"Step 5. Train with Booster",id:"step-5-train-with-booster",level:3},{value:"Step 6. Invoke Training Scripts",id:"step-6-invoke-training-scripts",level:3}],m={toc:c},d="wrapper";function u(e){let{components:t,...n}=e;return(0,a.kt)(d,(0,i.Z)({},m,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"auto-mixed-precision-training"},"Auto Mixed Precision Training"),(0,a.kt)("p",null,"Author: ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/jiangmingyan"},"Mingyan Jiang")),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"Prerequisite")),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"/docs/basics/booster_api"},"Training Booster"))),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"Related Paper")),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/0808.2794"},"Accelerating Scientific Computations with Mixed Precision Algorithms"))),(0,a.kt)("h2",{id:"introduction"},"Introduction"),(0,a.kt)("p",null,"AMP stands for automatic mixed precision training.\nIn Colossal-AI, we have incorporated different implementations of mixed precision training:"),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},"torch.cuda.amp"),(0,a.kt)("li",{parentName:"ol"},"apex.amp"),(0,a.kt)("li",{parentName:"ol"},"naive amp")),(0,a.kt)("table",null,(0,a.kt)("thead",{parentName:"table"},(0,a.kt)("tr",{parentName:"thead"},(0,a.kt)("th",{parentName:"tr",align:null},"Colossal-AI"),(0,a.kt)("th",{parentName:"tr",align:null},"support tensor parallel"),(0,a.kt)("th",{parentName:"tr",align:null},"support pipeline parallel"),(0,a.kt)("th",{parentName:"tr",align:null},"fp16 extent"))),(0,a.kt)("tbody",{parentName:"table"},(0,a.kt)("tr",{parentName:"tbody"},(0,a.kt)("td",{parentName:"tr",align:null},"AMP_TYPE.TORCH"),(0,a.kt)("td",{parentName:"tr",align:null},"\u2705"),(0,a.kt)("td",{parentName:"tr",align:null},"\u274c"),(0,a.kt)("td",{parentName:"tr",align:null},"Model parameters, activation, gradients are downcast to fp16 during forward and backward propagation")),(0,a.kt)("tr",{parentName:"tbody"},(0,a.kt)("td",{parentName:"tr",align:null},"AMP_TYPE.APEX"),(0,a.kt)("td",{parentName:"tr",align:null},"\u274c"),(0,a.kt)("td",{parentName:"tr",align:null},"\u274c"),(0,a.kt)("td",{parentName:"tr",align:null},"More fine-grained, we can choose opt_level O0, O1, O2, O3")),(0,a.kt)("tr",{parentName:"tbody"},(0,a.kt)("td",{parentName:"tr",align:null},"AMP_TYPE.NAIVE"),(0,a.kt)("td",{parentName:"tr",align:null},"\u2705"),(0,a.kt)("td",{parentName:"tr",align:null},"\u2705"),(0,a.kt)("td",{parentName:"tr",align:null},"Model parameters, forward and backward operations are all downcast to fp16")))),(0,a.kt)("p",null,"The first two rely on the original implementation of PyTorch (version 1.6 and above) and NVIDIA Apex.\nThe last method is similar to Apex O2 level.\nAmong these methods, apex AMP is not compatible with tensor parallelism.\nThis is because that tensors are split across devices in tensor parallelism, thus, it is required to communicate among different processes to check if inf or nan occurs in the whole model weights.\nWe modified the torch amp implementation so that it is compatible with tensor parallelism now."),(0,a.kt)("blockquote",null,(0,a.kt)("p",{parentName:"blockquote"},"\u274c\ufe0f fp16 and zero are not compatible"),(0,a.kt)("p",{parentName:"blockquote"},"\u26a0\ufe0f Pipeline only support naive AMP currently")),(0,a.kt)("p",null,"We recommend you to use torch AMP as it generally gives better accuracy than naive AMP if no pipeline is used."),(0,a.kt)("h2",{id:"table-of-contents"},"Table of Contents"),(0,a.kt)("p",null,"In this tutorial we will cover:"),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("a",{parentName:"li",href:"#amp-introduction"},"AMP introduction")),(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("a",{parentName:"li",href:"#amp-in-colossal-ai"},"AMP in Colossal-AI")),(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("a",{parentName:"li",href:"#hands-on-practice"},"Hands-on Practice"))),(0,a.kt)("h2",{id:"amp-introduction"},"AMP Introduction"),(0,a.kt)("p",null,"Automatic Mixed Precision training is a mixture of FP16 and FP32 training."),(0,a.kt)("p",null,"Half-precision float point format (FP16) has lower arithmetic complexity and higher compute efficiency. Besides, fp16 requires half of the storage needed by fp32 and saves memory & network bandwidth, which makes more memory available for large batch size and model size."),(0,a.kt)("p",null,"However, there are other operations, like reductions, which require the dynamic range of fp32 to avoid numeric overflow/underflow. That's the reason why we introduce automatic mixed precision, attempting to match each operation to its appropriate data type, which can reduce the memory footprint and augment training efficiency."),(0,a.kt)("figure",{style:{textAlign:"center"}},(0,a.kt)("img",{src:"https://s2.loli.net/2022/01/28/URzLJ3MPeDQbtck.png"}),(0,a.kt)("figcaption",null,"Illustration of an ordinary AMP (figure from ",(0,a.kt)("a",{href:"https://arxiv.org/abs/2108.05818"},"PatrickStar paper"),")")),(0,a.kt)("h2",{id:"amp-in-colossal-ai"},"AMP in Colossal-AI"),(0,a.kt)("p",null,"We supported three AMP training methods and allowed the user to train with AMP with no code. If you want to train with amp, just assign ",(0,a.kt)("inlineCode",{parentName:"p"},"mixed_precision")," with ",(0,a.kt)("inlineCode",{parentName:"p"},"fp16")," when you instantiate the ",(0,a.kt)("inlineCode",{parentName:"p"},"Booster"),". Next we will support ",(0,a.kt)("inlineCode",{parentName:"p"},"bf16"),", ",(0,a.kt)("inlineCode",{parentName:"p"},"fp8"),"."),(0,a.kt)("h3",{id:"start-with-booster"},"Start with Booster"),(0,a.kt)("p",null,"instantiate ",(0,a.kt)("inlineCode",{parentName:"p"},"Booster")," with ",(0,a.kt)("inlineCode",{parentName:"p"},'mixed_precision="fp16"'),", then you can train with torch amp."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"\"\"\"\n    Mapping:\n    'fp16': torch amp\n    'fp16_apex': apex amp,\n    'bf16': bf16,\n    'fp8': fp8,\n    'fp16_naive': naive amp\n\"\"\"\nfrom colossalai import Booster\nbooster = Booster(mixed_precision='fp16',...)\n")),(0,a.kt)("p",null,"or you can create a ",(0,a.kt)("inlineCode",{parentName:"p"},"FP16TorchMixedPrecision")," object, such as:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"from colossalai.mixed_precision import FP16TorchMixedPrecision\nmixed_precision = FP16TorchMixedPrecision(\n    init_scale=2.**16,\n    growth_factor=2.0,\n    backoff_factor=0.5,\n    growth_interval=2000)\nbooster = Booster(mixed_precision=mixed_precision,...)\n")),(0,a.kt)("p",null,"The same goes for other types of amps."),(0,a.kt)("h3",{id:"torch-amp-configuration"},"Torch AMP Configuration"),(0,a.kt)(o.Cl,{mdxType:"DocStringContainer"},(0,a.kt)("div",null,(0,a.kt)(o.Dx,{type:"class",name:"colossalai.booster.mixed_precision.FP16TorchMixedPrecision",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/booster/mixed_precision/fp16_torch.py#L96",mdxType:"Title"}),(0,a.kt)(o.Pc,{mdxType:"Signature"},"init_scale: float = 65536.0, growth_factor: float = 2.0, backoff_factor: float = 0.5, growth_interval: int = 2000"),(0,a.kt)(o.aE,{mdxType:"Parameters"},"- **init_scale** (float) -- Initial scale factor. Default: 2**16.\n- **growth_factor** (float) -- Factor by which the scale is multiplied during\n  [`torch.cuda.amp.GradScaler.step`] if gradients were found to be finite\n  this iteration. Default: 2.0.\n- **backoff_factor** (float) -- Factor by which the scale is multiplied during\n  [`torch.cuda.amp.GradScaler.step`] if gradients were found to be infinite\n  this iteration. Default: 0.5.\n- **growth_interval** (int) -- Number of iterations between [`torch.cuda.amp.GradScaler.step`]\n  calls that may cause the scale to increase. Default: 2000.")),(0,a.kt)("div",null,(0,a.kt)(o.iz,{name:"Description",mdxType:"Divider"}),(0,a.kt)("p",null,"Precision for mixed precision training in FP16 using PyTorch AMP."))),(0,a.kt)("h3",{id:"apex-amp-configuration"},"Apex AMP Configuration"),(0,a.kt)("p",null,"For this mode, we rely on the Apex implementation for mixed precision training.\nWe support this plugin because it allows for finer control on the granularity of mixed precision.\nFor example, O2 level (optimization level 2) will keep batch normalization in fp32."),(0,a.kt)("p",null,"If you look for more details, please refer to ",(0,a.kt)("a",{parentName:"p",href:"https://nvidia.github.io/apex/"},"Apex Documentation"),"."),(0,a.kt)(o.Cl,{mdxType:"DocStringContainer"},(0,a.kt)("div",null,(0,a.kt)(o.Dx,{type:"class",name:"colossalai.booster.mixed_precision.FP16ApexMixedPrecision",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/booster/mixed_precision/fp16_apex.py#L8",mdxType:"Title"}),(0,a.kt)(o.Pc,{mdxType:"Signature"},"opt_level: typing.Optional[str] = 'O1', cast_model_type: dtype = None, patch_torch_functions: bool = None, keep_batchnorm_fp32: typing.Union[bool, str] = None, master_weights: bool = None, loss_scale: typing.Union[float, str] = None, cast_model_outputs: typing.Any = None, num_losses: typing.Optional[int] = 1, verbosity: int = 1, min_loss_scale: float = None, max_loss_scale: float = 16777216.0"),(0,a.kt)(o.aE,{mdxType:"Parameters"},'- **opt_level(str,** optional, default="O1" ) -- Pure or mixed precision optimization level. Accepted values are \u201cO0\u201d, \u201cO1\u201d, \u201cO2\u201d, and \u201cO3\u201d, explained in detail above Apex AMP Documentation.\n- **cast_model_type** (torch.dtype, optional, default=None) -- Casts your model\u2019s parameters and buffers to the desired type.\n- **patch_torch_functions** (bool, optional, default=None) -- Patch all Torch functions and Tensor methods to perform Tensor Core-friendly ops like GEMMs and convolutions in FP16, and any ops that benefit from FP32 precision in FP32.\n- **keep_batchnorm_fp32** (bool or str, optional, default=None) -- To enhance precision and enable cudnn batchnorm (which improves performance), it\u2019s often beneficial to keep batchnorm weights in FP32 even if the rest of the model is FP16.\n- **master_weights** (bool, optional, default=None) -- Maintain FP32 master weights to accompany any FP16 model weights. FP32 master weights are stepped by the optimizer to enhance precision and capture small gradients.\n- **loss_scale** (float or str, optional, default=None) -- If loss_scale is a float value, use this value as the static (fixed) loss scale. If loss_scale is the string "dynamic", adaptively adjust the loss scale over time. Dynamic loss scale adjustments are performed by Amp automatically.\n- **cast_model_outputs** (torch.dpython --type, optional, default=None): Option to ensure that the outputs of your model(s) are always cast to a particular type regardless of opt_level.\n- **num_losses(int,** optional, default=1) -- Option to tell AMP in advance how many losses/backward passes you plan to use. When used in conjunction with the loss_id argument to `amp.scale_loss`, enables Amp to use a different loss scale per loss/backward pass, which can improve stability. If num_losses is left to 1, Amp will still support multiple losses/backward passes, but use a single global loss scale for all of them.\n- **verbosity(int,** default=1) -- Set to 0 to suppress Amp-related output.\n- **min_loss_scale(float,** default=None) -- Sets a floor for the loss scale values that can be chosen by dynamic loss scaling. The default value of None means that no floor is imposed. If dynamic loss scaling is not used, min_loss_scale is ignored.\n- **max_loss_scale(float,** default=2.**24 ) -- Sets a ceiling for the loss scale values that can be chosen by dynamic loss scaling. If dynamic loss scaling is not used, max_loss_scale is ignored.')),(0,a.kt)("div",null,(0,a.kt)(o.iz,{name:"Description",mdxType:"Divider"}),(0,a.kt)("p",null,"Precision for mixed precision training in FP16 using apex AMP."))),(0,a.kt)("h3",{id:"naive-amp-configuration"},"Naive AMP Configuration"),(0,a.kt)("p",null,"In Naive AMP mode, we achieved mixed precision training while maintaining compatibility with complex tensor and pipeline parallelism.\nThis AMP mode will cast all operations into fp16.\nThe following code block shows the mixed precision api for this mode."),(0,a.kt)(o.Cl,{mdxType:"DocStringContainer"},(0,a.kt)("div",null,(0,a.kt)(o.Dx,{type:"class",name:"colossalai.booster.mixed_precision.FP16NaiveMixedPrecision",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/booster/mixed_precision/fp16_naive.py#L4",mdxType:"Title"}),(0,a.kt)(o.Pc,{mdxType:"Signature"},"log_num_zeros_in_grad: bool, initial_scale: int, growth_factor: int, backoff_factor: float, hysteresis: int, max_scale: int, verbose: bool = None"),(0,a.kt)(o.aE,{mdxType:"Parameters"},"log_num_zeros_in_grad(bool) -- return number of zeros in the gradients.\ninitial_scale(int) -- initial scale of gradient scaler.\ngrowth_factor(int) -- the growth rate of loss scale.\nbackoff_factor(float) -- the decrease rate of loss scale.\nhysteresis(int) -- delay shift in dynamic loss scaling.\nmax_scale(int) -- maximum loss scale allowed.\nverbose(bool) -- if set to `True`, will print debug info.")),(0,a.kt)("div",null,(0,a.kt)(o.iz,{name:"Description",mdxType:"Divider"}),(0,a.kt)("p",null,"Precision for mixed precision training in FP16 using naive AMP."))),(0,a.kt)("p",null,"When using ",(0,a.kt)("inlineCode",{parentName:"p"},"colossalai.booster"),", you are required to first instantiate a model, an optimizer and a criterion.\nThe output model is converted to AMP model of smaller memory consumption.\nIf your input model is already too large to fit in a GPU, please instantiate your model weights in ",(0,a.kt)("inlineCode",{parentName:"p"},"dtype=torch.float16"),".\nOtherwise, try smaller models or checkout more parallelization training techniques!"),(0,a.kt)("h2",{id:"hands-on-practice"},"Hands-on Practice"),(0,a.kt)("p",null,"Now we will introduce the use of AMP with Colossal-AI. In this practice, we will use Torch AMP as an example."),(0,a.kt)("h3",{id:"step-1-import-libraries-in-trainpy"},"Step 1. Import libraries in train.py"),(0,a.kt)("p",null,"Create a ",(0,a.kt)("inlineCode",{parentName:"p"},"train.py")," and import the necessary dependencies. Remember to install ",(0,a.kt)("inlineCode",{parentName:"p"},"scipy")," and ",(0,a.kt)("inlineCode",{parentName:"p"},"timm")," by running\n",(0,a.kt)("inlineCode",{parentName:"p"},"pip install timm scipy"),"."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"import os\nfrom pathlib import Path\n\nimport torch\nfrom timm.models import vit_base_patch16_224\nfrom titans.utils import barrier_context\nfrom torchvision import datasets, transforms\n\nimport colossalai\nfrom colossalai.booster import Booster\nfrom colossalai.booster.plugin import TorchDDPPlugin\nfrom colossalai.logging import get_dist_logger\nfrom colossalai.nn.lr_scheduler import LinearWarmupLR\n")),(0,a.kt)("h3",{id:"step-2-initialize-distributed-environment"},"Step 2. Initialize Distributed Environment"),(0,a.kt)("p",null,"We then need to initialize distributed environment. For demo purpose, we uses ",(0,a.kt)("inlineCode",{parentName:"p"},"launch_from_torch"),". You can refer to ",(0,a.kt)("a",{parentName:"p",href:"/docs/basics/launch_colossalai"},"Launch Colossal-AI"),"\nfor other initialization methods."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"# initialize distributed setting\nparser = colossalai.get_default_parser()\nargs = parser.parse_args()\n\n# launch from torch\ncolossalai.launch_from_torch()\n\n")),(0,a.kt)("h3",{id:"step-3-create-training-components"},"Step 3. Create training components"),(0,a.kt)("p",null,"Build your model, optimizer, loss function, lr scheduler and dataloaders. Note that the root path of the dataset is\nobtained from the environment variable ",(0,a.kt)("inlineCode",{parentName:"p"},"DATA"),". You may ",(0,a.kt)("inlineCode",{parentName:"p"},"export DATA=/path/to/data")," or change ",(0,a.kt)("inlineCode",{parentName:"p"},"Path(os.environ['DATA'])"),"\nto a path on your machine. Data will be automatically downloaded to the root path."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"# define the constants\nNUM_EPOCHS = 2\nBATCH_SIZE = 128\n\n# build model\nmodel = vit_base_patch16_224(drop_rate=0.1)\n\n# build dataloader\ntrain_dataset = datasets.Caltech101(\n    root=Path(os.environ['DATA']),\n    download=True,\n    transform=transforms.Compose([\n        transforms.Resize(256),\n        transforms.RandomResizedCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        Gray2RGB(),\n        transforms.Normalize([0.5, 0.5, 0.5],\n                                [0.5, 0.5, 0.5])\n    ]))\n\n# build optimizer\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-2, weight_decay=0.1)\n\n# build loss\ncriterion = torch.nn.CrossEntropyLoss()\n\n# lr_scheduler\nlr_scheduler = LinearWarmupLR(optimizer, warmup_steps=50, total_steps=NUM_EPOCHS)\n")),(0,a.kt)("h3",{id:"step-4-inject-amp-feature"},"Step 4. Inject AMP Feature"),(0,a.kt)("p",null,"Create a ",(0,a.kt)("inlineCode",{parentName:"p"},"MixedPrecision"),"(if needed) and ",(0,a.kt)("inlineCode",{parentName:"p"},"TorchDDPPlugin")," object, call ",(0,a.kt)("inlineCode",{parentName:"p"},"colossalai.boost")," convert the training components to be running with FP16."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"plugin = TorchDDPPlugin()\ntrain_dataloader = plugin.prepare_dataloader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\nbooster = Booster(mixed_precision='fp16', plugin=plugin)\n\n# if you need to customize the config, do like this\n# >>> from colossalai.mixed_precision import FP16TorchMixedPrecision\n# >>> mixed_precision = FP16TorchMixedPrecision(\n# >>>     init_scale=2.**16,\n# >>>     growth_factor=2.0,\n# >>>     backoff_factor=0.5,\n# >>>     growth_interval=2000)\n# >>> plugin = TorchDDPPlugin()\n# >>> booster = Booster(mixed_precision=mixed_precision, plugin=plugin)\n\n# boost model, optimizer, criterion, dataloader, lr_scheduler\nmodel, optimizer, criterion, dataloader, lr_scheduler = booster.boost(model, optimizer, criterion, dataloader, lr_scheduler)\n")),(0,a.kt)("h3",{id:"step-5-train-with-booster"},"Step 5. Train with Booster"),(0,a.kt)("p",null,"Use booster in a normal training loops."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"model.train()\nfor epoch in range(NUM_EPOCHS):\n    for img, label in enumerate(train_dataloader):\n        img = img.cuda()\n        label = label.cuda()\n        optimizer.zero_grad()\n        output = model(img)\n        loss = criterion(output, label)\n        booster.backward(loss, optimizer)\n        optimizer.step()\n    lr_scheduler.step()\n")),(0,a.kt)("h3",{id:"step-6-invoke-training-scripts"},"Step 6. Invoke Training Scripts"),(0,a.kt)("p",null,"Use the following command to start the training scripts. You can change ",(0,a.kt)("inlineCode",{parentName:"p"},"--nproc_per_node")," to use a different number of GPUs."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell"},"colossalai run --nproc_per_node 1 train.py\n")))}u.isMDXComponent=!0}}]);