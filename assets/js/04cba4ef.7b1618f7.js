"use strict";(self.webpackChunkagile_docs=self.webpackChunkagile_docs||[]).push([[9145],{3905:function(e,t,n){n.d(t,{Zo:function(){return c},kt:function(){return m}});var o=n(7294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);t&&(o=o.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,o)}return n}function a(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,o,r=function(e,t){if(null==e)return{};var n,o,r={},i=Object.keys(e);for(o=0;o<i.length;o++)n=i[o],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(o=0;o<i.length;o++)n=i[o],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var s=o.createContext({}),p=function(e){var t=o.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):a(a({},t),e)),n},c=function(e){var t=p(e.components);return o.createElement(s.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return o.createElement(o.Fragment,{},t)}},u=o.forwardRef((function(e,t){var n=e.components,r=e.mdxType,i=e.originalType,s=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),u=p(n),m=r,h=u["".concat(s,".").concat(m)]||u[m]||d[m]||i;return n?o.createElement(h,a(a({ref:t},c),{},{components:n})):o.createElement(h,a({ref:t},c))}));function m(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=n.length,a=new Array(i);a[0]=u;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l.mdxType="string"==typeof e?e:r,a[1]=l;for(var p=2;p<i;p++)a[p]=n[p];return o.createElement.apply(null,a)}return o.createElement.apply(null,n)}u.displayName="MDXCreateElement"},3497:function(e,t,n){n.r(t),n.d(t,{frontMatter:function(){return l},contentTitle:function(){return s},metadata:function(){return p},toc:function(){return c},default:function(){return u}});var o=n(3117),r=n(102),i=(n(7294),n(3905)),a=["components"],l={},s="Parallelize Your Training like Megatron-LM via ColoTensor",p={unversionedId:"advanced_tutorials/parallelize_your_training_like_Megatron",id:"version-v0.2.2/advanced_tutorials/parallelize_your_training_like_Megatron",title:"Parallelize Your Training like Megatron-LM via ColoTensor",description:"Author: Haichen Huang and Jiarui Fang",source:"@site/i18n/en/docusaurus-plugin-content-docs/version-v0.2.2/advanced_tutorials/parallelize_your_training_like_Megatron.md",sourceDirName:"advanced_tutorials",slug:"/advanced_tutorials/parallelize_your_training_like_Megatron",permalink:"/docs/advanced_tutorials/parallelize_your_training_like_Megatron",tags:[],version:"v0.2.2",frontMatter:{}},c=[{value:"Introduction",id:"introduction",children:[],level:2},{value:"Definitions of the model and the loss function",id:"definitions-of-the-model-and-the-loss-function",children:[],level:2},{value:"Brief Review of GPT-2",id:"brief-review-of-gpt-2",children:[],level:2},{value:"Applied with ColoTensor",id:"applied-with-colotensor",children:[{value:"Initialize with ColoInitContext",id:"initialize-with-coloinitcontext",children:[],level:3},{value:"Setting ColoTensorSpec for each parameter",id:"setting-colotensorspec-for-each-parameter",children:[],level:3}],level:2},{value:"Pretrain GPT-2 On Single GPU",id:"pretrain-gpt-2-on-single-gpu",children:[],level:2}],d={toc:c};function u(e){var t=e.components,n=(0,r.Z)(e,a);return(0,i.kt)("wrapper",(0,o.Z)({},d,n,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"parallelize-your-training-like-megatron-lm-via-colotensor"},"Parallelize Your Training like Megatron-LM via ColoTensor"),(0,i.kt)("p",null,"Author: ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/1SAA"},"Haichen Huang")," and ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/feifeibear"},"Jiarui Fang")),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Prerequisite:")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"/docs/basics/colotensor_concept"},"ColoTensor Concepts"))),(0,i.kt)("h2",{id:"introduction"},"Introduction"),(0,i.kt)("p",null,"Thanks to the convenience given by ColoTensor, users can apply parallelism with the least edition to their serial code.\nIn this tutorial, we will illustrate how to modify the training model to automatically adapt the code to parallel training like Megatron-LM.\nWe take the GPT-2 model offered by HuggingFace as an example and provide a way for you to pre-train the GPT-2 model on a single GPU."),(0,i.kt)("p",null,"Megatron-LM provided a profound paradigm to parallelize large transformer language models.\nHowever, in order to train large transformer language models at scale, users have to build their models with those modules provided by Megatron.\nIt imposes several difficult jobs on users, such as loading the weights from the pre-trained models and constructing the parallelized models.\nTo mitigate users' trouble, we offer ColoTensor to enable the tensor model parallelism automatically."),(0,i.kt)("h2",{id:"definitions-of-the-model-and-the-loss-function"},"Definitions of the model and the loss function"),(0,i.kt)("p",null,"First we use the GPTModel and GPTLoss directly from the HuggingFace library."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"import torch\nimport torch.nn as nn\nfrom transformers import GPT2Config, GPT2LMHeadModel\n\nclass GPTLMModel(nn.Module):\n    def __init__(self, hidden_size=768, num_layers=12, num_attention_heads=12, max_seq_len=1024, vocab_size=50257, checkpoint=False):\n        super().__init__()\n        self.checkpoint = checkpoint\n        self.model = GPT2LMHeadModel(GPT2Config(n_embd=hidden_size, n_layer=num_layers,\n                                     n_head=num_attention_heads, n_positions=max_seq_len, n_ctx=max_seq_len, vocab_size=vocab_size))\n        if checkpoint:\n            self.model.gradient_checkpointing_enable()\n\n    def forward(self, input_ids, attention_mask):\n        # Only return lm_logits\n        return self.model(input_ids=input_ids, attention_mask=attention_mask, use_cache=not self.checkpoint)[0]\n\n\nclass GPTLMLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.loss_fn = nn.CrossEntropyLoss()\n\n    def forward(self, logits, labels):\n        shift_logits = logits[..., :-1, :].contiguous()\n        shift_labels = labels[..., 1:].contiguous()\n        # Flatten the tokens\n        return self.loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n")),(0,i.kt)("h2",{id:"brief-review-of-gpt-2"},"Brief Review of GPT-2"),(0,i.kt)("p",null,"Now, we recall the structure of each GPT-2 model.\nEvery GPT-2 model can be represented as a DAG.\nAs shown in the below pictures, each circle represents an operator and each square represents a weight.\nAn arrow indicates the flow of the input data, and the notation alongside the arrow demonstrates the shape of the input data."),(0,i.kt)("p",null,"Then, let's take an insight into this GPT-2 model. It consists of three parts.\nThey are the ",(0,i.kt)("strong",{parentName:"p"},"embedding module"),", ",(0,i.kt)("strong",{parentName:"p"},"transformer layers"),", and the ",(0,i.kt)("strong",{parentName:"p"},"classification head"),"."),(0,i.kt)("p",null,"The embedding module contains two weights, token embedding weight and position embedding weight.\nAfter the forward operation of the embedding module, each word in all sequences of the raw input data will be embedded into a hidden state."),(0,i.kt)("figure",{style:{textAlign:"center"}},(0,i.kt)("img",{src:"https://s2.loli.net/2022/08/17/omfkIEN6ui5jcL3.png"}),(0,i.kt)("figcaption",null,"The embedding module")),(0,i.kt)("p",null,"Each transformer layer contains two blocks. The self-attention operation is called in the first block and a two-layer percepton is located in the second block."),(0,i.kt)("figure",{style:{textAlign:"center"}},(0,i.kt)("img",{src:"https://s2.loli.net/2022/08/17/LAVzDlpRcj4dYeb.png"}),(0,i.kt)("figcaption",null,"The transformer layer")),(0,i.kt)("p",null,"In the end, the classification head is just a linear module without bias, which only has a weight inside."),(0,i.kt)("h2",{id:"applied-with-colotensor"},"Applied with ColoTensor"),(0,i.kt)("p",null,"Two steps make your serial code adapted to Megatron-LM tensor parallel style."),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"Initialize the model in the context of ColoInitContext."),(0,i.kt)("li",{parentName:"ol"},"Setting ColoTensorSpec for each parameter.")),(0,i.kt)("h3",{id:"initialize-with-coloinitcontext"},"Initialize with ColoInitContext"),(0,i.kt)("p",null,"We should build the model in the ColoInitContext.\nIn this context, any parameter initialized would be transformed to ColoParameter and moved to the corresponded device automatically."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"from colossalai.utils.model.colo_init_context import ColoInitContext\n\nwith ColoInitContext(device=torch.device('cpu')):\n    model = GPTLMModel()\n")),(0,i.kt)("h3",{id:"setting-colotensorspec-for-each-parameter"},"Setting ColoTensorSpec for each parameter"),(0,i.kt)("p",null,"After the creation of the model, we establish the distributed environment through ProcessGroup.\nHere, we specify the degree of the tensor parallelism as the same as the number of all GPUs, which means the degree of data parallelism is 1."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"import torch.distributed as dist\nfrom colossalai.tensor import ProcessGroup\n\npg = ProcessGroup(tp_degree=dist.get_world_size())\n")),(0,i.kt)("p",null,"Now, some auxiliary functions are necessary for the next step. We define two functions to split a parameter.\nMegatron-LM-like tensor parallelism requires splitting a parameter tensor along its first dimension or its last dimension."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"from colossalai.tensor import ShardSpec, ComputeSpec, ComputePattern, ColoParameter, ProcessGroup\n\ndef split_param_single_dim_tp1d(dim: int, param: ColoParameter, pg: ProcessGroup):\n    spec = (ShardSpec([dim], [pg.tp_world_size()]), ComputeSpec(ComputePattern.TP1D))\n    if param.process_group.tp_world_size() == 1:\n        param.set_process_group(pg)\n    param.set_tensor_spec(*spec)\n\n\ndef split_param_row_tp1d(param: ColoParameter, pg: ProcessGroup):\n    split_param_single_dim_tp1d(0, param, pg)\n\n\ndef split_param_col_tp1d(param: ColoParameter, pg: ProcessGroup):\n    split_param_single_dim_tp1d(-1, param, pg)\n")),(0,i.kt)("p",null,"Then we adapt the model to the tensor parallelism.\nAccording to the tensor parallelism applied in Megatron, it is supposed to shard along the last dimension of tensors, including the weights of token embedding, position embedding, all linear weights and biases in self-attention blocks, the first weight linear and bias in each MLP.\nAnd it shards the second linear weight along its first dimension."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"for mn, module in model.named_modules():\n    for pn, param in module.named_parameters(recurse=False):\n        # set process group for all parameters\n        param.set_process_group(pg)\n\n        if 'mlp.c_fc' in mn:\n            if 'weight' in pn or 'bias' in pn:\n                split_param_col_tp1d(param, pg)  # colmn slice\n                # keep the shape of the output from c_fc\n                param.compute_spec.set_output_replicate(False)\n        elif 'mlp.c_proj' in mn:\n            if 'weight' in pn:\n                split_param_row_tp1d(param, pg)  # row slice\n        elif 'wte' in mn or 'wpe' in mn:\n            split_param_col_tp1d(param, pg)  # colmn slice\n        elif 'c_attn' in mn or 'c_proj' in mn:\n            split_param_col_tp1d(param, pg)  # colmn slice\n")),(0,i.kt)("p",null,"The modified model is illustrated below."),(0,i.kt)("p",null,"The embedding module:"),(0,i.kt)("figure",{style:{textAlign:"center"}},(0,i.kt)("img",{src:"https://s2.loli.net/2022/08/17/Yu2xzXEabHV7pwe.png"}),(0,i.kt)("figcaption",null,"The modified embedding module")),(0,i.kt)("p",null,"The transformer layers:"),(0,i.kt)("figure",{style:{textAlign:"center"}},(0,i.kt)("img",{src:"https://s2.loli.net/2022/08/17/4HWsA2xz51IhPFO.png"}),(0,i.kt)("figcaption",null,"The modified transformer layer")),(0,i.kt)("p",null,"Once users have specified the distributed pattern of each parameter, ColoTensor is capable of inferring the computation patterns of all operators, including matrix multiplication, the linear function, other elementwise functions in torch.nn.functional, etc.\nIn this way, users can train their models as usual."),(0,i.kt)("p",null,"In our latest example, a Gemini + ZeRO DDP model is also defined to reduce overhead and improve efficiency.For the details of this part, please refer to ",(0,i.kt)("a",{parentName:"p",href:"/docs/features/zero_with_chunk"},"ZeRO"),". You can combine these two parts to understand our entire training process:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'def gemini_zero_dpp(model: torch.nn.Module, pg: ProcessGroup, placememt_policy: str = "auto"):\n    from colossalai.nn.parallel import GeminiDDP\n    model = GeminiDDP(model,\n                        device=get_current_device(),\n                        placement_policy=placememt_policy,\n                        pin_memory=True,\n                        search_range_mb=32)\n    return model\n')),(0,i.kt)("h2",{id:"pretrain-gpt-2-on-single-gpu"},"Pretrain GPT-2 On Single GPU"),(0,i.kt)("p",null,"The above optimization we made allows us to pretrain the GPT-2 model on a single GPU. We only need to set the parameter ",(0,i.kt)("inlineCode",{parentName:"p"},"GPUNUM"),"=1 in ",(0,i.kt)("inlineCode",{parentName:"p"},"run.sh"),", and then we can complete the model training on a single GPU when running the file."),(0,i.kt)("p",null,"The GPT-2 example is accessible at ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/gpt"},"Train GPT with Colossal-AI"),"."))}u.isMDXComponent=!0}}]);