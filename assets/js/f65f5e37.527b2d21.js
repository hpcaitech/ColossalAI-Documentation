"use strict";(self.webpackChunkdemo=self.webpackChunkdemo||[]).push([[3716],{6999:(e,t,o)=>{o.d(t,{Cl:()=>a,Dx:()=>p,Pc:()=>r,aE:()=>l,iz:()=>n,nT:()=>c});var i=o(7294),s=o(398);o(814);function a(e){return i.createElement("div",{className:"docstring-container"},e.children)}function r(e){return i.createElement("div",{className:"signature"},"(",e.children,")")}function n(e){return i.createElement("h3",{className:"divider"},e.name)}function l(e){return i.createElement("div",null,i.createElement(n,{name:"Parameters"}),i.createElement(s.D,null,e.children))}function c(e){return i.createElement("div",null,i.createElement(n,{name:"Returns"}),i.createElement(s.D,null,`${e.name}: ${e.desc}`))}function p(e){return i.createElement("div",{className:"title-container"},i.createElement("div",{className:"title-module"},i.createElement("h3",null,e.type),"\xa0 ",i.createElement("h2",null,e.name)),i.createElement("div",{className:"title-source"},"<",i.createElement("a",{href:e.source,className:"title-source"},"source"),">"))}},4925:(e,t,o)=>{o.r(t),o.d(t,{assets:()=>c,contentTitle:()=>n,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>p});var i=o(7462),s=(o(7294),o(3905)),a=o(6999);const r={},n="Booster Checkpoint",l={unversionedId:"basics/booster_checkpoint",id:"basics/booster_checkpoint",title:"Booster Checkpoint",description:"Author: Hongxin Liu",source:"@site/i18n/en/docusaurus-plugin-content-docs/current/basics/booster_checkpoint.md",sourceDirName:"basics",slug:"/basics/booster_checkpoint",permalink:"/docs/basics/booster_checkpoint",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/basics/booster_checkpoint.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Booster Plugins",permalink:"/docs/basics/booster_plugins"},next:{title:"Define Your Configuration",permalink:"/docs/basics/define_your_config"}},c={},p=[{value:"Introduction",id:"introduction",level:2},{value:"Model Checkpoint",id:"model-checkpoint",level:2},{value:"Optimizer Checkpoint",id:"optimizer-checkpoint",level:2},{value:"LR Scheduler Checkpoint",id:"lr-scheduler-checkpoint",level:2},{value:"Checkpoint design",id:"checkpoint-design",level:2}],d={toc:p},h="wrapper";function u(e){let{components:t,...o}=e;return(0,s.kt)(h,(0,i.Z)({},d,o,{components:t,mdxType:"MDXLayout"}),(0,s.kt)("h1",{id:"booster-checkpoint"},"Booster Checkpoint"),(0,s.kt)("p",null,"Author: ",(0,s.kt)("a",{parentName:"p",href:"https://github.com/ver217"},"Hongxin Liu")),(0,s.kt)("p",null,(0,s.kt)("strong",{parentName:"p"},"Prerequisite:")),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"/docs/basics/booster_api"},"Booster API"))),(0,s.kt)("h2",{id:"introduction"},"Introduction"),(0,s.kt)("p",null,"We've introduced the ",(0,s.kt)("a",{parentName:"p",href:"/docs/basics/booster_api"},"Booster API")," in the previous tutorial. In this tutorial, we will introduce how to save and load checkpoints using booster."),(0,s.kt)("h2",{id:"model-checkpoint"},"Model Checkpoint"),(0,s.kt)(a.Cl,{mdxType:"DocStringContainer"},(0,s.kt)("div",null,(0,s.kt)(a.Dx,{type:"",name:"colossalai.booster.Booster.save_model",source:"https://github.com/hpcaitech/ColossalAI/blob/main/src/colossalai/booster/booster.py#L180",mdxType:"Title"}),(0,s.kt)(a.Pc,{mdxType:"Signature"},"model: Module, checkpoint: str, prefix: str = None, shard: bool = False, size_per_shard: int = 1024"),(0,s.kt)(a.aE,{mdxType:"Parameters"},"- **model** (nn.Module) -- A model boosted by Booster.\n- **checkpoint** (str) -- Path to the checkpoint. It must be a local path.\n  It is a file path if `shard=False`. Otherwise, it is a directory path.\n- **prefix** (str, optional) -- A prefix added to parameter and buffer\n  names to compose the keys in state_dict. Defaults to None.\n- **shard** (bool, optional) -- Whether to save checkpoint a sharded way.\n  If true, the checkpoint will be a folder. Otherwise, it will be a single file. Defaults to False.\n- **size_per_shard** (int, optional) -- Maximum size of checkpoint shard file in MB. This is useful only when `shard=True`. Defaults to 1024.")),(0,s.kt)("div",null,(0,s.kt)(a.iz,{name:"Doc",mdxType:"Divider"}),"Save model to checkpoint.")),(0,s.kt)("p",null,"Model must be boosted by ",(0,s.kt)("inlineCode",{parentName:"p"},"colossalai.booster.Booster")," before saving. ",(0,s.kt)("inlineCode",{parentName:"p"},"checkpoint")," is the path to saved checkpoint. It can be a file, if ",(0,s.kt)("inlineCode",{parentName:"p"},"shard=False"),". Otherwise, it should be a directory. If ",(0,s.kt)("inlineCode",{parentName:"p"},"shard=True"),", the checkpoint will be saved in a sharded way. This is useful when the checkpoint is too large to be saved in a single file. Our sharded checkpoint format is compatible with ",(0,s.kt)("a",{parentName:"p",href:"https://github.com/huggingface/transformers"},"huggingface/transformers"),"."),(0,s.kt)(a.Cl,{mdxType:"DocStringContainer"},(0,s.kt)("div",null,(0,s.kt)(a.Dx,{type:"",name:"colossalai.booster.Booster.load_model",source:"https://github.com/hpcaitech/ColossalAI/blob/main/src/colossalai/booster/booster.py#L167",mdxType:"Title"}),(0,s.kt)(a.Pc,{mdxType:"Signature"},"model: Module, checkpoint: str, strict: bool = True"),(0,s.kt)(a.aE,{mdxType:"Parameters"},"- **model** (nn.Module) -- A model boosted by Booster.\n- **checkpoint** (str) -- Path to the checkpoint. It must be a local path.\n  It should be a directory path if the checkpoint is sharded. Otherwise, it should be a file path.\n- **strict** (bool, optional) -- whether to strictly enforce that the keys\n  in :attr:*state_dict* match the keys returned by this module's\n  [`~torch.nn.Module.state_dict`] function. Defaults to True.")),(0,s.kt)("div",null,(0,s.kt)(a.iz,{name:"Doc",mdxType:"Divider"}),"Load model from checkpoint.")),(0,s.kt)("p",null,"Model must be boosted by ",(0,s.kt)("inlineCode",{parentName:"p"},"colossalai.booster.Booster")," before loading. It will detect the checkpoint format automatically, and load in corresponding way."),(0,s.kt)("h2",{id:"optimizer-checkpoint"},"Optimizer Checkpoint"),(0,s.kt)("blockquote",null,(0,s.kt)("p",{parentName:"blockquote"},"\u26a0 Saving optimizer checkpoint in a sharded way is not supported yet.")),(0,s.kt)(a.Cl,{mdxType:"DocStringContainer"},(0,s.kt)("div",null,(0,s.kt)(a.Dx,{type:"",name:"colossalai.booster.Booster.save_optimizer",source:"https://github.com/hpcaitech/ColossalAI/blob/main/src/colossalai/booster/booster.py#L210",mdxType:"Title"}),(0,s.kt)(a.Pc,{mdxType:"Signature"},"optimizer: Optimizer, checkpoint: str, shard: bool = False, size_per_shard: int = 1024"),(0,s.kt)(a.aE,{mdxType:"Parameters"},"- **optimizer** (Optimizer) -- An optimizer boosted by Booster.\n- **checkpoint** (str) -- Path to the checkpoint. It must be a local path.\n  It is a file path if `shard=False`. Otherwise, it is a directory path.\n- **shard** (bool, optional) -- Whether to save checkpoint a sharded way.\n  If true, the checkpoint will be a folder. Otherwise, it will be a single file. Defaults to False.\n- **size_per_shard** (int, optional) -- Maximum size of checkpoint shard file in MB. This is useful only when `shard=True`. Defaults to 1024.")),(0,s.kt)("div",null,(0,s.kt)(a.iz,{name:"Doc",mdxType:"Divider"}),"Save optimizer to checkpoint. Warning: Saving sharded optimizer checkpoint is not supported yet.")),(0,s.kt)("p",null,"Optimizer must be boosted by ",(0,s.kt)("inlineCode",{parentName:"p"},"colossalai.booster.Booster")," before saving."),(0,s.kt)(a.Cl,{mdxType:"DocStringContainer"},(0,s.kt)("div",null,(0,s.kt)(a.Dx,{type:"",name:"colossalai.booster.Booster.load_optimizer",source:"https://github.com/hpcaitech/ColossalAI/blob/main/src/colossalai/booster/booster.py#L200",mdxType:"Title"}),(0,s.kt)(a.Pc,{mdxType:"Signature"},"optimizer: Optimizer, checkpoint: str"),(0,s.kt)(a.aE,{mdxType:"Parameters"},"- **optimizer** (Optimizer) -- An optimizer boosted by Booster.\n- **checkpoint** (str) -- Path to the checkpoint. It must be a local path.\n  It should be a directory path if the checkpoint is sharded. Otherwise, it should be a file path.")),(0,s.kt)("div",null,(0,s.kt)(a.iz,{name:"Doc",mdxType:"Divider"}),"Load optimizer from checkpoint.")),(0,s.kt)("p",null,"Optimizer must be boosted by ",(0,s.kt)("inlineCode",{parentName:"p"},"colossalai.booster.Booster")," before loading."),(0,s.kt)("h2",{id:"lr-scheduler-checkpoint"},"LR Scheduler Checkpoint"),(0,s.kt)(a.Cl,{mdxType:"DocStringContainer"},(0,s.kt)("div",null,(0,s.kt)(a.Dx,{type:"",name:"colossalai.booster.Booster.save_lr_scheduler",source:"https://github.com/hpcaitech/ColossalAI/blob/main/src/colossalai/booster/booster.py#L224",mdxType:"Title"}),(0,s.kt)(a.Pc,{mdxType:"Signature"},"lr_scheduler: _LRScheduler, checkpoint: str"),(0,s.kt)(a.aE,{mdxType:"Parameters"},"- **lr_scheduler** (LRScheduler) -- A lr scheduler boosted by Booster.\n- **checkpoint** (str) -- Path to the checkpoint. It must be a local file path.")),(0,s.kt)("div",null,(0,s.kt)(a.iz,{name:"Doc",mdxType:"Divider"}),"Save lr scheduler to checkpoint.")),(0,s.kt)("p",null,"LR scheduler must be boosted by ",(0,s.kt)("inlineCode",{parentName:"p"},"colossalai.booster.Booster")," before saving. ",(0,s.kt)("inlineCode",{parentName:"p"},"checkpoint")," is the local path to checkpoint file."),(0,s.kt)(a.Cl,{mdxType:"DocStringContainer"},(0,s.kt)("div",null,(0,s.kt)(a.Dx,{type:"",name:"colossalai.booster.Booster.load_lr_scheduler",source:"https://github.com/hpcaitech/ColossalAI/blob/main/src/colossalai/booster/booster.py#L233",mdxType:"Title"}),(0,s.kt)(a.Pc,{mdxType:"Signature"},"lr_scheduler: _LRScheduler, checkpoint: str"),(0,s.kt)(a.aE,{mdxType:"Parameters"},"- **lr_scheduler** (LRScheduler) -- A lr scheduler boosted by Booster.\n- **checkpoint** (str) -- Path to the checkpoint. It must be a local file path.")),(0,s.kt)("div",null,(0,s.kt)(a.iz,{name:"Doc",mdxType:"Divider"}),"Load lr scheduler from checkpoint.")),(0,s.kt)("p",null,"LR scheduler must be boosted by ",(0,s.kt)("inlineCode",{parentName:"p"},"colossalai.booster.Booster")," before loading. ",(0,s.kt)("inlineCode",{parentName:"p"},"checkpoint")," is the local path to checkpoint file."),(0,s.kt)("h2",{id:"checkpoint-design"},"Checkpoint design"),(0,s.kt)("p",null,"More details about checkpoint design can be found in our discussion ",(0,s.kt)("a",{parentName:"p",href:"https://github.com/hpcaitech/ColossalAI/discussions/3339"},"A Unified Checkpoint System Design"),"."))}u.isMDXComponent=!0}}]);