"use strict";(self.webpackChunkdemo=self.webpackChunkdemo||[]).push([[6584],{3905:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>h});var r=n(7294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,r,a=function(e,t){if(null==e)return{};var n,r,a={},i=Object.keys(e);for(r=0;r<i.length;r++)n=i[r],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)n=i[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var l=r.createContext({}),p=function(e){var t=r.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},c=function(e){var t=p(e.components);return r.createElement(l.Provider,{value:t},e.children)},u="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},m=r.forwardRef((function(e,t){var n=e.components,a=e.mdxType,i=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),u=p(n),m=a,h=u["".concat(l,".").concat(m)]||u[m]||d[m]||i;return n?r.createElement(h,o(o({ref:t},c),{},{components:n})):r.createElement(h,o({ref:t},c))}));function h(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var i=n.length,o=new Array(i);o[0]=m;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[u]="string"==typeof e?e:a,o[1]=s;for(var p=2;p<i;p++)o[p]=n[p];return r.createElement.apply(null,o)}return r.createElement.apply(null,n)}m.displayName="MDXCreateElement"},1872:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>i,metadata:()=>s,toc:()=>p});var r=n(7462),a=(n(7294),n(3905));const i={},o="Gradient Accumulation",s={unversionedId:"features/gradient_accumulation_with_booster",id:"features/gradient_accumulation_with_booster",title:"Gradient Accumulation",description:"Author: Mingyan Jiang",source:"@site/i18n/en/docusaurus-plugin-content-docs/current/features/gradient_accumulation_with_booster.md",sourceDirName:"features",slug:"/features/gradient_accumulation_with_booster",permalink:"/docs/features/gradient_accumulation_with_booster",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/features/gradient_accumulation_with_booster.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Auto Mixed Precision Training",permalink:"/docs/features/mixed_precision_training_with_booster"},next:{title:"Gradient Clipping",permalink:"/docs/features/gradient_clipping_with_booster"}},l={},p=[{value:"Introduction",id:"introduction",level:2},{value:"Usage",id:"usage",level:2},{value:"Hands-on Practice",id:"hands-on-practice",level:2},{value:"Step 1. Import libraries in train.py",id:"step-1-import-libraries-in-trainpy",level:3},{value:"Step 2. Initialize Distributed Environment",id:"step-2-initialize-distributed-environment",level:3},{value:"Step 3. Create training components",id:"step-3-create-training-components",level:3},{value:"Step 4. Inject Feature",id:"step-4-inject-feature",level:3},{value:"Step 5. Train with Booster",id:"step-5-train-with-booster",level:3},{value:"Step 6. Invoke Training Scripts",id:"step-6-invoke-training-scripts",level:3}],c={toc:p},u="wrapper";function d(e){let{components:t,...n}=e;return(0,a.kt)(u,(0,r.Z)({},c,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"gradient-accumulation"},"Gradient Accumulation"),(0,a.kt)("p",null,"Author: ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/jiangmingyan"},"Mingyan Jiang")),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"Prerequisite")),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"/docs/basics/booster_api"},"Training Booster"))),(0,a.kt)("h2",{id:"introduction"},"Introduction"),(0,a.kt)("p",null,"Gradient accumulation is a common way to enlarge your batch size for training. When training large-scale models, memory can easily become the bottleneck and the batch size can be very small, (e.g. 2), leading to unsatisfactory convergence. Gradient accumulation works by adding up the gradients calculated in multiple iterations, and only update the parameters in the preset iteration."),(0,a.kt)("h2",{id:"usage"},"Usage"),(0,a.kt)("p",null,"It is simple to use gradient accumulation in Colossal-AI. Just call ",(0,a.kt)("inlineCode",{parentName:"p"},"booster.no_sync()")," which returns a context manager. It accumulate gradients without synchronization, meanwhile you should not update the weights."),(0,a.kt)("h2",{id:"hands-on-practice"},"Hands-on Practice"),(0,a.kt)("p",null,"We now demonstrate gradient accumulation. In this example, we let the gradient accumulation size to be 4."),(0,a.kt)("h3",{id:"step-1-import-libraries-in-trainpy"},"Step 1. Import libraries in train.py"),(0,a.kt)("p",null,"Create a ",(0,a.kt)("inlineCode",{parentName:"p"},"train.py")," and import the necessary dependencies. The version of ",(0,a.kt)("inlineCode",{parentName:"p"},"torch")," should not be lower than 1.8.1."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"import os\nfrom pathlib import Path\n\nimport torch\nfrom torchvision import transforms\nfrom torchvision.datasets import CIFAR10\nfrom torchvision.models import resnet18\nfrom torch.utils.data import DataLoader\n\nimport colossalai\nfrom colossalai.booster import Booster\nfrom colossalai.booster.plugin import TorchDDPPlugin\nfrom colossalai.logging import get_dist_logger\nfrom colossalai.cluster.dist_coordinator import priority_execution\n")),(0,a.kt)("h3",{id:"step-2-initialize-distributed-environment"},"Step 2. Initialize Distributed Environment"),(0,a.kt)("p",null,"We then need to initialize distributed environment. For demo purpose, we uses ",(0,a.kt)("inlineCode",{parentName:"p"},"launch_from_torch"),". You can refer to ",(0,a.kt)("a",{parentName:"p",href:"/docs/basics/launch_colossalai"},"Launch Colossal-AI")," for other initialization methods."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"# initialize distributed setting\nparser = colossalai.get_default_parser()\nargs = parser.parse_args()\n# launch from torch\ncolossalai.launch_from_torch(config=dict())\n")),(0,a.kt)("h3",{id:"step-3-create-training-components"},"Step 3. Create training components"),(0,a.kt)("p",null,"Build your model, optimizer, loss function, lr scheduler and dataloaders. Note that the root path of the dataset is obtained from the environment variable ",(0,a.kt)("inlineCode",{parentName:"p"},"DATA"),". You may ",(0,a.kt)("inlineCode",{parentName:"p"},"export DATA=/path/to/data")," or change ",(0,a.kt)("inlineCode",{parentName:"p"},"Path(os.environ['DATA'])")," to a path on your machine. Data will be automatically downloaded to the root path."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"# define the training hyperparameters\nBATCH_SIZE = 128\nGRADIENT_ACCUMULATION = 4\n\n# build resnet\nmodel = resnet18(num_classes=10)\n\n# build dataloaders\nwith priority_execution():\n    train_dataset = CIFAR10(root=Path(os.environ.get('DATA', './data')),\n                            download=True,\n                            transform=transforms.Compose([\n                                transforms.RandomCrop(size=32, padding=4),\n                                transforms.RandomHorizontalFlip(),\n                                transforms.ToTensor(),\n                                transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010]),\n                            ]))\n\n# build criterion\ncriterion = torch.nn.CrossEntropyLoss()\n\n# optimizer\noptimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n")),(0,a.kt)("h3",{id:"step-4-inject-feature"},"Step 4. Inject Feature"),(0,a.kt)("p",null,"Create a ",(0,a.kt)("inlineCode",{parentName:"p"},"TorchDDPPlugin")," object to instantiate a ",(0,a.kt)("inlineCode",{parentName:"p"},"Booster"),", and boost these training components."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"plugin = TorchDDPPlugin()\nbooster = Booster(plugin=plugin)\ntrain_dataloader = plugin.prepare_dataloader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\nmodel, optimizer, criterion, train_dataloader, _ = booster.boost(model=model,\n                                                                    optimizer=optimizer,\n                                                                    criterion=criterion,\n                                                                    dataloader=train_dataloader)\n")),(0,a.kt)("h3",{id:"step-5-train-with-booster"},"Step 5. Train with Booster"),(0,a.kt)("p",null,"Use booster in a normal training loops, and verify gradient accumulation. ",(0,a.kt)("inlineCode",{parentName:"p"},"param_by_iter")," is to record the distributed training information."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"optimizer.zero_grad()\nfor idx, (img, label) in enumerate(train_dataloader):\n        sync_context = booster.no_sync(model)\n        img = img.cuda()\n        label = label.cuda()\n        if idx % (GRADIENT_ACCUMULATION - 1) != 0:\n            with sync_context:\n                output = model(img)\n                train_loss = criterion(output, label)\n                train_loss = train_loss / GRADIENT_ACCUMULATION\n                booster.backward(train_loss, optimizer)\n        else:\n            output = model(img)\n            train_loss = criterion(output, label)\n            train_loss = train_loss / GRADIENT_ACCUMULATION\n            booster.backward(train_loss, optimizer)\n            optimizer.step()\n            optimizer.zero_grad()\n\n        ele_1st = next(model.parameters()).flatten()[0]\n        param_by_iter.append(str(ele_1st.item()))\n\n        if idx != 0 and idx % (GRADIENT_ACCUMULATION - 1) == 0:\n            break\n\n    for iteration, val in enumerate(param_by_iter):\n        print(f'iteration {iteration} - value: {val}')\n\n    if param_by_iter[-1] != param_by_iter[0]:\n        print('The parameter is only updated in the last iteration')\n\n")),(0,a.kt)("h3",{id:"step-6-invoke-training-scripts"},"Step 6. Invoke Training Scripts"),(0,a.kt)("p",null,"To verify gradient accumulation, we can just check the change of parameter values. When gradient accumulation is set, parameters are only updated in the last step. You can run the script using this command:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell"},"colossalai run --nproc_per_node 1 train.py\n")),(0,a.kt)("p",null,"You will see output similar to the text below. This shows gradient is indeed accumulated as the parameter is not updated\nin the first 3 steps, but only updated in the last step."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-text"},"iteration 0, first 10 elements of param: tensor([-0.0208,  0.0189,  0.0234,  0.0047,  0.0116, -0.0283,  0.0071, -0.0359, -0.0267, -0.0006], device='cuda:0', grad_fn=<SliceBackward0>)\niteration 1, first 10 elements of param: tensor([-0.0208,  0.0189,  0.0234,  0.0047,  0.0116, -0.0283,  0.0071, -0.0359, -0.0267, -0.0006], device='cuda:0', grad_fn=<SliceBackward0>)\niteration 2, first 10 elements of param: tensor([-0.0208,  0.0189,  0.0234,  0.0047,  0.0116, -0.0283,  0.0071, -0.0359, -0.0267, -0.0006], device='cuda:0', grad_fn=<SliceBackward0>)\niteration 3, first 10 elements of param: tensor([-0.0141,  0.0464,  0.0507,  0.0321,  0.0356, -0.0150,  0.0172, -0.0118, 0.0222,  0.0473], device='cuda:0', grad_fn=<SliceBackward0>)\n")))}d.isMDXComponent=!0}}]);