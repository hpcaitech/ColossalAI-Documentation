"use strict";(self.webpackChunkdemo=self.webpackChunkdemo||[]).push([[6609],{3905:(e,n,t)=>{t.d(n,{Zo:()=>d,kt:()=>c});var a=t(7294);function i(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function r(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,a)}return t}function o(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?r(Object(t),!0).forEach((function(n){i(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):r(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function l(e,n){if(null==e)return{};var t,a,i=function(e,n){if(null==e)return{};var t,a,i={},r=Object.keys(e);for(a=0;a<r.length;a++)t=r[a],n.indexOf(t)>=0||(i[t]=e[t]);return i}(e,n);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)t=r[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(i[t]=e[t])}return i}var s=a.createContext({}),p=function(e){var n=a.useContext(s),t=n;return e&&(t="function"==typeof e?e(n):o(o({},n),e)),t},d=function(e){var n=p(e.components);return a.createElement(s.Provider,{value:n},e.children)},m="mdxType",u={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},_=a.forwardRef((function(e,n){var t=e.components,i=e.mdxType,r=e.originalType,s=e.parentName,d=l(e,["components","mdxType","originalType","parentName"]),m=p(t),_=i,c=m["".concat(s,".").concat(_)]||m[_]||u[_]||r;return t?a.createElement(c,o(o({ref:n},d),{},{components:t})):a.createElement(c,o({ref:n},d))}));function c(e,n){var t=arguments,i=n&&n.mdxType;if("string"==typeof e||i){var r=t.length,o=new Array(r);o[0]=_;var l={};for(var s in n)hasOwnProperty.call(n,s)&&(l[s]=n[s]);l.originalType=e,l[m]="string"==typeof e?e:i,o[1]=l;for(var p=2;p<r;p++)o[p]=t[p];return a.createElement.apply(null,o)}return a.createElement.apply(null,t)}_.displayName="MDXCreateElement"},2936:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>s,contentTitle:()=>o,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>p});var a=t(7462),i=(t(7294),t(3905));const r={},o="Train GPT Using Hybrid Parallelism",l={unversionedId:"advanced_tutorials/train_gpt_using_hybrid_parallelism",id:"version-v0.2.4/advanced_tutorials/train_gpt_using_hybrid_parallelism",title:"Train GPT Using Hybrid Parallelism",description:"Author: Hongxin Liu, Yongbin Li",source:"@site/i18n/en/docusaurus-plugin-content-docs/version-v0.2.4/advanced_tutorials/train_gpt_using_hybrid_parallelism.md",sourceDirName:"advanced_tutorials",slug:"/advanced_tutorials/train_gpt_using_hybrid_parallelism",permalink:"/docs/advanced_tutorials/train_gpt_using_hybrid_parallelism",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/versioned_docs/version-v0.2.4/advanced_tutorials/train_gpt_using_hybrid_parallelism.md",tags:[],version:"v0.2.4",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Step By Step: Accelerate ViT Training With Colossal-AI (From Data Parallel to Hybrid Parallel)",permalink:"/docs/advanced_tutorials/train_vit_with_hybrid_parallelism"},next:{title:"Define your own parallel model",permalink:"/docs/advanced_tutorials/define_your_own_parallel_model"}},s={},p=[{value:"Introduction",id:"introduction",level:2},{value:"Table of content",id:"table-of-content",level:2},{value:"Import libraries",id:"import-libraries",level:2},{value:"Define GPT model",id:"define-gpt-model",level:2},{value:"Process the dataset",id:"process-the-dataset",level:2},{value:"Training GPT using hybrid parallelism",id:"training-gpt-using-hybrid-parallelism",level:2}],d={toc:p},m="wrapper";function u(e){let{components:n,...t}=e;return(0,i.kt)(m,(0,a.Z)({},d,t,{components:n,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"train-gpt-using-hybrid-parallelism"},"Train GPT Using Hybrid Parallelism"),(0,i.kt)("p",null,"Author: Hongxin Liu, Yongbin Li"),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Example Code")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://github.com/hpcaitech/ColossalAI-Examples/tree/main/language/gpt_2"},"ColossalAI-Examples GPT2")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://github.com/hpcaitech/ColossalAI-Examples/tree/main/language/gpt_3"},"ColossalAI-Examples GPT3"))),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Related Paper")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2110.14883"},"Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2104.04473"},"Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM"))),(0,i.kt)("h2",{id:"introduction"},"Introduction"),(0,i.kt)("p",null,"In the previous tutorial, we introduce how to train ViT with pipeline. In this tutorial, you will learn a more complex scenario -- train GPT with hybrid parallelism. In this case, GPT-3 is so large that CPU memory cannot fit it as well. Therefore, you must split the model by yourself."),(0,i.kt)("h2",{id:"table-of-content"},"Table of content"),(0,i.kt)("p",null,"In this tutorial we will cover:"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"The definition of GPT model, based on colossalai/model_zoo"),(0,i.kt)("li",{parentName:"ol"},"Processing the dataset"),(0,i.kt)("li",{parentName:"ol"},"Training GPT using hybrid parallelism")),(0,i.kt)("h2",{id:"import-libraries"},"Import libraries"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"import json\nimport os\nfrom typing import Callable\n\nimport colossalai\nimport colossalai.utils as utils\nimport model_zoo.gpt.gpt as col_gpt\nimport torch\nimport torch.nn as nn\nfrom colossalai import nn as col_nn\nfrom colossalai.amp import AMP_TYPE\nfrom colossalai.builder.pipeline import partition_uniform\nfrom colossalai.context.parallel_mode import ParallelMode\nfrom colossalai.core import global_context as gpc\nfrom colossalai.engine.schedule import (InterleavedPipelineSchedule,\n                                        PipelineSchedule)\nfrom colossalai.logging import disable_existing_loggers, get_dist_logger\nfrom colossalai.nn.layer.wrapper import PipelineSharedModuleWrapper\nfrom colossalai.trainer import Trainer, hooks\nfrom colossalai.utils.timer import MultiTimer\nfrom model_zoo.gpt import GPTLMLoss\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset\nfrom transformers import GPT2Tokenizer\n")),(0,i.kt)("h2",{id:"define-gpt-model"},"Define GPT model"),(0,i.kt)("p",null,"In the previous tutorial, we introduced 3 ways to build a pipelined model. But for huge models like GPT-3, you can't even build the model in CPU. In this case, you must split the model by yourself."),(0,i.kt)("p",null,"GPT dataloader returns ",(0,i.kt)("inlineCode",{parentName:"p"},"input_ids")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"attention_mask"),", so we use two keyword arguments in ",(0,i.kt)("inlineCode",{parentName:"p"},"forward()")," to get them. Note that for stages except the first stage, the first positional argument of ",(0,i.kt)("inlineCode",{parentName:"p"},"forward()")," is the output tensor from the previous stage. So the ",(0,i.kt)("inlineCode",{parentName:"p"},"hidden_states")," is from the previous stage, and for the first stage it's ",(0,i.kt)("inlineCode",{parentName:"p"},"None"),"."),(0,i.kt)("p",null,"For GPT, the ",(0,i.kt)("em",{parentName:"p"},"word embedding layer")," shares the weights with the ",(0,i.kt)("em",{parentName:"p"},"output head"),". We provide ",(0,i.kt)("inlineCode",{parentName:"p"},"PipelineSharedModuleWrapper")," to share parameters among pipeline stages. It takes a ",(0,i.kt)("inlineCode",{parentName:"p"},"list")," of ",(0,i.kt)("inlineCode",{parentName:"p"},"int")," as argument, which means those ranks share the parameters. You can use ",(0,i.kt)("inlineCode",{parentName:"p"},"register_module()")," or ",(0,i.kt)("inlineCode",{parentName:"p"},"register_parameter()")," to register a module or a parameter as the shared module or parameter. If you have multiple sets of shared modules / parameters, you should have multiple ",(0,i.kt)("inlineCode",{parentName:"p"},"PipelineSharedModuleWrapper")," instance. If the parameter is shared within ",(0,i.kt)("strong",{parentName:"p"},"one")," stage, you should not use ",(0,i.kt)("inlineCode",{parentName:"p"},"PipelineSharedModuleWrapper"),", and just use the same module / parameter instance. In this example, the ",(0,i.kt)("em",{parentName:"p"},"word embedding layer")," is at the first stage, and the ",(0,i.kt)("em",{parentName:"p"},"output head")," is at the last stage. Thus, they are shared among ranks ",(0,i.kt)("inlineCode",{parentName:"p"},"[0, pipeline_size - 1]"),"."),(0,i.kt)("p",null,"For the first stage, it maintains the embedding layer and some transformer blocks. For the last stage, it maintains some transformer blocks and the output head layer. For other stages, they just maintain some transformer blocks. ",(0,i.kt)("inlineCode",{parentName:"p"},"partition_uniform(num_layers, pipeline_size, num_chunks)")," returns the parts of all ranks, and the part is a ",(0,i.kt)("inlineCode",{parentName:"p"},"tuple")," of ",(0,i.kt)("inlineCode",{parentName:"p"},"(start, end)")," (exclude end). ",(0,i.kt)("inlineCode",{parentName:"p"},"start == 0")," means that it's the first stage, and ",(0,i.kt)("inlineCode",{parentName:"p"},"end == num_layers")," means it's the last stage."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"class PipelineGPTHybrid(nn.Module):\n    def __init__(self,\n                 num_layers: int = 12,\n                 hidden_size: int = 768,\n                 num_attention_heads: int = 12,\n                 vocab_size: int = 50304,\n                 embed_drop_rate: float = 0.,\n                 act_func: Callable = F.gelu,\n                 mlp_ratio: int = 4,\n                 attn_drop_rate: float = 0.,\n                 drop_rate: float = 0.,\n                 dtype: torch.dtype = torch.float,\n                 checkpoint: bool = False,\n                 max_position_embeddings: int = 1024,\n                 layer_norm_epsilon: float = 1e-5,\n                 first: bool = False,\n                 last: bool = False):\n        super().__init__()\n        self.embedding = None\n        self.norm = None\n        self.head = None\n        if first:\n            self.embedding = col_gpt.GPTEmbedding(\n                hidden_size, vocab_size, max_position_embeddings, dropout=embed_drop_rate, dtype=dtype)\n        self.blocks = nn.ModuleList([\n            col_gpt.GPTBlock(hidden_size, num_attention_heads, mlp_ratio=mlp_ratio, attention_dropout=attn_drop_rate,\n                             dropout=drop_rate, dtype=dtype, checkpoint=checkpoint, activation=act_func)\n            for _ in range(num_layers)\n        ])\n        if last:\n            self.norm = col_nn.LayerNorm(hidden_size, eps=layer_norm_epsilon)\n            self.head = col_gpt.GPTLMHead(vocab_size=vocab_size,\n                                          dim=hidden_size,\n                                          dtype=dtype,\n                                          bias=False)\n\n    def forward(self, hidden_states=None, input_ids=None, attention_mask=None):\n        if self.embedding is not None:\n            hidden_states = self.embedding(input_ids=input_ids)\n        batch_size = hidden_states.shape[0]\n        attention_mask = attention_mask.view(batch_size, -1)\n        attention_mask = attention_mask[:, None, None, :]\n        attention_mask = attention_mask.to(dtype=hidden_states.dtype)  # fp16 compatibility\n        attention_mask = (1.0 - attention_mask) * -10000.0\n        for block in self.blocks:\n            hidden_states, attention_mask = block(hidden_states, attention_mask)\n        if self.norm is not None:\n            hidden_states = self.head(self.norm(hidden_states))\n        return hidden_states\n\n\ndef build_gpt_pipeline(num_layers, num_chunks, device=torch.device('cuda'), **kwargs):\n    logger = get_dist_logger()\n    pipeline_size = gpc.get_world_size(ParallelMode.PIPELINE)\n    pipeline_rank = gpc.get_local_rank(ParallelMode.PIPELINE)\n    rank = gpc.get_global_rank()\n    wrapper = PipelineSharedModuleWrapper([0, pipeline_size - 1])\n    parts = partition_uniform(num_layers, pipeline_size, num_chunks)[pipeline_rank]\n    models = []\n    for start, end in parts:\n        kwargs['num_layers'] = end - start\n        kwargs['first'] = start == 0\n        kwargs['last'] = end == num_layers\n        logger.info(f'Rank{rank} build layer {start}-{end}, {end-start}/{num_layers} layers')\n        chunk = PipelineGPTHybrid(**kwargs).to(device)\n        if start == 0:\n            wrapper.register_module(chunk.embedding.word_embeddings)\n        elif end == num_layers:\n            wrapper.register_module(chunk.head)\n        models.append(chunk)\n    if len(models) == 1:\n        model = models[0]\n    else:\n        model = nn.ModuleList(models)\n    return model\n\n\ndef GPT2_exlarge_pipeline_hybrid(num_chunks=1, checkpoint=False, dtype=torch.float):\n    cfg = dict(hidden_size=1600, num_attention_heads=32, checkpoint=checkpoint, dtype=dtype)\n    return build_gpt_pipeline(48, num_chunks, **cfg)\n\n\ndef GPT3_pipeline_hybrid(num_chunks=1, checkpoint=False, dtype=torch.float):\n    cfg = dict(hidden_size=12288, num_attention_heads=96,\n               checkpoint=checkpoint, max_position_embeddings=2048, dtype=dtype)\n    return build_gpt_pipeline(96, num_chunks, **cfg)\n")),(0,i.kt)("h2",{id:"process-the-dataset"},"Process the dataset"),(0,i.kt)("p",null,"We provide a small GPT web-text dataset here. The original format is loose JSON, and we will save the processed dataset."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"class WebtextDataset(Dataset):\n    def __init__(self, path, seq_len=1024) -> None:\n        super().__init__()\n        root = os.path.dirname(path)\n        encoded_data_cache_path = os.path.join(root, f'gpt_webtext_{seq_len}.pt')\n        if os.path.isfile(encoded_data_cache_path):\n            seq_len_, data, attention_mask = torch.load(\n                encoded_data_cache_path)\n            if seq_len_ == seq_len:\n                self.data = data\n                self.attention_mask = attention_mask\n                return\n        raw_data = []\n        with open(path) as f:\n            for line in f.readlines():\n                raw_data.append(json.loads(line)['text'])\n        tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n        tokenizer.pad_token = tokenizer.unk_token\n        encoded_data = tokenizer(\n            raw_data, padding=True, truncation=True, max_length=seq_len, return_tensors='pt')\n        self.data = encoded_data['input_ids']\n        self.attention_mask = encoded_data['attention_mask']\n        torch.save((seq_len, self.data, self.attention_mask),\n                   encoded_data_cache_path)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        return {\n            'input_ids': self.data[index],\n            'attention_mask': self.attention_mask[index]\n        }, self.data[index]\n")),(0,i.kt)("h2",{id:"training-gpt-using-hybrid-parallelism"},"Training GPT using hybrid parallelism"),(0,i.kt)("p",null,"In the previous tutorial, we explained the meanings of some pipeline arguments. In this case, we can determine the shape of each output tensor which is exchanged among pipeline stages. For GPT, the shape is ",(0,i.kt)("inlineCode",{parentName:"p"},"(MICRO BATCH SIZE, SEQUENCE LEN, HIDDEN SIZE)"),". By setting this, we can avoid exchanging the tensor shape of each stage. When you are not sure of the tensor shape, you can just  leave it ",(0,i.kt)("inlineCode",{parentName:"p"},"None"),", and the shape is inferred automatically. Make sure that the ",(0,i.kt)("inlineCode",{parentName:"p"},"dtype")," of your model is correct. When you use ",(0,i.kt)("inlineCode",{parentName:"p"},"fp16"),", the ",(0,i.kt)("inlineCode",{parentName:"p"},"dtype")," of your model must be ",(0,i.kt)("inlineCode",{parentName:"p"},"torch.half"),". Otherwise, the ",(0,i.kt)("inlineCode",{parentName:"p"},"dtype")," must be ",(0,i.kt)("inlineCode",{parentName:"p"},"torch.float"),". For pipeline parallelism, only ",(0,i.kt)("inlineCode",{parentName:"p"},"AMP_TYPE.NAIVE")," is supported."),(0,i.kt)("p",null,"You can easily use tensor parallel by setting ",(0,i.kt)("inlineCode",{parentName:"p"},"parallel")," in ",(0,i.kt)("inlineCode",{parentName:"p"},"CONFIG"),". The data parallelism size is automatically set based on the number of GPUs."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"NUM_EPOCHS = 60\nSEQ_LEN = 1024\nBATCH_SIZE = 192\nNUM_CHUNKS = None\nTENSOR_SHAPE = (1, 1024, 1600)\n# only pipeline parallel\n# CONFIG = dict(parallel=dict(pipeline=2), fp16=dict(mode=AMP_TYPE.NAIVE))\n# pipeline + 1D model parallel\nCONFIG = dict(NUM_MICRO_BATCHES = 192, parallel=dict(pipeline=2, tensor=dict(mode='1d', size=2)), fp16=dict(mode=AMP_TYPE.NAIVE))\n\n\ndef train():\n    disable_existing_loggers()\n    parser = colossalai.get_default_parser()\n    args = parser.parse_args()\n    colossalai.launch_from_torch(config=CONFIG, backend=args.backend)\n    logger = get_dist_logger()\n\n    train_ds = WebtextDataset(os.environ['DATA'], seq_len=SEQ_LEN)\n    train_dataloader = utils.get_dataloader(train_ds,\n                                            seed=42,\n                                            batch_size=BATCH_SIZE,\n                                            pin_memory=True,\n                                            shuffle=True,\n                                            drop_last=True)\n\n    use_interleaved = NUM_CHUNKS is not None\n    num_chunks = 1 if not use_interleaved else NUM_CHUNKS\n    model = GPT2_exlarge_pipeline_hybrid(num_chunks=num_chunks, checkpoint=True, dtype=torch.half)\n    # model = GPT3_pipeline_hybrid(num_chunks=num_chunks, checkpoint=True, dtype=torch.half)\n    if use_interleaved and not isinstance(model, nn.ModuleList):\n        model = nn.ModuleList([model])\n\n    criterion = GPTLMLoss()\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.00015, weight_decay=1e-2,)\n\n    engine, train_dataloader, _, _ = colossalai.initialize(model,\n                                                           optimizer,\n                                                           criterion,\n                                                           train_dataloader=train_dataloader)\n    global_batch_size = BATCH_SIZE * \\\n        gpc.get_world_size(ParallelMode.DATA) * getattr(gpc.config, \"gradient_accumulation\", 1)\n    logger.info(f'Init done, global batch size = {global_batch_size}', ranks=[0])\n\n    timer = MultiTimer()\n\n    trainer = Trainer(\n        engine=engine,\n        logger=logger,\n        timer=timer\n    )\n\n    hook_list = [\n        hooks.LossHook(),\n        hooks.LogMetricByEpochHook(logger),\n        hooks.ThroughputHook(),\n        hooks.LogMetricByStepHook(),\n    ]\n\n    trainer.fit(\n        train_dataloader=train_dataloader,\n        epochs=NUM_EPOCHS,\n        test_interval=1,\n        hooks=hook_list,\n        display_progress=True,\n        return_output_label=False,\n    )\n")))}u.isMDXComponent=!0}}]);