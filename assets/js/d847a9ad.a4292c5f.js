"use strict";(self.webpackChunkdemo=self.webpackChunkdemo||[]).push([[4105],{3905:(e,n,t)=>{t.d(n,{Zo:()=>u,kt:()=>g});var a=t(7294);function l(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function r(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,a)}return t}function i(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?r(Object(t),!0).forEach((function(n){l(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):r(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function s(e,n){if(null==e)return{};var t,a,l=function(e,n){if(null==e)return{};var t,a,l={},r=Object.keys(e);for(a=0;a<r.length;a++)t=r[a],n.indexOf(t)>=0||(l[t]=e[t]);return l}(e,n);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)t=r[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(l[t]=e[t])}return l}var o=a.createContext({}),p=function(e){var n=a.useContext(o),t=n;return e&&(t="function"==typeof e?e(n):i(i({},n),e)),t},u=function(e){var n=p(e.components);return a.createElement(o.Provider,{value:n},e.children)},m="mdxType",d={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},c=a.forwardRef((function(e,n){var t=e.components,l=e.mdxType,r=e.originalType,o=e.parentName,u=s(e,["components","mdxType","originalType","parentName"]),m=p(t),c=l,g=m["".concat(o,".").concat(c)]||m[c]||d[c]||r;return t?a.createElement(g,i(i({ref:n},u),{},{components:t})):a.createElement(g,i({ref:n},u))}));function g(e,n){var t=arguments,l=n&&n.mdxType;if("string"==typeof e||l){var r=t.length,i=new Array(r);i[0]=c;var s={};for(var o in n)hasOwnProperty.call(n,o)&&(s[o]=n[o]);s.originalType=e,s[m]="string"==typeof e?e:l,i[1]=s;for(var p=2;p<r;p++)i[p]=t[p];return a.createElement.apply(null,i)}return a.createElement.apply(null,t)}c.displayName="MDXCreateElement"},2281:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>o,contentTitle:()=>i,default:()=>d,frontMatter:()=>r,metadata:()=>s,toc:()=>p});var a=t(7462),l=(t(7294),t(3905));const r={},i="Sequence Parallelism",s={unversionedId:"features/sequence_parallelism",id:"features/sequence_parallelism",title:"Sequence Parallelism",description:"Author: Mingyan Jiang",source:"@site/i18n/en/docusaurus-plugin-content-docs/current/features/sequence_parallelism.md",sourceDirName:"features",slug:"/features/sequence_parallelism",permalink:"/docs/features/sequence_parallelism",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/features/sequence_parallelism.md",tags:[],version:"current",frontMatter:{}},o={},p=[{value:"Quick Overview",id:"quick-overview",level:2},{value:"Table Of Content",id:"table-of-content",level:2},{value:"Implementation in Colossal-AI",id:"implementation-in-colossal-ai",level:2},{value:"Using Sequence Parallelism with HybridParallelPlugin",id:"using-sequence-parallelism-with-hybridparallelplugin",level:3},{value:"Defining Model Components",id:"defining-model-components",level:4},{value:"Using TP+SP",id:"using-tpsp",level:3},{value:"Using DeepSpeed-Ulysses",id:"using-deepspeed-ulysses",level:4},{value:"Using Ring Attention",id:"using-ring-attention",level:4},{value:"Using Booster",id:"using-booster",level:4},{value:"Training the Model",id:"training-the-model",level:4},{value:"Sequence Parallelism with MoeHybridParallelPlugin",id:"sequence-parallelism-with-moehybridparallelplugin",level:3},{value:"Conclusion",id:"conclusion",level:3}],u={toc:p},m="wrapper";function d(e){let{components:n,...t}=e;return(0,l.kt)(m,(0,a.Z)({},u,t,{components:n,mdxType:"MDXLayout"}),(0,l.kt)("h1",{id:"sequence-parallelism"},"Sequence Parallelism"),(0,l.kt)("p",null,"Author: Mingyan Jiang"),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"Prerequisite Tutorials")),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"/docs/concepts/paradigms_of_parallelism"},"Paradigms of Parallelism")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"/docs/basics/booster_api"},"Booster API")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"/docs/features/shardformer"},"Shardformer")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"/docs/basics/booster_plugins"},"Booster plugin"))),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"Example Code")),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"https://github.com/hpcaitech/ColossalAI/blob/main/examples/language/llama/benchmark.py"},"Using Sequence Parallelism Strategy"))),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"Related Papers"),"\n",(0,l.kt)("a",{parentName:"p",href:"https://arxiv.org/pdf/2205.05198"},"Reducing Activation Recomputation in Large Transformer Models"),"\n",(0,l.kt)("a",{parentName:"p",href:"https://arxiv.org/abs/2309.14509"},"DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models"),"\n",(0,l.kt)("a",{parentName:"p",href:"https://arxiv.org/pdf/2310.01889"},"Ring Attention with Blockwise Transformers for Near-Infinite Context")),(0,l.kt)("h2",{id:"quick-overview"},"Quick Overview"),(0,l.kt)("p",null,"In this tutorial, you will learn how to use sequence parallelism. In Colossal-AI, we have implemented several types of sequence parallelism, including TP+SP, DeepSpeed-Ulysses, and ring attention. Below, we will introduce how to use these different types of sequence parallelism."),(0,l.kt)("h2",{id:"table-of-content"},"Table Of Content"),(0,l.kt)("p",null,"In this tutorial, we will cover the use of three sequence parallelism strategies:"),(0,l.kt)("ol",null,(0,l.kt)("li",{parentName:"ol"},"Using TP+SP;"),(0,l.kt)("li",{parentName:"ol"},"Using DeepSpeed-Ulysses;"),(0,l.kt)("li",{parentName:"ol"},"Using ring attention.")),(0,l.kt)("h2",{id:"implementation-in-colossal-ai"},"Implementation in Colossal-AI"),(0,l.kt)("p",null,"In Colossal-AI, sequence parallelism is implemented via the shardformer and can be invoked through the ",(0,l.kt)("inlineCode",{parentName:"p"},"HybridParallelPlugin")," and ",(0,l.kt)("inlineCode",{parentName:"p"},"MoeHybridParallelPlugin")," interfaces. For more information about the plugins, refer to the ",(0,l.kt)("a",{parentName:"p",href:"/docs/basics/booster_plugins"},"plugin usage documentation"),"."),(0,l.kt)("h3",{id:"using-sequence-parallelism-with-hybridparallelplugin"},"Using Sequence Parallelism with HybridParallelPlugin"),(0,l.kt)("p",null,"The ",(0,l.kt)("inlineCode",{parentName:"p"},"HybridParallelPlugin")," supports three types of sequence parallelism: TP+SP, DeepSpeed-Ulysses, and ring attention. You can refer to the parallel techniques introduction ",(0,l.kt)("a",{parentName:"p",href:"/docs/concepts/paradigms_of_parallelism"},"document")," for more details. An ",(0,l.kt)("a",{parentName:"p",href:"https://github.com/hpcaitech/ColossalAI/blob/main/examples/language/llama/benchmark.py"},"example")," of sequence parallelism with HybridParallelPlugin can be found here."),(0,l.kt)("h4",{id:"defining-model-components"},"Defining Model Components"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'from tqdm import tqdm\nfrom transformers import AutoModelForCausalLM\nfrom transformers.models.llama.configuration_llama import LlamaConfig\nfrom torch.optim.lr_scheduler import _LRScheduler as LRScheduler\nimport torch.distributed as dist\nfrom colossalai.booster import Booster\nconfig = LlamaConfig(max_position_embeddings=4096)\nfrom colossalai.booster.plugin import HybridParallelPlugin\n\n# define dataset\nclass RandomDataset(Dataset):\n    def __init__(self, num_samples: int = 1000, max_length: int = 2048, vocab_size: int = 32000):\n        self.num_samples = num_samples\n        self.max_length = max_length\n        self.input_ids = torch.randint(\n            0, vocab_size, (num_samples, max_length), device=get_accelerator().get_current_device()\n        )\n        self.attention_mask = torch.ones_like(self.input_ids)\n\n    def __len__(self):\n        return self.num_samples\n\n    def __getitem__(self, idx):\n        return {\n            "input_ids": self.input_ids[idx],\n            "attention_mask": self.attention_mask[idx],\n            "labels": self.input_ids[idx],\n        }\n\nparser = argparse.ArgumentParser()\nparser.add_argument("-b", "--batch_size", type=int, default=2, help="Batch size")\nparser.add_argument("-s", "--num_steps", type=int, default=5, help="Number of steps to run")\nparser.add_argument("-l", "--max_length", type=int, default=4096, help="Max sequence length")\nparser.add_argument("--tp", type=int, default=1, help="Tensor parallel size")\nparser.add_argument("--sp", type=int, default=1, help="Sequence parallel size")\nargs = parser.parse_args()\n\nmodel = AutoModelForCausalLM.from_config(\n    config,\n    trust_remote_code=True,\n    attn_implementation="flash_attention_2",\n    torch_dtype=torch.bfloat16,\n)\noptimizer = HybridAdam(model.parameters())\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n# usually, num_samples=args.batch_size * args.num_steps * dp_size\ndataset = RandomDataset(\n        num_samples=10000, max_length=args.max_length, vocab_size=config.vocab_size\n    )\n')),(0,l.kt)("h3",{id:"using-tpsp"},"Using TP+SP"),(0,l.kt)("p",null,"Define the plugin. When using this sequence parallelism, sp_size will be set to match tp_size, and the tp group will overlap with the sp group."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'plugin = HybridParallelPlugin(\n            tp_size=4,\n            sp_size=1,\n            enable_all_optimization=True,\n            enable_sequence_parallelism=True,\n            sequence_parallelism_mode="split_gather",\n        )\n')),(0,l.kt)("h4",{id:"using-deepspeed-ulysses"},"Using DeepSpeed-Ulysses"),(0,l.kt)("p",null,"Define the plugin. In the DeepSpeed-Ulysses sequence parallelism, the tp group and sp group are orthogonal."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'plugin = HybridParallelPlugin(\n            tp_size=2,\n            sp_size=2,\n            enable_all_optimization=True,\n            enable_sequence_parallelism=True,\n            sequence_parallelism_mode="all_to_all",\n        )\n')),(0,l.kt)("h4",{id:"using-ring-attention"},"Using Ring Attention"),(0,l.kt)("p",null,"Define the plugin. In ring attention sequence parallelism, the tp group and sp group are orthogonal, and sp_size must be set to the correct parallel size."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'plugin = HybridParallelPlugin(\n            tp_size=2,\n            sp_size=2,\n            enable_all_optimization=True,\n            enable_sequence_parallelism=True,\n            sequence_parallelism_mode="ring_attn",\n        )\n')),(0,l.kt)("h4",{id:"using-booster"},"Using Booster"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"booster = Booster(plugin=plugin)\ndataloader = plugin.prepare_dataloader(dataset, batch_size=args.batch_size, shuffle=True, drop_last=True, seed=42)\nmodel, optimizer, _, dataloader, _ = booster.boost(model, optimizer, dataloader=dataloader)\n")),(0,l.kt)("h4",{id:"training-the-model"},"Training the Model"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'for step, batch in enumerate(tqdm(dataloader, desc="Step", disable=not dist.get_rank()==0)):\n    outputs = model(**batch)\n    loss = outputs[0]\n    del outputs  # free memory\n\n    if dist.get_rank() == dist.get_world_size() - 1:\n        print(f"Step {step} loss: {loss}")\n    booster.backward(loss, optimizer)\n    optimizer.step()\n    optimizer.zero_grad()\n')),(0,l.kt)("h3",{id:"sequence-parallelism-with-moehybridparallelplugin"},"Sequence Parallelism with MoeHybridParallelPlugin"),(0,l.kt)("p",null,"Currently, the ",(0,l.kt)("inlineCode",{parentName:"p"},"MoeHybridParallelPlugin")," only supports DeepSpeed-Ulysses sequence parallelism. The usage is similar to HybridParallelPlugin. For specific examples, refer to this ",(0,l.kt)("a",{parentName:"p",href:"https://github.com/hpcaitech/ColossalAI/blob/main/examples/language/deepseek/benchmark.py"},"example"),"."),(0,l.kt)("h3",{id:"conclusion"},"Conclusion"),(0,l.kt)("p",null,"Among the sequence parallelism methods mentioned, ring attention has no requirements for the number of attention heads and can train ultra-long sequences. However, due to the division of computation, its performance may decrease. TP+SP and DeepSpeed-Ulysses have requirements for the number of attention heads, which must be divisible by the sp group size. These sequence parallelism methods are all compatible with high-performance attention mechanisms like flash attention. Sequence parallelism can also be used with Gemini to train extremely large-scale models, and it can be combined with TP, PP, and DP to form 4D parallelism."))}d.isMDXComponent=!0}}]);