"use strict";(self.webpackChunkdemo=self.webpackChunkdemo||[]).push([[4634],{6999:(e,t,o)=>{o.d(t,{Cl:()=>i,Dx:()=>u,Pc:()=>n,aE:()=>s,e_:()=>c,iz:()=>r,nT:()=>p});var a=o(7294),l=o(398);o(814);function i(e){return a.createElement("div",{className:"docstring-container"},e.children)}function n(e){return a.createElement("div",{className:"signature"},"(",e.children,")")}function r(e){return a.createElement("div",{class:"divider"},a.createElement("span",{class:"divider-text"},e.name))}function s(e){return a.createElement("div",null,a.createElement(r,{name:"Parameters"}),a.createElement(l.D,null,e.children))}function p(e){return a.createElement("div",null,a.createElement(r,{name:"Returns"}),a.createElement(l.D,null,`${e.name}: ${e.desc}`))}function u(e){return a.createElement("div",{className:"title-container"},a.createElement("div",{className:"title-module"},a.createElement("h5",null,e.type),"\xa0 ",a.createElement("h3",null,e.name)),a.createElement("div",{className:"title-source"},"<",a.createElement("a",{href:e.source,className:"title-source"},"source"),">"))}function c(e){return a.createElement("div",null,a.createElement(r,{name:"Example"}),a.createElement(l.D,null,e.code))}},5099:(e,t,o)=>{o.r(t),o.d(t,{assets:()=>p,contentTitle:()=>r,default:()=>m,frontMatter:()=>n,metadata:()=>s,toc:()=>u});var a=o(7462),l=(o(7294),o(3905)),i=o(6999);const n={},r="Booster Plugins",s={unversionedId:"basics/booster_plugins",id:"basics/booster_plugins",title:"Booster Plugins",description:"Author: Hongxin Liu, Baizhou Zhang",source:"@site/i18n/en/docusaurus-plugin-content-docs/current/basics/booster_plugins.md",sourceDirName:"basics",slug:"/basics/booster_plugins",permalink:"/docs/basics/booster_plugins",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/basics/booster_plugins.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Booster API",permalink:"/docs/basics/booster_api"},next:{title:"Booster Checkpoint",permalink:"/docs/basics/booster_checkpoint"}},p={},u=[{value:"Introduction",id:"introduction",level:2},{value:"Plugins",id:"plugins",level:2},{value:"Low Level Zero Plugin",id:"low-level-zero-plugin",level:3},{value:"Gemini Plugin",id:"gemini-plugin",level:3},{value:"Torch DDP Plugin",id:"torch-ddp-plugin",level:3},{value:"Torch FSDP Plugin",id:"torch-fsdp-plugin",level:3},{value:"Hybrid Parallel Plugin",id:"hybrid-parallel-plugin",level:3}],c={toc:u},d="wrapper";function m(e){let{components:t,...o}=e;return(0,l.kt)(d,(0,a.Z)({},c,o,{components:t,mdxType:"MDXLayout"}),(0,l.kt)("h1",{id:"booster-plugins"},"Booster Plugins"),(0,l.kt)("p",null,"Author: ",(0,l.kt)("a",{parentName:"p",href:"https://github.com/ver217"},"Hongxin Liu"),", ",(0,l.kt)("a",{parentName:"p",href:"https://github.com/Fridge003"},"Baizhou Zhang")),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"Prerequisite:")),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"/docs/basics/booster_api"},"Booster API"))),(0,l.kt)("h2",{id:"introduction"},"Introduction"),(0,l.kt)("p",null,"As mentioned in ",(0,l.kt)("a",{parentName:"p",href:"/docs/basics/booster_api"},"Booster API"),", we can use booster plugins to customize the parallel training. In this tutorial, we will introduce how to use booster plugins."),(0,l.kt)("p",null,"We currently provide the following plugins:"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#low-level-zero-plugin"},"Low Level Zero Plugin"),": It wraps the ",(0,l.kt)("inlineCode",{parentName:"li"},"colossalai.zero.low_level.LowLevelZeroOptimizer")," and can be used to train models with zero-dp. It only supports zero stage-1 and stage-2."),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#gemini-plugin"},"Gemini Plugin"),": It wraps the ",(0,l.kt)("a",{parentName:"li",href:"/docs/features/zero_with_chunk"},"Gemini")," which implements Zero-3 with chunk-based and heterogeneous memory management."),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#torch-ddp-plugin"},"Torch DDP Plugin"),": It is a wrapper of ",(0,l.kt)("inlineCode",{parentName:"li"},"torch.nn.parallel.DistributedDataParallel")," and can be used to train models with data parallelism."),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#torch-fsdp-plugin"},"Torch FSDP Plugin"),": It is a wrapper of ",(0,l.kt)("inlineCode",{parentName:"li"},"torch.distributed.fsdp.FullyShardedDataParallel")," and can be used to train models with zero-dp."),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#hybrid-parallel-plugin"},"Hybrid Pararllel Plugin"),": It provides a tidy interface that integrates the power of Shardformer, pipeline manager, mixied precision training, TorchDDP and Zero stage 1/2 feature.  With this plugin, transformer models can be easily trained with any combination of tensor parallel, pipeline parallel and data parallel (DDP/Zero) efficiently, along with various kinds of optimization tools for acceleration and memory saving. Detailed information about supported parallel strategies and optimization tools is explained in the section below.")),(0,l.kt)("p",null,"More plugins are coming soon."),(0,l.kt)("h2",{id:"plugins"},"Plugins"),(0,l.kt)("h3",{id:"low-level-zero-plugin"},"Low Level Zero Plugin"),(0,l.kt)("p",null,"This plugin implements Zero-1 and Zero-2 (w/wo CPU offload), using ",(0,l.kt)("inlineCode",{parentName:"p"},"reduce")," and ",(0,l.kt)("inlineCode",{parentName:"p"},"gather")," to synchronize gradients and weights."),(0,l.kt)("p",null,"Zero-1 can be regarded as a better substitute of Torch DDP, which is more memory efficient and faster. It can be easily used in hybrid parallelism."),(0,l.kt)("p",null,"Zero-2 does not support local gradient accumulation. Though you can accumulate gradient if you insist, it cannot reduce communication cost. That is to say, it's not a good idea to use Zero-2 with pipeline parallelism."),(0,l.kt)(i.Cl,{mdxType:"DocStringContainer"},(0,l.kt)("div",null,(0,l.kt)(i.Dx,{type:"class",name:"colossalai.booster.plugin.LowLevelZeroPlugin",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/booster/plugin/low_level_zero_plugin.py#L238",mdxType:"Title"}),(0,l.kt)(i.Pc,{mdxType:"Signature"},"stage: int = 1, precision: str = 'fp16', initial_scale: float = 4294967296, min_scale: float = 1, growth_factor: float = 2, backoff_factor: float = 0.5, growth_interval: int = 1000, hysteresis: int = 2, max_scale: float = 4294967296, max_norm: float = 0.0, norm_type: float = 2.0, reduce_bucket_size_in_m: int = 12, communication_dtype: typing.Optional[torch.dtype] = None, overlap_communication: bool = True, cpu_offload: bool = False, verbose: bool = False"),(0,l.kt)(i.aE,{mdxType:"Parameters"},"- **strage** (int, optional) -- ZeRO stage. Defaults to 1.\n- **precision** (str, optional) -- precision. Support 'fp16', 'bf16' and 'fp32'. Defaults to 'fp16'.\n- **initial_scale** (float, optional) -- Initial scale used by DynamicGradScaler. Defaults to 2**32.\n- **min_scale** (float, optional) -- Min scale used by DynamicGradScaler. Defaults to 1.\n- **growth_factor** (float, optional) -- growth_factor used by DynamicGradScaler. Defaults to 2.\n- **backoff_factor** (float, optional) -- backoff_factor used by DynamicGradScaler. Defaults to 0.5.\n- **growth_interval** (float, optional) -- growth_interval used by DynamicGradScaler. Defaults to 1000.\n- **hysteresis** (float, optional) -- hysteresis used by DynamicGradScaler. Defaults to 2.\n- **max_scale** (int, optional) -- max_scale used by DynamicGradScaler. Defaults to 2**32.\n- **max_norm** (float, optional) -- max_norm used for `clip_grad_norm`. You should notice that you shall not do\n  clip_grad_norm by yourself when using ZeRO DDP. The ZeRO optimizer will take care of clip_grad_norm.\n- **norm_type** (float, optional) -- norm_type used for `clip_grad_norm`.\n- **reduce_bucket_size_in_m** (int, optional) -- grad reduce bucket size in M. Defaults to 12.\n- **communication_dtype** (torch.dtype, optional) -- communication dtype. If not specified, the dtype of param will be used. Defaults to None.\n- **overlap_communication** (bool, optional) -- whether to overlap communication and computation. Defaults to True.\n- **cpu_offload** (bool, optional) -- whether to offload grad, master weight and optimizer state to cpu. Defaults to False.\n- **verbose** (bool, optional) -- verbose mode. Debug info including grad overflow will be printed. Defaults to False.")),(0,l.kt)("div",null,(0,l.kt)(i.iz,{name:"Description",mdxType:"Divider"}),(0,l.kt)("p",null,"Plugin for low level zero."),(0,l.kt)("p",null,"Example:"),(0,l.kt)("blockquote",null,(0,l.kt)("blockquote",{parentName:"blockquote"},(0,l.kt)("blockquote",{parentName:"blockquote"},(0,l.kt)("p",{parentName:"blockquote"},"from colossalai.booster import Booster\nfrom colossalai.booster.plugin import LowLevelZeroPlugin"),(0,l.kt)("p",{parentName:"blockquote"},"model, train_dataset, optimizer, criterion = ...\nplugin = LowLevelZeroPlugin()")))),(0,l.kt)("blockquote",null,(0,l.kt)("blockquote",{parentName:"blockquote"},(0,l.kt)("blockquote",{parentName:"blockquote"},(0,l.kt)("p",{parentName:"blockquote"},"train_dataloader = plugin.prepare_dataloader(train_dataset, batch_size=8)\nbooster = Booster(plugin=plugin)\nmodel, optimizer, train_dataloader, criterion = booster.boost(model, optimizer, train_dataloader, criterion)")))))),(0,l.kt)("p",null,"We've tested compatibility on some famous models, following models may not be supported:"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"timm.models.convit_base")),(0,l.kt)("li",{parentName:"ul"},"dlrm and deepfm models in ",(0,l.kt)("inlineCode",{parentName:"li"},"torchrec")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"diffusers.VQModel")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"transformers.AlbertModel")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"transformers.AlbertForPreTraining")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"transformers.BertModel")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"transformers.BertForPreTraining")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"transformers.GPT2DoubleHeadsModel"))),(0,l.kt)("p",null,"Compatibility problems will be fixed in the future."),(0,l.kt)("h3",{id:"gemini-plugin"},"Gemini Plugin"),(0,l.kt)("p",null,"This plugin implements Zero-3 with chunk-based and heterogeneous memory management. It can train large models without much loss in speed. It also does not support local gradient accumulation. More details can be found in ",(0,l.kt)("a",{parentName:"p",href:"/docs/features/zero_with_chunk"},"Gemini Doc"),"."),(0,l.kt)(i.Cl,{mdxType:"DocStringContainer"},(0,l.kt)("div",null,(0,l.kt)(i.Dx,{type:"class",name:"colossalai.booster.plugin.GeminiPlugin",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/booster/plugin/gemini_plugin.py#L223",mdxType:"Title"}),(0,l.kt)(i.Pc,{mdxType:"Signature"},"chunk_config_dict: typing.Optional[dict] = None, chunk_init_device: typing.Optional[torch.device] = None, placement_policy: str = 'static', shard_param_frac: float = 1.0, offload_optim_frac: float = 0.0, offload_param_frac: float = 0.0, warmup_non_model_data_ratio: float = 0.8, steady_cuda_cap_ratio: float = 0.9, precision: str = 'fp16', pin_memory: bool = False, force_outputs_fp32: bool = False, strict_ddp_mode: bool = False, search_range_m: int = 32, hidden_dim: typing.Optional[int] = None, min_chunk_size_m: float = 32, memstats: typing.Optional[colossalai.zero.gemini.memory_tracer.memory_stats.MemStats] = None, gpu_margin_mem_ratio: float = 0.0, initial_scale: float = 65536, min_scale: float = 1, growth_factor: float = 2, backoff_factor: float = 0.5, growth_interval: int = 1000, hysteresis: int = 2, max_scale: float = 4294967296, max_norm: float = 0.0, norm_type: float = 2.0, verbose: bool = False"),(0,l.kt)(i.aE,{mdxType:"Parameters"},'- **chunk_config_dict** (dict, optional) -- chunk configuration dictionary.\n- **chunk_init_device** (torch.device, optional) -- device to initialize the chunk.\n- **placement_policy** (str, optional) -- "static" and "auto". Defaults to "static".\n- **shard_param_frac** (float, optional) -- fraction of parameters to be sharded. Only for "static" placement.\n  If `shard_param_frac` is 1.0, it\'s equal to zero-3. If `shard_param_frac` is 0.0, it\'s equal to zero-2. Defaults to 1.0.\n- **offload_optim_frac** (float, optional) -- fraction of optimizer states to be offloaded. Only for "static" placement.\n  If `shard_param_frac` is 1.0 and `offload_optim_frac` is 0.0, it\'s equal to old "cuda" placement. Defaults to 0.0.\n- **offload_param_frac** (float, optional) -- fraction of parameters to be offloaded. Only for "static" placement.\n  For efficiency, this argument is useful only when `shard_param_frac` is 1.0 and `offload_optim_frac` is 1.0.\n  If `shard_param_frac` is 1.0, `offload_optim_frac` is 1.0 and `offload_param_frac` is 1.0, it\'s equal to old "cpu" placement.\n  When using static placement, we recommend users to tune `shard_param_frac` first and then `offload_optim_frac`.\n  Defaults to 0.0.\n- **warmup_non_model_data_ratio** (float, optional) -- ratio of expected non-model data memory during warmup. Only for "auto" placement. Defaults to 0.8.\n- **steady_cuda_cap_ratio** (float, optional) -- ratio of allowed cuda capacity for model data during steady state. Only for "auto" placement. Defaults to 0.9.\n- **precision** (str, optional) -- precision. Support \'fp16\' and \'bf16\'. Defaults to \'fp16\'.\n- **pin_memory** (bool, optional) -- use pin memory on CPU. Defaults to False.\n- **force_outputs_fp32** (bool, optional) -- force outputs are fp32. Defaults to False.\n- **strict_ddp_mode** (bool, optional) -- use strict ddp mode (only use dp without other parallelism). Defaults to False.\n- **search_range_m** (int, optional) -- chunk size searching range divided by 2^20. Defaults to 32.\n- **hidden_dim** (int, optional) -- the hidden dimension of DNN.\n  Users can provide this argument to speed up searching.\n  If users do not know this argument before training, it is ok. We will use a default value 1024.\n- **min_chunk_size_m** (float, optional) -- the minimum chunk size divided by 2^20.\n  If the aggregate size of parameters is still smaller than the minimum chunk size,\n  all parameters will be compacted into one small chunk.\n- **memstats** (MemStats, optional) the memory statistics collector by a runtime memory tracer. --\n- **gpu_margin_mem_ratio** (float, optional) -- The ratio of GPU remaining memory (after the first forward-backward)\n  which will be used when using hybrid CPU optimizer.\n  This argument is meaningless when `placement_policy` of `GeminiManager` is not "auto".\n  Defaults to 0.0.\n- **initial_scale** (float, optional) -- Initial scale used by DynamicGradScaler. Defaults to 2**16.\n- **min_scale** (float, optional) -- Min scale used by DynamicGradScaler. Defaults to 1.\n- **growth_factor** (float, optional) -- growth_factor used by DynamicGradScaler. Defaults to 2.\n- **backoff_factor** (float, optional) -- backoff_factor used by DynamicGradScaler. Defaults to 0.5.\n- **growth_interval** (float, optional) -- growth_interval used by DynamicGradScaler. Defaults to 1000.\n- **hysteresis** (float, optional) -- hysteresis used by DynamicGradScaler. Defaults to 2.\n- **max_scale** (int, optional) -- max_scale used by DynamicGradScaler. Defaults to 2**32.\n- **max_norm** (float, optional) -- max_norm used for `clip_grad_norm`. You should notice that you shall not do\n  clip_grad_norm by yourself when using ZeRO DDP. The ZeRO optimizer will take care of clip_grad_norm.\n- **norm_type** (float, optional) -- norm_type used for `clip_grad_norm`.\n- **verbose** (bool, optional) -- verbose mode. Debug info including chunk search result will be printed. Defaults to False.')),(0,l.kt)("div",null,(0,l.kt)(i.iz,{name:"Description",mdxType:"Divider"}),(0,l.kt)("p",null,"Plugin for Gemini."),(0,l.kt)("p",null,"Example:"),(0,l.kt)("blockquote",null,(0,l.kt)("blockquote",{parentName:"blockquote"},(0,l.kt)("blockquote",{parentName:"blockquote"},(0,l.kt)("p",{parentName:"blockquote"},"from colossalai.booster import Booster\nfrom colossalai.booster.plugin import GeminiPlugin"),(0,l.kt)("p",{parentName:"blockquote"},"model, train_dataset, optimizer, criterion = ...\nplugin = GeminiPlugin()")))),(0,l.kt)("blockquote",null,(0,l.kt)("blockquote",{parentName:"blockquote"},(0,l.kt)("blockquote",{parentName:"blockquote"},(0,l.kt)("p",{parentName:"blockquote"},"train_dataloader = plugin.prepare_dataloader(train_dataset, batch_size=8)\nbooster = Booster(plugin=plugin)\nmodel, optimizer, train_dataloader, criterion = booster.boost(model, optimizer, train_dataloader, criterion)")))))),(0,l.kt)("h3",{id:"torch-ddp-plugin"},"Torch DDP Plugin"),(0,l.kt)("p",null,"More details can be found in ",(0,l.kt)("a",{parentName:"p",href:"https://pytorch.org/docs/main/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel"},"Pytorch Docs"),"."),(0,l.kt)(i.Cl,{mdxType:"DocStringContainer"},(0,l.kt)("div",null,(0,l.kt)(i.Dx,{type:"class",name:"colossalai.booster.plugin.TorchDDPPlugin",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/booster/plugin/torch_ddp_plugin.py#L90",mdxType:"Title"}),(0,l.kt)(i.Pc,{mdxType:"Signature"},"broadcast_buffers: bool = True, bucket_cap_mb: int = 25, find_unused_parameters: bool = False, check_reduction: bool = False, gradient_as_bucket_view: bool = False, static_graph: bool = False"),(0,l.kt)(i.aE,{mdxType:"Parameters"},"- **broadcast_buffers** (bool, optional) -- Whether to broadcast buffers in the beginning of training. Defaults to True.\n- **bucket_cap_mb** (int, optional) -- The bucket size in MB. Defaults to 25.\n- **find_unused_parameters** (bool, optional) -- Whether to find unused parameters. Defaults to False.\n- **check_reduction** (bool, optional) -- Whether to check reduction. Defaults to False.\n- **gradient_as_bucket_view** (bool, optional) -- Whether to use gradient as bucket view. Defaults to False.\n- **static_graph** (bool, optional) -- Whether to use static graph. Defaults to False.")),(0,l.kt)("div",null,(0,l.kt)(i.iz,{name:"Description",mdxType:"Divider"}),(0,l.kt)("p",null,"Plugin for PyTorch DDP."),(0,l.kt)("p",null,"Example:"),(0,l.kt)("blockquote",null,(0,l.kt)("blockquote",{parentName:"blockquote"},(0,l.kt)("blockquote",{parentName:"blockquote"},(0,l.kt)("p",{parentName:"blockquote"},"from colossalai.booster import Booster\nfrom colossalai.booster.plugin import TorchDDPPlugin"),(0,l.kt)("p",{parentName:"blockquote"},"model, train_dataset, optimizer, criterion = ...\nplugin = TorchDDPPlugin()")))),(0,l.kt)("blockquote",null,(0,l.kt)("blockquote",{parentName:"blockquote"},(0,l.kt)("blockquote",{parentName:"blockquote"},(0,l.kt)("p",{parentName:"blockquote"},"train_dataloader = plugin.prepare_dataloader(train_dataset, batch_size=8)\nbooster = Booster(plugin=plugin)\nmodel, optimizer, train_dataloader, criterion = booster.boost(model, optimizer, train_dataloader, criterion)")))))),(0,l.kt)("h3",{id:"torch-fsdp-plugin"},"Torch FSDP Plugin"),(0,l.kt)("blockquote",null,(0,l.kt)("p",{parentName:"blockquote"},"\u26a0 This plugin is not available when torch version is lower than 1.12.0.")),(0,l.kt)("blockquote",null,(0,l.kt)("p",{parentName:"blockquote"},"\u26a0 This plugin does not support save/load sharded model checkpoint now.")),(0,l.kt)("blockquote",null,(0,l.kt)("p",{parentName:"blockquote"},"\u26a0 This plugin does not support optimizer that use multi params group.")),(0,l.kt)("p",null,"More details can be found in ",(0,l.kt)("a",{parentName:"p",href:"https://pytorch.org/docs/main/fsdp.html"},"Pytorch Docs"),"."),(0,l.kt)(i.Cl,{mdxType:"DocStringContainer"},(0,l.kt)("div",null,(0,l.kt)(i.Dx,{type:"class",name:"colossalai.booster.plugin.TorchFSDPPlugin",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/booster/plugin/torch_fsdp_plugin.py#L138",mdxType:"Title"}),(0,l.kt)(i.Pc,{mdxType:"Signature"},"process_group: typing.Optional[torch.distributed.distributed_c10d.ProcessGroup] = None, sharding_strategy: typing.Optional[torch.distributed.fsdp.api.ShardingStrategy] = None, cpu_offload: typing.Optional[torch.distributed.fsdp.api.CPUOffload] = None, auto_wrap_policy: typing.Optional[typing.Callable] = None, backward_prefetch: typing.Optional[torch.distributed.fsdp.api.BackwardPrefetch] = None, mixed_precision: typing.Optional[torch.distributed.fsdp.api.MixedPrecision] = None, ignored_modules: typing.Optional[typing.Iterable[torch.nn.modules.module.Module]] = None, param_init_fn: typing.Optional[typing.Callable[[torch.nn.modules.module.Module]], NoneType] = None, sync_module_states: bool = False"),(0,l.kt)(i.aE,{mdxType:"Parameters"},"- **See** https --//pytorch.org/docs/stable/fsdp.html for details.")),(0,l.kt)("div",null,(0,l.kt)(i.iz,{name:"Description",mdxType:"Divider"}),(0,l.kt)("p",null,"Plugin for PyTorch FSDP."),(0,l.kt)("p",null,"Example:"),(0,l.kt)("blockquote",null,(0,l.kt)("blockquote",{parentName:"blockquote"},(0,l.kt)("blockquote",{parentName:"blockquote"},(0,l.kt)("p",{parentName:"blockquote"},"from colossalai.booster import Booster\nfrom colossalai.booster.plugin import TorchFSDPPlugin"),(0,l.kt)("p",{parentName:"blockquote"},"model, train_dataset, optimizer, criterion = ...\nplugin = TorchFSDPPlugin()")))),(0,l.kt)("blockquote",null,(0,l.kt)("blockquote",{parentName:"blockquote"},(0,l.kt)("blockquote",{parentName:"blockquote"},(0,l.kt)("p",{parentName:"blockquote"},"train_dataloader = plugin.prepare_train_dataloader(train_dataset, batch_size=8)\nbooster = Booster(plugin=plugin)\nmodel, optimizer, train_dataloader, criterion = booster.boost(model, optimizer, train_dataloader, criterion)")))))),(0,l.kt)("h3",{id:"hybrid-parallel-plugin"},"Hybrid Parallel Plugin"),(0,l.kt)("p",null,"This plugin implements the combination of various parallel training strategies and optimization tools. The features of HybridParallelPlugin can be generally divided into four parts:"),(0,l.kt)("ol",null,(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"Shardformer: This plugin provides an entrance to Shardformer, which controls model sharding under tensor parallel and pipeline parallel setting. Shardformer also overloads the logic of model's forward/backward process to ensure the smooth working of tp/pp. Also, optimization tools including fused normalization, flash attention (xformers), JIT and sequence parallel are injected into the overloaded forward/backward method by Shardformer. More details can be found in chapter ",(0,l.kt)("a",{parentName:"p",href:"/docs/features/shardformer"},"Shardformer Doc"),".")),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"Mixed Precision Training: Support for fp16/bf16 mixed precision training. More details about its arguments configuration can be found in ",(0,l.kt)("a",{parentName:"p",href:"/docs/features/mixed_precision_training_with_booster"},"Mixed Precision Training Doc"),".")),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"Torch DDP: This plugin will automatically adopt Pytorch DDP as data parallel strategy when pipeline parallel and Zero is not used. More details about its arguments configuration can be found in ",(0,l.kt)("a",{parentName:"p",href:"https://pytorch.org/docs/main/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel"},"Pytorch DDP Docs"),".")),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"Zero: This plugin can adopt Zero 1/2 as data parallel strategy through setting the ",(0,l.kt)("inlineCode",{parentName:"p"},"zero_stage")," argument as 1 or 2 when initializing plugin. Zero 1 is compatible with pipeline parallel strategy, while Zero 2 is not. More details about its argument configuration can be found in ",(0,l.kt)("a",{parentName:"p",href:"#low-level-zero-plugin"},"Low Level Zero Plugin"),"."))),(0,l.kt)("blockquote",null,(0,l.kt)("p",{parentName:"blockquote"},"\u26a0 When using this plugin, only the subset of Huggingface transformers supported by Shardformer are compatible with tensor parallel, pipeline parallel and optimization tools. Mainstream transformers such as Llama 1, Llama 2, OPT, Bloom, Bert and GPT2 etc. are all supported by Shardformer.")),(0,l.kt)("blockquote",null,(0,l.kt)("p",{parentName:"blockquote"},"\u26a0 This plugin only supports sharded checkpointing methods for model/optimizer at present. Unsharded checkpointing methods will be supported in future release.")),(0,l.kt)(i.Cl,{mdxType:"DocStringContainer"},(0,l.kt)("div",null,(0,l.kt)(i.Dx,{type:"class",name:"colossalai.booster.plugin.HybridParallelPlugin",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/booster/plugin/hybrid_parallel_plugin.py#L253",mdxType:"Title"}),(0,l.kt)(i.Pc,{mdxType:"Signature"},"tp_size: int, pp_size: int, precision: str = 'fp16', zero_stage: int = 0, enable_all_optimization: bool = False, enable_fused_normalization: bool = False, enable_flash_attention: bool = False, enable_jit_fused: bool = False, enable_sequence_parallelism: bool = False, enable_sequence_overlap: bool = False, num_microbatches: typing.Optional[int] = None, microbatch_size: typing.Optional[int] = None, initial_scale: float = 65536, min_scale: float = 1, growth_factor: float = 2, backoff_factor: float = 0.5, growth_interval: int = 1000, hysteresis: int = 2, max_scale: float = 4294967296, max_norm: float = 0, broadcast_buffers: bool = True, ddp_bucket_cap_mb: int = 25, find_unused_parameters: bool = False, check_reduction: bool = False, gradient_as_bucket_view: bool = False, static_graph: bool = False, zero_bucket_size_in_m: int = 12, cpu_offload: bool = False, communication_dtype: typing.Optional[torch.dtype] = None, overlap_communication: bool = True, custom_policy: Policy = None"),(0,l.kt)(i.aE,{mdxType:"Parameters"},"- **tp_size** (int) -- The size of tensor parallelism. Tensor parallelism will not be used when tp_size is set to 1.\n- **pp_size** (int) -- The number of pipeline stages in pipeline parallelism. Pipeline parallelism will not be used when pp_size is set to 1.\n- **precision** (str, optional) -- Specifies the precision of parameters during training.\n  Auto-mixied precision will be used when this argument is set to 'fp16' or 'bf16', otherwise model is trained with 'fp32'.\n  Defaults to 'fp16'.\n- **zero_stage** (int, optional) -- The stage of ZeRO for data parallelism. Can only be choosed from [0, 1, 2].\n  When set to 0, ZeRO will not be used. Defaults to 0.\n- **enable_all_optimization** (bool, optional) -- Whether to switch on all the optimizations supported by Shardformer.\n  Currently all the optimization methods include fused normalization, flash attention and JIT.\n  Defaults to False.\n- **enable_fused_normalization** (bool, optional) -- Whether to switch on fused normalization in Shardformer. Defaults to False.\n- **enable_flash_attention** (bool, optional) -- Whether to switch on flash attention in Shardformer. Defaults to False.\n- **enable_jit_fused** (bool, optional) -- Whether to switch on JIT in Shardformer. Default to False.\n- **enable_sequence_parallelism** (bool) -- Whether to turn on sequence parallelism in Shardformer. Defaults to False.\n- **enable_sequence_overlap** (bool) -- Whether to turn on sequence overlap in Shardformer. Defaults to False.\n- **num_microbatches** (int, optional) -- Number of microbatches when using pipeline parallelism. Defaults to None.\n- **microbatch_size** (int, optional) -- Microbatch size when using pipeline parallelism.\n  Either `num_microbatches` or `microbatch_size` should be provided if using pipeline.\n  If `num_microbatches` is provided, this will be ignored. Defaults to None.\n- **initial_scale** (float, optional) -- The initial loss scale of AMP. Defaults to 2**16.\n- **min_scale** (float, optional) -- The minimum loss scale of AMP. Defaults to 1.\n- **growth_factor** (float, optional) -- The multiplication factor for increasing loss scale when using AMP. Defaults to 2.\n- **backoff_factor** (float, optional) -- The multiplication factor for decreasing loss scale when using AMP. Defaults to 0.5.\n- **growth_interval** (int, optional) -- The number of steps to increase loss scale when no overflow occurs when using AMP. Defaults to 1000.\n- **hysteresis** (int, optional) --  The number of overflows before decreasing loss scale when using AMP. Defaults to 2.\n- **max_scale** (float, optional) -- The maximum loss scale of AMP. Defaults to 2**32.\n- **max_norm** (float, optional) -- Maximum norm for gradient clipping. Defaults to 0.\n- **broadcast_buffers** (bool, optional) -- Whether to broadcast buffers in the beginning of training when using DDP. Defaults to True.\n- **ddp_bucket_cap_mb** (int, optional) -- The bucket size in MB when using DDP. Defaults to 25.\n- **find_unused_parameters** (bool, optional) -- Whether to find unused parameters when using DDP. Defaults to False.\n- **check_reduction** (bool, optional) -- Whether to check reduction when using DDP. Defaults to False.\n- **gradient_as_bucket_view** (bool, optional) -- Whether to use gradient as bucket view when using DDP. Defaults to False.\n- **static_graph** (bool, optional) -- Whether to use static graph when using DDP. Defaults to False.\n- **zero_bucket_size_in_m** (int, optional) -- Gradient reduce bucket size in million elements when using ZeRO. Defaults to 12.\n- **cpu_offload** (bool, optional) -- Whether to open cpu_offload when using ZeRO. Defaults to False.\n- **communication_dtype** (torch.dtype, optional) -- Communication dtype when using ZeRO. If not specified, the dtype of param will be used. Defaults to None.\n- **overlap_communication** (bool, optional) -- Whether to overlap communication and computation when using ZeRO. Defaults to True.\n- **custom_policy** (Policy, optional) -- Custom policy for Shardformer. Defaults to None.")),(0,l.kt)("div",null,(0,l.kt)(i.iz,{name:"Description",mdxType:"Divider"}),(0,l.kt)("p",null,"Plugin for Hybrid Parallel Training.\nTensor parallel, pipeline parallel and data parallel(DDP/ZeRO) can be picked and combined in this plugin.\nThe size of tp and pp should be passed in by user, then the size of dp is automatically calculated from dp_size = world_size / (tp_size * pp_size)."),(0,l.kt)("p",null,"Example:"),(0,l.kt)("blockquote",null,(0,l.kt)("blockquote",{parentName:"blockquote"},(0,l.kt)("blockquote",{parentName:"blockquote"},(0,l.kt)("p",{parentName:"blockquote"},"from colossalai.booster import Booster\nfrom colossalai.booster.plugin import HybridParallelPlugin")))),(0,l.kt)("blockquote",null,(0,l.kt)("blockquote",{parentName:"blockquote"},(0,l.kt)("blockquote",{parentName:"blockquote"},(0,l.kt)("p",{parentName:"blockquote"},"model, train_dataset, optimizer, criterion = ...\nplugin =  HybridParallelPlugin(tp_size=2, pp_size=2)")))),(0,l.kt)("blockquote",null,(0,l.kt)("blockquote",{parentName:"blockquote"},(0,l.kt)("blockquote",{parentName:"blockquote"},(0,l.kt)("p",{parentName:"blockquote"},"train",(0,l.kt)("em",{parentName:"p"},"dataloader = plugin.prepare_dataloader(train_dataset, batch_size=8)\nbooster = Booster(plugin=plugin)\nmodel, optimizer, criterion, train_dataloader, ")," = booster.boost(model, optimizer, criterion, train_dataloader)"))))),(0,l.kt)(i.Cl,{mdxType:"DocStringContainer"},(0,l.kt)("div",null,(0,l.kt)(i.Dx,{type:"function",name:"prepare_dataloader",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/booster/plugin/hybrid_parallel_plugin.py#L522",mdxType:"Title"}),(0,l.kt)(i.Pc,{mdxType:"Signature"},"dataset, batch_size, shuffle = False, seed = 1024, drop_last = False, pin_memory = False, num_workers = 0, **kwargs"),(0,l.kt)(i.aE,{mdxType:"Parameters"},"- **dataset** (*torch.utils.data.Dataset*) -- The dataset to be loaded.\n- **shuffle** (bool, optional) -- Whether to shuffle the dataset. Defaults to False.\n- **seed** (int, optional) -- Random worker seed for sampling, defaults to 1024.\n  add_sampler -- Whether to add `DistributedDataParallelSampler` to the dataset. Defaults to True.\n- **drop_last** (bool, optional) -- Set to True to drop the last incomplete batch, if the dataset size\n  is not divisible by the batch size. If False and the size of dataset is not divisible by\n  the batch size, then the last batch will be smaller, defaults to False.\n- **pin_memory** (bool, optional) -- Whether to pin memory address in CPU memory. Defaults to False.\n- **num_workers** (int, optional) -- Number of worker threads for this dataloader. Defaults to 0.\n- **kwargs** (dict) -- optional parameters for `torch.utils.data.DataLoader`, more details could be found in\n  [DataLoader](https://pytorch.org/docs/stable/_modules/torch/utils/data/dataloader.html#DataLoader)."),(0,l.kt)(i.nT,{name:"[`torch.utils.data.DataLoader`]",desc:"A DataLoader used for training or testing.",mdxType:"Returns"})),(0,l.kt)("div",null,(0,l.kt)(i.iz,{name:"Description",mdxType:"Divider"}),(0,l.kt)("p",null,"Prepare a dataloader for distributed training. The dataloader will be wrapped by\n",(0,l.kt)("em",{parentName:"p"},"torch.utils.data.DataLoader")," and ",(0,l.kt)("em",{parentName:"p"},"torch.utils.data.DistributedSampler"),".")))))}m.isMDXComponent=!0}}]);