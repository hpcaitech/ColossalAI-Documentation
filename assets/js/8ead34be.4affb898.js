"use strict";(self.webpackChunkdemo=self.webpackChunkdemo||[]).push([[2111],{6999:(e,t,o)=>{o.d(t,{Cl:()=>a,Dx:()=>p,Pc:()=>l,aE:()=>s,iz:()=>n,nT:()=>c});var i=o(7294),r=o(398);o(814);function a(e){return i.createElement("div",{className:"docstring-container"},e.children)}function l(e){return i.createElement("div",{className:"signature"},"(",e.children,")")}function n(e){return i.createElement("h3",{className:"divider"},e.name)}function s(e){return i.createElement("div",null,i.createElement(n,{name:"Parameters"}),i.createElement(r.D,null,e.children))}function c(e){return i.createElement("div",null,i.createElement(n,{name:"Returns"}),i.createElement(r.D,null,`${e.name}: ${e.desc}`))}function p(e){return i.createElement("div",{className:"title-container"},i.createElement("div",{className:"title-module"},i.createElement("h3",null,e.type),"\xa0 ",i.createElement("h2",null,e.name)),i.createElement("div",{className:"title-source"},"<",i.createElement("a",{href:e.source,className:"title-source"},"source"),">"))}},8423:(e,t,o)=>{o.r(t),o.d(t,{assets:()=>c,contentTitle:()=>n,default:()=>m,frontMatter:()=>l,metadata:()=>s,toc:()=>p});var i=o(7462),r=(o(7294),o(3905)),a=o(6999);const l={},n="Booster API",s={unversionedId:"basics/booster_api",id:"basics/booster_api",title:"Booster API",description:"Author: Mingyan Jiang",source:"@site/i18n/en/docusaurus-plugin-content-docs/current/basics/booster_api.md",sourceDirName:"basics",slug:"/basics/booster_api",permalink:"/docs/basics/booster_api",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/basics/booster_api.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Launch Colossal-AI",permalink:"/docs/basics/launch_colossalai"},next:{title:"Booster Plugins",permalink:"/docs/basics/booster_plugins"}},c={},p=[{value:"Introduction",id:"introduction",level:2},{value:"Plugin",id:"plugin",level:3},{value:"API of booster",id:"api-of-booster",level:3},{value:"Usage",id:"usage",level:2}],d={toc:p},u="wrapper";function m(e){let{components:t,...o}=e;return(0,r.kt)(u,(0,i.Z)({},d,o,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"booster-api"},"Booster API"),(0,r.kt)("p",null,"Author: ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/jiangmingyan"},"Mingyan Jiang")),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Prerequisite:")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"/docs/concepts/distributed_training"},"Distributed Training")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"/docs/concepts/colossalai_overview"},"Colossal-AI Overview"))),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Example Code")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://github.com/hpcaitech/ColossalAI/blob/main/examples/tutorial/new_api/cifar_resnet/README.md"},"Train with Booster"))),(0,r.kt)("h2",{id:"introduction"},"Introduction"),(0,r.kt)("p",null,"In our new design, ",(0,r.kt)("inlineCode",{parentName:"p"},"colossalai.booster")," replaces the role of ",(0,r.kt)("inlineCode",{parentName:"p"},"colossalai.initialize")," to inject features into your training components (e.g. model, optimizer, dataloader) seamlessly. With these new APIs, you can integrate your model with our parallelism features more friendly. Also calling ",(0,r.kt)("inlineCode",{parentName:"p"},"colossalai.booster")," is the standard procedure before you run into your training loops. In the sections below, I will cover how ",(0,r.kt)("inlineCode",{parentName:"p"},"colossalai.booster")," works and what we should take note of."),(0,r.kt)("h3",{id:"plugin"},"Plugin"),(0,r.kt)("p",null,"Plugin is an important component that manages parallel configuration (eg: The gemini plugin encapsulates the gemini acceleration solution). Currently supported plugins are as follows:"),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("em",{parentName:"strong"},"GeminiPlugin:"))," This plugin wrapps the Gemini acceleration solution, that ZeRO with chunk-based memory management."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("em",{parentName:"strong"},"TorchDDPPlugin:"))," This plugin wrapps the DDP acceleration solution, it implements data parallelism at the module level which can run across multiple machines."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("em",{parentName:"strong"},"LowLevelZeroPlugin:"))," This plugin wraps the 1/2 stage of Zero Redundancy Optimizer. Stage 1 : Shards optimizer states across data parallel workers/GPUs. Stage 2 : Shards optimizer states + gradients across data parallel workers/GPUs."),(0,r.kt)("h3",{id:"api-of-booster"},"API of booster"),(0,r.kt)(a.Cl,{mdxType:"DocStringContainer"},(0,r.kt)("div",null,(0,r.kt)(a.Dx,{type:"class",name:"colossalai.booster.Booster",source:"https://github.com/hpcaitech/ColossalAI/blob/main/src/colossalai/booster/booster.py#L20",mdxType:"Title"}),(0,r.kt)(a.Pc,{mdxType:"Signature"},"device: str = 'cuda', mixed_precision: typing.Union[colossalai.booster.mixed_precision.mixed_precision_base.MixedPrecision, str] = None, plugin: typing.Optional[colossalai.booster.plugin.plugin_base.Plugin] = None"),(0,r.kt)(a.aE,{mdxType:"Parameters"},"- **device** (str or torch.device) -- The device to run the training. Default: 'cuda'.\n- **mixed_precision** (str or MixedPrecision) -- The mixed precision to run the training. Default: None.\n  If the argument is a string, it can be 'fp16', 'fp16_apex', 'bf16', or 'fp8'.\n  'fp16' would use PyTorch AMP while `fp16_apex` would use Nvidia Apex.\n- **plugin** (Plugin) -- The plugin to run the training. Default: None.")),(0,r.kt)("div",null,(0,r.kt)(a.iz,{name:"Doc",mdxType:"Divider"}),(0,r.kt)("p",null,"Booster is a high-level API for training neural networks. It provides a unified interface for\ntraining with different precision, accelerator, and plugin."),(0,r.kt)("p",null,"Examples:"),(0,r.kt)("blockquote",null,(0,r.kt)("blockquote",{parentName:"blockquote"},(0,r.kt)("blockquote",{parentName:"blockquote"},(0,r.kt)("p",{parentName:"blockquote"},"colossalai.launch(...)\nplugin = GeminiPlugin(stage=3, ...)\nbooster = Booster(precision='fp16', plugin=plugin)"),(0,r.kt)("p",{parentName:"blockquote"},"model = GPT2()\noptimizer = Adam(model.parameters())\ndataloader = Dataloader(Dataset)\nlr_scheduler = LinearWarmupScheduler()\ncriterion = GPTLMLoss()"),(0,r.kt)("p",{parentName:"blockquote"},"model, optimizer, lr_scheduler, dataloader = booster.boost(model, optimizer, lr_scheduler, dataloader)"),(0,r.kt)("p",{parentName:"blockquote"},"for epoch in range(max_epochs):\nfor input_ids, attention_mask in dataloader:\noutputs = model(input_ids, attention_mask)\nloss = criterion(outputs.logits, input_ids)\nbooster.backward(loss, optimizer)\noptimizer.step()\nlr_scheduler.step()\noptimizer.zero_grad()"))))),(0,r.kt)("div",null,(0,r.kt)("div",null,(0,r.kt)(a.Dx,{type:"",name:"boost",source:"https://github.com/hpcaitech/ColossalAI/blob/main/src/colossalai/booster/booster.py#L96",mdxType:"Title"}),(0,r.kt)(a.Pc,{mdxType:"Signature"},"model: Module, optimizer: Optimizer, criterion: typing.Callable = None, dataloader: DataLoader = None, lr_scheduler: _LRScheduler = None"),(0,r.kt)(a.aE,{mdxType:"Parameters"},"- **model** (nn.Module) -- The model to be boosted.\n- **optimizer** (Optimizer) -- The optimizer to be boosted.\n- **criterion** (Callable) -- The criterion to be boosted.\n- **dataloader** (DataLoader) -- The dataloader to be boosted.\n- **lr_scheduler** (LRScheduler) -- The lr_scheduler to be boosted.")),(0,r.kt)("div",null,(0,r.kt)(a.iz,{name:"Doc",mdxType:"Divider"}),(0,r.kt)("p",null,"Boost the model, optimizer, criterion, lr_scheduler, and dataloader.")))),(0,r.kt)(a.Cl,{mdxType:"DocStringContainer"},(0,r.kt)("div",null,(0,r.kt)(a.Dx,{type:"",name:"colossalai.booster.Booster.boost",source:"https://github.com/hpcaitech/ColossalAI/blob/main/src/colossalai/booster/booster.py#L96",mdxType:"Title"}),(0,r.kt)(a.Pc,{mdxType:"Signature"},"model: Module, optimizer: Optimizer, criterion: typing.Callable = None, dataloader: DataLoader = None, lr_scheduler: _LRScheduler = None"),(0,r.kt)(a.aE,{mdxType:"Parameters"},"- **model** (nn.Module) -- The model to be boosted.\n- **optimizer** (Optimizer) -- The optimizer to be boosted.\n- **criterion** (Callable) -- The criterion to be boosted.\n- **dataloader** (DataLoader) -- The dataloader to be boosted.\n- **lr_scheduler** (LRScheduler) -- The lr_scheduler to be boosted.")),(0,r.kt)("div",null,(0,r.kt)(a.iz,{name:"Doc",mdxType:"Divider"}),(0,r.kt)("p",null,"Boost the model, optimizer, criterion, lr_scheduler, and dataloader."))),(0,r.kt)(a.Cl,{mdxType:"DocStringContainer"},(0,r.kt)("div",null,(0,r.kt)(a.Dx,{type:"",name:"colossalai.booster.Booster.backward",source:"https://github.com/hpcaitech/ColossalAI/blob/main/src/colossalai/booster/booster.py#L132",mdxType:"Title"}),(0,r.kt)(a.Pc,{mdxType:"Signature"},"loss: Tensor, optimizer: Optimizer")),(0,r.kt)("div",null,(0,r.kt)(a.iz,{name:"Doc",mdxType:"Divider"}))),(0,r.kt)(a.Cl,{mdxType:"DocStringContainer"},(0,r.kt)("div",null,(0,r.kt)(a.Dx,{type:"",name:"colossalai.booster.Booster.no_sync",source:"https://github.com/hpcaitech/ColossalAI/blob/main/src/colossalai/booster/booster.py#L148",mdxType:"Title"}),(0,r.kt)(a.Pc,{mdxType:"Signature"},"model: Module")),(0,r.kt)("div",null,(0,r.kt)(a.iz,{name:"Doc",mdxType:"Divider"}))),(0,r.kt)(a.Cl,{mdxType:"DocStringContainer"},(0,r.kt)("div",null,(0,r.kt)(a.Dx,{type:"",name:"colossalai.booster.Booster.save_model",source:"https://github.com/hpcaitech/ColossalAI/blob/main/src/colossalai/booster/booster.py#L156",mdxType:"Title"}),(0,r.kt)(a.Pc,{mdxType:"Signature"},"model: Module, checkpoint: str, prefix: str = None, shard: bool = False, size_per_shard: int = 1024")),(0,r.kt)("div",null,(0,r.kt)(a.iz,{name:"Doc",mdxType:"Divider"}))),(0,r.kt)(a.Cl,{mdxType:"DocStringContainer"},(0,r.kt)("div",null,(0,r.kt)(a.Dx,{type:"",name:"colossalai.booster.Booster.load_model",source:"https://github.com/hpcaitech/ColossalAI/blob/main/src/colossalai/booster/booster.py#L153",mdxType:"Title"}),(0,r.kt)(a.Pc,{mdxType:"Signature"},"model: Module, checkpoint: str, strict: bool = True")),(0,r.kt)("div",null,(0,r.kt)(a.iz,{name:"Doc",mdxType:"Divider"}))),(0,r.kt)(a.Cl,{mdxType:"DocStringContainer"},(0,r.kt)("div",null,(0,r.kt)(a.Dx,{type:"",name:"colossalai.booster.Booster.save_optimizer",source:"https://github.com/hpcaitech/ColossalAI/blob/main/src/colossalai/booster/booster.py#L167",mdxType:"Title"}),(0,r.kt)(a.Pc,{mdxType:"Signature"},"optimizer: Optimizer, checkpoint: str, shard: bool = False, size_per_shard: int = 1024")),(0,r.kt)("div",null,(0,r.kt)(a.iz,{name:"Doc",mdxType:"Divider"}))),(0,r.kt)(a.Cl,{mdxType:"DocStringContainer"},(0,r.kt)("div",null,(0,r.kt)(a.Dx,{type:"",name:"colossalai.booster.Booster.load_optimizer",source:"https://github.com/hpcaitech/ColossalAI/blob/main/src/colossalai/booster/booster.py#L164",mdxType:"Title"}),(0,r.kt)(a.Pc,{mdxType:"Signature"},"optimizer: Optimizer, checkpoint: str")),(0,r.kt)("div",null,(0,r.kt)(a.iz,{name:"Doc",mdxType:"Divider"}))),(0,r.kt)(a.Cl,{mdxType:"DocStringContainer"},(0,r.kt)("div",null,(0,r.kt)(a.Dx,{type:"",name:"colossalai.booster.Booster.save_lr_scheduler",source:"https://github.com/hpcaitech/ColossalAI/blob/main/src/colossalai/booster/booster.py#L170",mdxType:"Title"}),(0,r.kt)(a.Pc,{mdxType:"Signature"},"lr_scheduler: _LRScheduler, checkpoint: str")),(0,r.kt)("div",null,(0,r.kt)(a.iz,{name:"Doc",mdxType:"Divider"}))),(0,r.kt)(a.Cl,{mdxType:"DocStringContainer"},(0,r.kt)("div",null,(0,r.kt)(a.Dx,{type:"",name:"colossalai.booster.Booster.load_lr_scheduler",source:"https://github.com/hpcaitech/ColossalAI/blob/main/src/colossalai/booster/booster.py#L173",mdxType:"Title"}),(0,r.kt)(a.Pc,{mdxType:"Signature"},"lr_scheduler: _LRScheduler, checkpoint: str")),(0,r.kt)("div",null,(0,r.kt)(a.iz,{name:"Doc",mdxType:"Divider"}))),(0,r.kt)("h2",{id:"usage"},"Usage"),(0,r.kt)("p",null,"In a typical workflow, you should launch distributed environment at the beginning of training script and create objects needed (such as models, optimizers, loss function, data loaders etc.) firstly, then call ",(0,r.kt)("inlineCode",{parentName:"p"},"colossalai.booster")," to inject features into these objects, After that, you can use our booster APIs and these returned objects to continue the rest of your training processes."),(0,r.kt)("p",null,"A pseudo-code example is like below:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"import torch\nfrom torch.optim import SGD\nfrom torchvision.models import resnet18\n\nimport colossalai\nfrom colossalai.booster import Booster\nfrom colossalai.booster.plugin import TorchDDPPlugin\n\ndef train():\n    colossalai.launch(config=dict(), rank=rank, world_size=world_size, port=port, host='localhost')\n    plugin = TorchDDPPlugin()\n    booster = Booster(plugin=plugin)\n    model = resnet18()\n    criterion = lambda x: x.mean()\n    optimizer = SGD((model.parameters()), lr=0.001)\n    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n    model, optimizer, criterion, _, scheduler = booster.boost(model, optimizer, criterion, lr_scheduler=scheduler)\n\n    x = torch.randn(4, 3, 224, 224)\n    x = x.to('cuda')\n    output = model(x)\n    loss = criterion(output)\n    booster.backward(loss, optimizer)\n    optimizer.clip_grad_by_norm(1.0)\n    optimizer.step()\n    scheduler.step()\n\n    save_path = \"./model\"\n    booster.save_model(model, save_path, True, True, \"\", 10, use_safetensors=use_safetensors)\n\n    new_model = resnet18()\n    booster.load_model(new_model, save_path)\n")),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://github.com/hpcaitech/ColossalAI/discussions/3046"},"more design details")))}m.isMDXComponent=!0}}]);