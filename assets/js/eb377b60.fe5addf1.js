"use strict";(self.webpackChunkdemo=self.webpackChunkdemo||[]).push([[4915],{6696:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>h,contentTitle:()=>c,default:()=>N,frontMatter:()=>u,metadata:()=>f,toc:()=>g});var n=a(7462),o=a(7294),i=a(3905),r=a(398);a(814);function l(e){return o.createElement("div",{className:"docstring-container"},e.children)}function s(e){return o.createElement("div",{className:"signature"},"(",e.children,")")}function m(e){return o.createElement("h3",{className:"divider"},e.name)}function p(e){return o.createElement("div",null,o.createElement(m,{name:"Parameters"}),o.createElement(r.D,null,e.children))}function d(e){return o.createElement("div",{className:"title-container"},o.createElement("div",{className:"title-module"},o.createElement("h3",null,e.type),"\xa0 ",o.createElement("h2",null,e.name)),o.createElement("div",{className:"title-source"},"<",o.createElement("a",{href:e.source,className:"title-source"},"source"),">"))}const u={},c="NVMe offload",f={unversionedId:"features/nvme_offload",id:"features/nvme_offload",title:"NVMe offload",description:"Author: Hongxin Liu",source:"@site/i18n/en/docusaurus-plugin-content-docs/current/features/nvme_offload.md",sourceDirName:"features",slug:"/features/nvme_offload",permalink:"/docs/features/nvme_offload",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/features/nvme_offload.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Pipeline Parallel",permalink:"/docs/features/pipeline_parallel"},next:{title:"Train ViT Using Pipeline Parallelism",permalink:"/docs/advanced_tutorials/train_vit_using_pipeline_parallelism"}},h={},g=[{value:"Introduction",id:"introduction",level:2},{value:"Usage",id:"usage",level:2},{value:"Exampls",id:"exampls",level:2},{value:"API Reference",id:"api-reference",level:2}],k={toc:g},_="wrapper";function N(e){let{components:t,...a}=e;return(0,i.kt)(_,(0,n.Z)({},k,a,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"nvme-offload"},"NVMe offload"),(0,i.kt)("p",null,"Author: Hongxin Liu"),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Prerequisite:")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"/docs/features/zero_with_chunk"},"Zero Redundancy Optimizer with chunk-based memory management"))),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Related Paper")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2101.06840"},"ZeRO-Offload: Democratizing Billion-Scale Model Training")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2104.07857"},"ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning"))),(0,i.kt)("h2",{id:"introduction"},"Introduction"),(0,i.kt)("p",null,"If a model has ",(0,i.kt)("inlineCode",{parentName:"p"},"N")," parameters, when using Adam, it has ",(0,i.kt)("inlineCode",{parentName:"p"},"8N")," optimizer states. For billion-scale models, optimizer states take at least 32 GB memory. GPU memory limits the model scale we can train, which is called GPU memory wall. If we offload optimizer states to the disk, we can break through GPU memory wall."),(0,i.kt)("p",null,"We implement a user-friendly and efficient asynchronous Tensor I/O library: ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/hpcaitech/TensorNVMe"},"TensorNVMe"),". With this library, we can simply implement NVMe offload."),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"This library is compatible with all kinds of disk (HDD, SATA SSD, and NVMe SSD). As I/O bandwidth of HDD or SATA SSD is low, it's recommended to use this lib only on NVMe disk.")),(0,i.kt)("p",null,"When optimizing a parameter, we can divide the optimization process into three stages: read, compute and offload. We perform the optimization process in a pipelined fashion, which can overlap computation and I/O."),(0,i.kt)("figure",{style:{textAlign:"center"}},(0,i.kt)("img",{src:"https://s2.loli.net/2022/08/16/CvRnowrsNyB4hza.jpg"}),(0,i.kt)("figcaption",null,"Optimization process")),(0,i.kt)("h2",{id:"usage"},"Usage"),(0,i.kt)("p",null,"First, please make sure you installed ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/hpcaitech/TensorNVMe"},"TensorNVMe"),":"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-shell"},"pip install packaging\npip install tensornvme\n")),(0,i.kt)("p",null,"We implement NVMe offload of optimizer states for Adam (",(0,i.kt)("a",{parentName:"p",href:"https://colossalai.readthedocs.io/en/latest/colossalai/colossalai.nn.optimizer.cpu_adam.html"},"CPUAdam")," and ",(0,i.kt)("a",{parentName:"p",href:"https://colossalai.readthedocs.io/en/latest/colossalai/colossalai.nn.optimizer.hybrid_adam.html"},"HybridAdam"),")."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"from colossalai.nn.optimizer import CPUAdam, HybridAdam\n\noptimizer = HybridAdam(model.parameters(), lr=1e-3, nvme_offload_fraction=1.0, nvme_offload_dir='./')\n")),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"nvme_offload_fraction")," is the fraction of optimizer states to be offloaded to NVMe. ",(0,i.kt)("inlineCode",{parentName:"p"},"nvme_offload_dir")," is the directory to save NVMe offload files. If ",(0,i.kt)("inlineCode",{parentName:"p"},"nvme_offload_dir")," is ",(0,i.kt)("inlineCode",{parentName:"p"},"None"),", a random temporary directory will be used."),(0,i.kt)("p",null,"It's compatible with all parallel methods in ColossalAI."),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"\u26a0 It only offloads optimizer states on CPU. This means it only affects CPU training or Zero/Gemini with offloading.")),(0,i.kt)("h2",{id:"exampls"},"Exampls"),(0,i.kt)("p",null,"Let's start from two simple examples -- training GPT with different methods. These examples relies on ",(0,i.kt)("inlineCode",{parentName:"p"},"transformers"),"."),(0,i.kt)("p",null,"We should install dependencies first:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-shell"},"pip install psutil transformers\n")),(0,i.kt)("p",null,"First, we import essential packages and modules:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"import os\nimport time\nfrom typing import Dict, Optional\n\nimport psutil\nimport torch\nimport torch.nn as nn\nfrom transformers.models.gpt2.configuration_gpt2 import GPT2Config\nfrom transformers.models.gpt2.modeling_gpt2 import GPT2LMHeadModel\n\nimport colossalai\nfrom colossalai.nn.optimizer import HybridAdam\nfrom colossalai.zero import zero_model_wrapper, zero_optim_wrapper\nfrom colossalai.utils.model.colo_init_context import ColoInitContext\n")),(0,i.kt)("p",null,"Then we define a loss function:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"class GPTLMLoss(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.loss_fn = nn.CrossEntropyLoss()\n\n    def forward(self, logits, labels):\n        shift_logits = logits[..., :-1, :].contiguous()\n        shift_labels = labels[..., 1:].contiguous()\n        # Flatten the tokens\n        return self.loss_fn(shift_logits.view(-1, shift_logits.size(-1)),\n                            shift_labels.view(-1))\n")),(0,i.kt)("p",null,"And we define some utility functions, which generates random data, computes the number of parameters of a model and get memory usage of current process:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"def get_data(batch_size: int, seq_len: int,\n             vocab_size: int, device: Optional[str] = None) -> Dict[str, torch.Tensor]:\n    device = torch.cuda.current_device() if device is None else device\n    input_ids = torch.randint(vocab_size, (batch_size, seq_len),\n                              device=device)\n    attn_mask = torch.ones_like(input_ids)\n    return dict(input_ids=input_ids, attention_mask=attn_mask)\n\n\ndef get_model_numel(model: nn.Module) -> int:\n    return sum(p.numel() for p in model.parameters())\n\n\ndef get_mem_usage() -> int:\n    proc = psutil.Process(os.getpid())\n    return proc.memory_info().rss\n")),(0,i.kt)("p",null,"We first try to train GPT model on CPU:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"def train_cpu(nvme_offload_fraction: float = 0.0):\n    config = GPT2Config()\n    model = GPT2LMHeadModel(config)\n    criterion = GPTLMLoss()\n    optimizer = HybridAdam(model.parameters(), nvme_offload_fraction=nvme_offload_fraction)\n    print(f'Model numel: {get_model_numel(model) / 1024**3:.3f} B')\n\n    start = time.time()\n    for step in range(3):\n        data = get_data(4, 128, config.vocab_size, device='cpu')\n        outputs = model(**data)\n        loss = criterion(outputs.logits, data['input_ids'])\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        print(f'[{step}] loss: {loss.item():.3f}')\n\n    print(f'Time: {time.time() - start:.3f} s')\n    print(f'Mem usage: {get_mem_usage() / 1024**2:.3f} MB')\n")),(0,i.kt)("p",null,"Run without NVME offload:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"train_cpu(0.0)\n")),(0,i.kt)("p",null,"We may get below output:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"Model numel: 0.116 B\n[0] loss: 10.953\n[1] loss: 10.974\n[2] loss: 10.965\nTime: 7.739 s\nMem usage: 5966.445 MB\n")),(0,i.kt)("p",null,"And then run with (full) NVME offload:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"train_cpu(1.0)\n")),(0,i.kt)("p",null,"We may get:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"Model numel: 0.116 B\n[0] loss: 10.951\n[1] loss: 10.994\n[2] loss: 10.984\nTime: 8.527 s\nMem usage: 4968.016 MB\n")),(0,i.kt)("p",null,"For GPT2-S, which has 0.116 billion parameters, its optimizer states take about 0.928 GB memory. And NVME offload saves about 998 MB memory, which meets our expectations."),(0,i.kt)("p",null,"Then we can train GPT model with Gemini. The placement policy of Gemini should be ",(0,i.kt)("inlineCode",{parentName:"p"},'"auto"'),", ",(0,i.kt)("inlineCode",{parentName:"p"},'"cpu"')," or ",(0,i.kt)("inlineCode",{parentName:"p"},'"const"'),"."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"def train_gemini_cpu(nvme_offload_fraction: float = 0.0):\n    colossalai.launch_from_torch({})\n    config = GPT2Config()\n    with ColoInitContext(device=torch.cuda.current_device()):\n        model = GPT2LMHeadModel(config)\n    criterion = GPTLMLoss()\n    optimizer = HybridAdam(model.parameters(), nvme_offload_fraction=nvme_offload_fraction)\n    print(f'Model numel: {get_model_numel(model) / 1024**3:.3f} B')\n\n    gemini_config = dict(strict_ddp_mode=True, device=torch.cuda.current_device(),\n                         placement_policy='cpu', pin_memory=True, hidden_dim=config.n_embd)\n    model = zero_model_wrapper(model, zero_stage=3, gemini_config=gemini_config)\n    optimizer = zero_optim_wrapper(model, optimizer, initial_scale=2**5)\n\n    start = time.time()\n    for step in range(3):\n        data = get_data(4, 128, config.vocab_size)\n        outputs = model(**data)\n        loss = criterion(outputs.logits, data['input_ids'])\n        optimizer.backward(loss)\n        optimizer.step()\n        optimizer.zero_grad()\n        print(f'[{step}] loss: {loss.item():.3f}')\n\n    print(f'Time: {time.time() - start:.3f} s')\n    print(f'Mem usage: {get_mem_usage() / 1024**2:.3f} MB')\n")),(0,i.kt)("p",null,"Run without NVME offload:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"train_gemini_cpu(0.0)\n")),(0,i.kt)("p",null,"We may get:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"Model numel: 0.116 B\nsearching chunk configuration is completed in 0.27 s.\nused number: 118.68 MB, wasted number: 0.75 MB\ntotal wasted percentage is 0.63%\n[0] loss: 10.953\n[1] loss: 10.938\n[2] loss: 10.969\nTime: 2.997 s\nMem usage: 5592.227 MB\n")),(0,i.kt)("p",null,"And run with (full) NVME offload:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"train_gemini_cpu(1.0)\n")),(0,i.kt)("p",null,"We may get:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"Model numel: 0.116 B\nsearching chunk configuration is completed in 0.27 s.\nused number: 118.68 MB, wasted number: 0.75 MB\ntotal wasted percentage is 0.63%\n[0] loss: 10.953\n[1] loss: 10.938\n[2] loss: 10.969\nTime: 3.691 s\nMem usage: 5298.344 MB\n")),(0,i.kt)("p",null,"NVME offload saves about 294 MB memory. Note that enabling ",(0,i.kt)("inlineCode",{parentName:"p"},"pin_memory")," of Gemini can accelerate training but increase memory usage. So this result also meets our expectation. If we disable ",(0,i.kt)("inlineCode",{parentName:"p"},"pin_memory"),", we can also observe a memory usage drop about 900 MB."),(0,i.kt)("h2",{id:"api-reference"},"API Reference"),(0,i.kt)(l,{mdxType:"DocStringContainer"},(0,i.kt)("div",null,(0,i.kt)(d,{type:"class",name:"colossalai.nn.HybridAdam",source:"https://github.com/hpcaitech/ColossalAI/blob/main/src/colossalai/nn/optimizer/hybrid_adam.py#L13",mdxType:"Title"}),(0,i.kt)(s,{mdxType:"Signature"},"model_params, lr = 0.001, bias_correction = True, betas = (0.9, 0.999), eps = 1e-08, weight_decay = 0, adamw_mode = True, nvme_offload_fraction: float = 0.0, nvme_offload_dir: typing.Optional[str] = None, **defaults: typing.Any"),(0,i.kt)(p,{mdxType:"Parameters"},"- **model_params** (iterable) -- iterable of parameters of dicts defining\n  parameter groups.\n- **lr** (float, optional) -- learning rate. (default: 1e-3)\n- **betas** (Tuple[float, float], optional) -- coefficients used for computing\n  running averages of gradient and its square. (default: (0.9, 0.999))\n- **eps** (float, optional) -- term added to the denominator to improve\n  numerical stability. (default: 1e-8)\n- **weight_decay** (float, optional) -- weight decay (L2 penalty) (default: 0)\n- **amsgrad** (boolean, optional) -- whether to use the AMSGrad variant of this\n  algorithm from the paper *On the Convergence of Adam and Beyond*_\n  (default: False) NOT SUPPORTED yet in CPUAdam!\n- **adamw_mode** (boolean, optional) -- Apply L2 regularization or weight decay\n  True for decoupled weight decay(also known as AdamW) (default: True)\n- **simd_log** (boolean, optional) -- whether to show if you are using SIMD to\n  accelerate. (default: False)\n- **nvme_offload_fraction** (float, optional) -- Fraction of optimizer states to be offloaded to NVMe. Defaults to 0.0.\n- **nvme_offload_dir** (Optional[str], optional) -- Directory to save NVMe offload files.\n  If it's `None`, a random temporary directory will be used. Defaults to None.")),(0,i.kt)("div",null,(0,i.kt)(m,{name:"Doc",mdxType:"Divider"}),"Implements Adam algorithm.",(0,i.kt)("p",null,"Supports parameters updating on both GPU and CPU, depanding on the device of paramters.\nBut the parameters and gradients should on the same device:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Parameters on CPU and gradients on CPU is allowed."),(0,i.kt)("li",{parentName:"ul"},"Parameters on GPU and gradients on GPU is allowed."),(0,i.kt)("li",{parentName:"ul"},"Parameters on GPU and gradients on CPU is ",(0,i.kt)("strong",{parentName:"li"},"not")," allowed.")),(0,i.kt)("p",null,(0,i.kt)("em",{parentName:"p"},"HybriadAdam")," requires CUDA extensions which can be built during installation or runtime."),(0,i.kt)("p",null,"This version of Hybrid Adam is an hybrid of CPUAdam and FusedAdam."),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"For parameters updating on CPU, it uses CPUAdam."),(0,i.kt)("li",{parentName:"ul"},"For parameters updating on GPU, it uses FusedAdam."),(0,i.kt)("li",{parentName:"ul"},"Hybird precision calculation of fp16 and fp32 is supported, eg fp32 parameters and fp16 gradients.")),(0,i.kt)("p",null,"[",(0,i.kt)("inlineCode",{parentName:"p"},"colossalai.nn.optimizer.HybridAdam"),"]"," may be used as a drop-in replacement for ",(0,i.kt)("inlineCode",{parentName:"p"},"torch.optim.AdamW"),",\nor ",(0,i.kt)("inlineCode",{parentName:"p"},"torch.optim.Adam")," with ",(0,i.kt)("inlineCode",{parentName:"p"},"adamw_mode=False")),(0,i.kt)("p",null,"Adam was been proposed in ",(0,i.kt)("em",{parentName:"p"},"Adam: A Method for Stochastic Optimization"),"_."),(0,i.kt)("p",null,".. _Adam\\: A Method for Stochastic Optimization:\n",(0,i.kt)("a",{parentName:"p",href:"https://arxiv.org/abs/1412.6980"},"https://arxiv.org/abs/1412.6980"),"\n.. _On the Convergence of Adam and Beyond:\n",(0,i.kt)("a",{parentName:"p",href:"https://openreview.net/forum?id=ryQu7f-RZ"},"https://openreview.net/forum?id=ryQu7f-RZ")))),(0,i.kt)(l,{mdxType:"DocStringContainer"},(0,i.kt)("div",null,(0,i.kt)(d,{type:"class",name:"colossalai.nn.CPUAdam",source:"https://github.com/hpcaitech/ColossalAI/blob/main/src/colossalai/nn/optimizer/cpu_adam.py#L13",mdxType:"Title"}),(0,i.kt)(s,{mdxType:"Signature"},"model_params, lr = 0.001, bias_correction = True, betas = (0.9, 0.999), eps = 1e-08, weight_decay = 0, adamw_mode = True, nvme_offload_fraction: float = 0.0, nvme_offload_dir: typing.Optional[str] = None"),(0,i.kt)(p,{mdxType:"Parameters"},"- **model_params** (iterable) -- iterable of parameters of dicts defining\n  parameter groups.\n- **lr** (float, optional) -- learning rate. (default: 1e-3)\n- **betas** (Tuple[float, float], optional) -- coefficients used for computing\n  running averages of gradient and its square. (default: (0.9, 0.999))\n- **eps** (float, optional) -- term added to the denominator to improve\n  numerical stability. (default: 1e-8)\n- **weight_decay** (float, optional) -- weight decay (L2 penalty) (default: 0)\n- **amsgrad** (boolean, optional) -- whether to use the AMSGrad variant of this\n  algorithm from the paper *On the Convergence of Adam and Beyond*_\n  (default: False) NOT SUPPORTED yet in CPUAdam!\n- **adamw_mode** (boolean, optional) -- Apply L2 regularization or weight decay\n  True for decoupled weight decay(also known as AdamW) (default: True)\n- **simd_log** (boolean, optional) -- whether to show if you are using SIMD to\n  accelerate. (default: False)\n- **nvme_offload_fraction** (float, optional) -- Fraction of optimizer states to be offloaded to NVMe. Defaults to 0.0.\n- **nvme_offload_dir** (Optional[str], optional) -- Directory to save NVMe offload files.\n  If it's `None`, a random temporary directory will be used. Defaults to None.")),(0,i.kt)("div",null,(0,i.kt)(m,{name:"Doc",mdxType:"Divider"}),"Implements Adam algorithm.",(0,i.kt)("p",null,"Supports parameters updating on both GPU and CPU, depanding on the device of paramters.\nBut the parameters and gradients should on the same device:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Parameters on CPU and gradients on CPU is allowed."),(0,i.kt)("li",{parentName:"ul"},"Parameters on GPU and gradients on GPU is allowed."),(0,i.kt)("li",{parentName:"ul"},"Parameters on GPU and gradients on CPU is ",(0,i.kt)("strong",{parentName:"li"},"not")," allowed.")),(0,i.kt)("p",null,(0,i.kt)("em",{parentName:"p"},"CPUAdam")," requires CUDA extensions which can be built during installation or runtime."),(0,i.kt)("p",null,"This version of CPU Adam accelates parameters updating on CPU with SIMD.\nSupport of AVX2 or AVX512 is required."),(0,i.kt)("p",null,"The GPU part is implemented in an naive way."),(0,i.kt)("p",null,"CPU Adam also supports the hybrid precision calculation, eg. fp32 parameters and fp16 gradients."),(0,i.kt)("p",null,"[",(0,i.kt)("inlineCode",{parentName:"p"},"colossalai.nn.optimizer.CPUAdam"),"]"," may be used as a drop-in replacement for ",(0,i.kt)("inlineCode",{parentName:"p"},"torch.optim.AdamW"),",\nor ",(0,i.kt)("inlineCode",{parentName:"p"},"torch.optim.Adam")," with ",(0,i.kt)("inlineCode",{parentName:"p"},"adamw_mode=False")),(0,i.kt)("p",null,"Adam was been proposed in ",(0,i.kt)("em",{parentName:"p"},"Adam: A Method for Stochastic Optimization"),"_."),(0,i.kt)("p",null,".. _Adam\\: A Method for Stochastic Optimization:\n",(0,i.kt)("a",{parentName:"p",href:"https://arxiv.org/abs/1412.6980"},"https://arxiv.org/abs/1412.6980"),"\n.. _On the Convergence of Adam and Beyond:\n",(0,i.kt)("a",{parentName:"p",href:"https://openreview.net/forum?id=ryQu7f-RZ"},"https://openreview.net/forum?id=ryQu7f-RZ")))))}N.isMDXComponent=!0}}]);