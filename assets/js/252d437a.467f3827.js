"use strict";(self.webpackChunkdemo=self.webpackChunkdemo||[]).push([[8801],{6999:(e,t,a)=>{a.d(t,{Cl:()=>i,Dx:()=>d,Pc:()=>r,aE:()=>s,e_:()=>u,iz:()=>o,nT:()=>m});var n=a(7294),l=a(398);a(814);function i(e){return n.createElement("div",{className:"docstring-container"},e.children)}function r(e){return n.createElement("div",{className:"signature"},"(",e.children,")")}function o(e){return n.createElement("div",{class:"divider"},n.createElement("span",{class:"divider-text"},e.name))}function s(e){return n.createElement("div",null,n.createElement(o,{name:"Parameters"}),n.createElement(l.D,null,e.children))}function m(e){return n.createElement("div",null,n.createElement(o,{name:"Returns"}),n.createElement(l.D,null,`${e.name}: ${e.desc}`))}function d(e){return n.createElement("div",{className:"title-container"},n.createElement("div",{className:"title-module"},n.createElement("h5",null,e.type),"\xa0 ",n.createElement("h3",null,e.name)),n.createElement("div",{className:"title-source"},"<",n.createElement("a",{href:e.source,className:"title-source"},"source"),">"))}function u(e){return n.createElement("div",null,n.createElement(o,{name:"Example"}),n.createElement(l.D,null,e.code))}},9311:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>m,contentTitle:()=>o,default:()=>c,frontMatter:()=>r,metadata:()=>s,toc:()=>d});var n=a(7462),l=(a(7294),a(3905)),i=a(6999);const r={},o="Lazy initialization",s={unversionedId:"features/lazy_init",id:"features/lazy_init",title:"Lazy initialization",description:"Author: Hongxin Liu",source:"@site/i18n/en/docusaurus-plugin-content-docs/current/features/lazy_init.md",sourceDirName:"features",slug:"/features/lazy_init",permalink:"/docs/features/lazy_init",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/features/lazy_init.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"NVMe offload",permalink:"/docs/features/nvme_offload"},next:{title:"Cluster Utilities",permalink:"/docs/features/cluster_utils"}},m={},d=[{value:"Introduction",id:"introduction",level:2},{value:"Usage",id:"usage",level:2},{value:"API reference",id:"api-reference",level:3},{value:"Example",id:"example",level:3},{value:"Limitations",id:"limitations",level:2}],u={toc:d},p="wrapper";function c(e){let{components:t,...a}=e;return(0,l.kt)(p,(0,n.Z)({},u,a,{components:t,mdxType:"MDXLayout"}),(0,l.kt)("h1",{id:"lazy-initialization"},"Lazy initialization"),(0,l.kt)("p",null,"Author: ",(0,l.kt)("a",{parentName:"p",href:"https://github.com/ver217"},"Hongxin Liu")),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"Prerequisite:")),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"/docs/basics/booster_api"},"Train with booster"))),(0,l.kt)("h2",{id:"introduction"},"Introduction"),(0,l.kt)("p",null,"Lazy initialization defers model initialization. It saves memory when initializing large models."),(0,l.kt)("p",null,"If your model has ",(0,l.kt)("inlineCode",{parentName:"p"},"N")," billion parameters and your memory (or GPU memory) is ",(0,l.kt)("inlineCode",{parentName:"p"},"M")," GB, we recommend you use lazy initialization when ",(0,l.kt)("inlineCode",{parentName:"p"},"4N >= M"),". Otherwise, it is optional."),(0,l.kt)("h2",{id:"usage"},"Usage"),(0,l.kt)("p",null,"Lazy initialization must be used with booster."),(0,l.kt)("h3",{id:"api-reference"},"API reference"),(0,l.kt)(i.Cl,{mdxType:"DocStringContainer"},(0,l.kt)("div",null,(0,l.kt)(i.Dx,{type:"class",name:"colossalai.lazy.LazyInitContext",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/lazy/lazy_init.py#L472",mdxType:"Title"}),(0,l.kt)(i.Pc,{mdxType:"Signature"},"tensor_cls: typing.Union[colossalai.lazy.lazy_init._MyTensor, colossalai.lazy.lazy_init.LazyTensor] = <class 'colossalai.lazy.lazy_init.LazyTensor'>, default_device: typing.Union[torch.device, str, int, NoneType] = None"),(0,l.kt)(i.aE,{mdxType:"Parameters"},"- **tensor_cls** (Union[_MyTensor, LazyTensor], optional) -- This is only for test. Defaults to LazyTensor.\n- **default_device** (Optional[Union[torch.device, str, int]], optional) -- Defalt device for initialization.\n  If it's cuda, initilization will be accelerated, but cuda memory will be allocated. By default, it's cpu.\n  Defaults to None.")),(0,l.kt)("div",null,(0,l.kt)(i.iz,{name:"Description",mdxType:"Divider"}),"Context manager for lazy initialization. Enables initializing the model without allocating real memory."),(0,l.kt)(i.Cl,{mdxType:"DocStringContainer"},(0,l.kt)("div",null,(0,l.kt)(i.Dx,{type:"function",name:"materialize",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/lazy/lazy_init.py#L588",mdxType:"Title"}),(0,l.kt)(i.Pc,{mdxType:"Signature"},"module: Module, verbose: bool = False"),(0,l.kt)(i.aE,{mdxType:"Parameters"},"- **module** (nn.Module) -- Target `nn.Module`\n- **verbose** (bool) -- Whether to print lazy initialization rate. Defaults to False.")),(0,l.kt)("div",null,(0,l.kt)(i.iz,{name:"Description",mdxType:"Divider"}),"Initialize all `Parameter` from `LazyTensor`. This function will modify the module in-place."))),(0,l.kt)("h3",{id:"example"},"Example"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'import colossalai\nfrom colossalai.lazy import LazyInitContext\nfrom colossalai.booster import Booster\nfrom colossalai.booster.plugin import GeminiPlugin\n\nfrom transformers import LlamaForCausalLM, LlamaConfig, BertForPreTraining\n\ncolossalai.launch({})\nplugin = GeminiPlugin()\nbooster = Booster(plugin)\n\n# 1. Initialize model from scratch\n# Initialization on cuda will accelerate the initialization process but take more GPU memory.\nwith LazyInitContext(default_device="cuda"):\n    model = LlamaForCausalLM(LlamaConfig(hidden_size=64, intermediate_size=172, num_hidden_layers=4, num_attention_heads=4))\nmodel, *_ = booster.boost(model)\n\n# 2. Initialize model from pretrained\nwith LazyInitContext():\n    model = BertForPreTraining.from_pretrained("prajjwal1/bert-tiny")\nmodel, *_ = booster.boost(model)\n')),(0,l.kt)("blockquote",null,(0,l.kt)("p",{parentName:"blockquote"},"\u26a0\ufe0f Lazy initialization from pretrained is supported for colossalai>0.3.3 or main branch.")),(0,l.kt)("h2",{id:"limitations"},"Limitations"),(0,l.kt)("p",null,"As we claimed, lazy initialization must be used with booster. And only several plugins support it."),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null},"Plugin"),(0,l.kt)("th",{parentName:"tr",align:null},"Supported"),(0,l.kt)("th",{parentName:"tr",align:null},"Remarks"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"Gemini"),(0,l.kt)("td",{parentName:"tr",align:null},"Yes"),(0,l.kt)("td",{parentName:"tr",align:null})),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"Hybrid Parallel"),(0,l.kt)("td",{parentName:"tr",align:null},"Yes"),(0,l.kt)("td",{parentName:"tr",align:null})),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"Low Level Zero"),(0,l.kt)("td",{parentName:"tr",align:null},"No"),(0,l.kt)("td",{parentName:"tr",align:null},"No need")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"Torch DDP"),(0,l.kt)("td",{parentName:"tr",align:null},"No"),(0,l.kt)("td",{parentName:"tr",align:null},"Incompatible")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"Torch FSDP"),(0,l.kt)("td",{parentName:"tr",align:null},"No"),(0,l.kt)("td",{parentName:"tr",align:null},"Incompatible")))),(0,l.kt)("p",null,"Not all models can be lazily initialized. In some cases, a part of parameters/buffers may be early initialized. But don't worry, this part usually takes a small proportion of the whole model."),(0,l.kt)("p",null,"And some models are not supported at all which will raise an error. We tested models in torchvision, diffusers, timm, transformers, torchaudio and torchrec. Below models are not supported:"),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null},"Model"),(0,l.kt)("th",{parentName:"tr",align:null},"Category"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"wav2vec2_base"),(0,l.kt)("td",{parentName:"tr",align:null},"torchaudio")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"hubert_base"),(0,l.kt)("td",{parentName:"tr",align:null},"torchaudio")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"ViTModel"),(0,l.kt)("td",{parentName:"tr",align:null},"transformers")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"ViTForMaskedImageModeling"),(0,l.kt)("td",{parentName:"tr",align:null},"transformers")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"ViTForImageClassification"),(0,l.kt)("td",{parentName:"tr",align:null},"transformers")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"Blip2Model"),(0,l.kt)("td",{parentName:"tr",align:null},"transformers")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"Blip2ForConditionalGeneration"),(0,l.kt)("td",{parentName:"tr",align:null},"transformers")))))}c.isMDXComponent=!0}}]);