"use strict";(self.webpackChunkdemo=self.webpackChunkdemo||[]).push([[2111],{6999:(e,t,o)=>{o.d(t,{Cl:()=>a,Dx:()=>d,Pc:()=>r,aE:()=>l,e_:()=>c,iz:()=>s,nT:()=>p});var i=o(7294),n=o(398);o(814);function a(e){return i.createElement("div",{className:"docstring-container"},e.children)}function r(e){return i.createElement("div",{className:"signature"},"(",e.children,")")}function s(e){return i.createElement("div",{class:"divider"},i.createElement("span",{class:"divider-text"},e.name))}function l(e){return i.createElement("div",null,i.createElement(s,{name:"Parameters"}),i.createElement(n.D,null,e.children))}function p(e){return i.createElement("div",null,i.createElement(s,{name:"Returns"}),i.createElement(n.D,null,`${e.name}: ${e.desc}`))}function d(e){return i.createElement("div",{className:"title-container"},i.createElement("div",{className:"title-module"},i.createElement("h5",null,e.type),"\xa0 ",i.createElement("h3",null,e.name)),i.createElement("div",{className:"title-source"},"<",i.createElement("a",{href:e.source,className:"title-source"},"source"),">"))}function c(e){return i.createElement("div",null,i.createElement(s,{name:"Example"}),i.createElement(n.D,null,e.code))}},8423:(e,t,o)=>{o.r(t),o.d(t,{assets:()=>p,contentTitle:()=>s,default:()=>m,frontMatter:()=>r,metadata:()=>l,toc:()=>d});var i=o(7462),n=(o(7294),o(3905)),a=o(6999);const r={},s="Booster API",l={unversionedId:"basics/booster_api",id:"basics/booster_api",title:"Booster API",description:"Author: Mingyan Jiang, Jianghai Chen, Baizhou Zhang",source:"@site/i18n/en/docusaurus-plugin-content-docs/current/basics/booster_api.md",sourceDirName:"basics",slug:"/basics/booster_api",permalink:"/docs/basics/booster_api",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/basics/booster_api.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Launch Colossal-AI",permalink:"/docs/basics/launch_colossalai"},next:{title:"Booster Plugins",permalink:"/docs/basics/booster_plugins"}},p={},d=[{value:"Introduction",id:"introduction",level:2},{value:"Plugin",id:"plugin",level:3},{value:"API of booster",id:"api-of-booster",level:3},{value:"Usage",id:"usage",level:2}],c={toc:d},u="wrapper";function m(e){let{components:t,...o}=e;return(0,n.kt)(u,(0,i.Z)({},c,o,{components:t,mdxType:"MDXLayout"}),(0,n.kt)("h1",{id:"booster-api"},"Booster API"),(0,n.kt)("p",null,"Author: ",(0,n.kt)("a",{parentName:"p",href:"https://github.com/jiangmingyan"},"Mingyan Jiang"),", ",(0,n.kt)("a",{parentName:"p",href:"https://github.com/CjhHa1"},"Jianghai Chen"),", ",(0,n.kt)("a",{parentName:"p",href:"https://github.com/Fridge003"},"Baizhou Zhang")),(0,n.kt)("p",null,(0,n.kt)("strong",{parentName:"p"},"Prerequisite:")),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"/docs/concepts/distributed_training"},"Distributed Training")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"/docs/concepts/colossalai_overview"},"Colossal-AI Overview"))),(0,n.kt)("p",null,(0,n.kt)("strong",{parentName:"p"},"Example Code")),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/hpcaitech/ColossalAI/blob/main/examples/tutorial/new_api/cifar_resnet"},"Train ResNet on CIFAR-10 with Booster")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/llama2"},"Train LLaMA-1/2 on RedPajama with Booster"))),(0,n.kt)("h2",{id:"introduction"},"Introduction"),(0,n.kt)("p",null,"In our new design, ",(0,n.kt)("inlineCode",{parentName:"p"},"colossalai.booster")," replaces the role of ",(0,n.kt)("inlineCode",{parentName:"p"},"colossalai.initialize")," to inject features into your training components (e.g. model, optimizer, dataloader) seamlessly. With these new APIs, you can integrate your model with our parallelism features more friendly. Also, calling ",(0,n.kt)("inlineCode",{parentName:"p"},"colossalai.booster")," is the standard procedure before you run into your training loops. In the sections below, we will cover how ",(0,n.kt)("inlineCode",{parentName:"p"},"colossalai.booster")," works and what we should take note of."),(0,n.kt)("h3",{id:"plugin"},"Plugin"),(0,n.kt)("p",null,"Plugin is an important component that manages parallel configuration (eg: The gemini plugin encapsulates the gemini acceleration solution). Currently supported plugins are as follows:"),(0,n.kt)("p",null,(0,n.kt)("strong",{parentName:"p"},(0,n.kt)("em",{parentName:"strong"},"HybridParallelPlugin:"))," This plugin wraps the hybrid parallel training acceleration solution. It provides an interface for any combination of tensor parallel, pipeline parallel and data parallel strategies including DDP and ZeRO."),(0,n.kt)("p",null,(0,n.kt)("strong",{parentName:"p"},(0,n.kt)("em",{parentName:"strong"},"GeminiPlugin:"))," This plugin wraps the Gemini acceleration solution, that ZeRO with chunk-based memory management."),(0,n.kt)("p",null,(0,n.kt)("strong",{parentName:"p"},(0,n.kt)("em",{parentName:"strong"},"TorchDDPPlugin:"))," This plugin wraps the DDP acceleration solution of Pytorch. It implements data parallel at the module level which can run across multiple machines."),(0,n.kt)("p",null,(0,n.kt)("strong",{parentName:"p"},(0,n.kt)("em",{parentName:"strong"},"LowLevelZeroPlugin:"))," This plugin wraps the 1/2 stage of Zero Redundancy Optimizer. Stage 1 : Shards optimizer states across data parallel workers/GPUs. Stage 2 : Shards optimizer states + gradients across data parallel workers/GPUs."),(0,n.kt)("p",null,(0,n.kt)("strong",{parentName:"p"},(0,n.kt)("em",{parentName:"strong"},"TorchFSDPPlugin:"))," This plugin wraps the FSDP acceleration solution of Pytorch and can be used to train models with zero-dp."),(0,n.kt)("p",null,"More details about usages of each plugin can be found in chapter ",(0,n.kt)("a",{parentName:"p",href:"/docs/basics/booster_plugins"},"Booster Plugins"),"."),(0,n.kt)("p",null,"Some plugins support lazy initialization, which can be used to save memory when initializing large models. For more details, please see ",(0,n.kt)("a",{parentName:"p",href:"/docs/features/lazy_init"},"Lazy Initialization"),"."),(0,n.kt)("h3",{id:"api-of-booster"},"API of booster"),(0,n.kt)(a.Cl,{mdxType:"DocStringContainer"},(0,n.kt)("div",null,(0,n.kt)(a.Dx,{type:"class",name:"colossalai.booster.Booster",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/booster/booster.py#L33",mdxType:"Title"}),(0,n.kt)(a.Pc,{mdxType:"Signature"},"device: typing.Optional[str] = None, mixed_precision: typing.Union[colossalai.booster.mixed_precision.mixed_precision_base.MixedPrecision, str, NoneType] = None, plugin: typing.Optional[colossalai.booster.plugin.plugin_base.Plugin] = None"),(0,n.kt)(a.aE,{mdxType:"Parameters"},"- **device** (str or torch.device) -- The device to run the training. Default: None.\n  If plugin is not used or plugin doesn't control the device,\n  this argument will be set as training device ('cuda' will be used if argument is None).\n- **mixed_precision** (str or MixedPrecision) -- The mixed precision to run the training. Default: None.\n  If the argument is a string, it can be 'fp16', 'fp16_apex', 'bf16', or 'fp8'.\n  'fp16' would use PyTorch AMP while `fp16_apex` would use Nvidia Apex.\n- **plugin** (Plugin) -- The plugin to run the training. Default: None.")),(0,n.kt)("div",null,(0,n.kt)(a.iz,{name:"Description",mdxType:"Divider"}),(0,n.kt)("p",null,"Booster is a high-level API for training neural networks. It provides a unified interface for\ntraining with different precision, accelerator, and plugin."),(0,n.kt)(a.e_,{code:"```python\n# Following is pseudocode\n\ncolossalai.launch(...)\nplugin = GeminiPlugin(...)\nbooster = Booster(precision='fp16', plugin=plugin)\n\nmodel = GPT2()\noptimizer = HybridAdam(model.parameters())\ndataloader = plugin.prepare_dataloader(train_dataset, batch_size=8)\nlr_scheduler = LinearWarmupScheduler()\ncriterion = GPTLMLoss()\n\nmodel, optimizer, criterion, dataloader, lr_scheduler = booster.boost(model, optimizer, criterion, dataloader, lr_scheduler)\n\nfor epoch in range(max_epochs):\n    for input_ids, attention_mask in dataloader:\n        outputs = model(input_ids.cuda(), attention_mask.cuda())\n        loss = criterion(outputs.logits, input_ids)\n        booster.backward(loss, optimizer)\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n```",mdxType:"ExampleCode"})),(0,n.kt)(a.Cl,{mdxType:"DocStringContainer"},(0,n.kt)("div",null,(0,n.kt)(a.Dx,{type:"function",name:"backward",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/booster/booster.py#L175",mdxType:"Title"}),(0,n.kt)(a.Pc,{mdxType:"Signature"},"loss: Tensor, optimizer: Optimizer"),(0,n.kt)(a.aE,{mdxType:"Parameters"},"- **loss** (torch.Tensor) -- The loss for backpropagation.\n- **optimizer** (Optimizer) -- The optimizer to be updated.")),(0,n.kt)("div",null,(0,n.kt)(a.iz,{name:"Description",mdxType:"Divider"}),"Execution of backward during training step.")),(0,n.kt)(a.Cl,{mdxType:"DocStringContainer"},(0,n.kt)("div",null,(0,n.kt)(a.Dx,{type:"function",name:"boost",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/booster/booster.py#L126",mdxType:"Title"}),(0,n.kt)(a.Pc,{mdxType:"Signature"},"model: Module, optimizer: typing.Optional[torch.optim.optimizer.Optimizer] = None, criterion: typing.Optional[typing.Callable] = None, dataloader: typing.Optional[torch.utils.data.dataloader.DataLoader] = None, lr_scheduler: typing.Optional[torch.optim.lr_scheduler._LRScheduler] = None"),(0,n.kt)(a.aE,{mdxType:"Parameters"},"- **model** (nn.Module) -- Convert model into a wrapped model for distributive training.\n  The model might be decorated or partitioned by plugin's strategy after execution of this method.\n- **optimizer** (Optimizer, optional) -- Convert optimizer into a wrapped optimizer for distributive training.\n  The optimizer's param groups or states might be decorated or partitioned by plugin's strategy after execution of this method. Defaults to None.\n- **criterion** (Callable, optional) -- The function that calculates loss. Defaults to None.\n- **dataloader** (DataLoader, optional) -- The prepared dataloader for training. Defaults to None.\n- **lr_scheduler** (LRScheduler, optional) -- The learning scheduler for training. Defaults to None."),(0,n.kt)(a.nT,{name:"List[Union[nn.Module, Optimizer, LRScheduler, DataLoader]]",desc:"The list of boosted input arguments.",mdxType:"Returns"})),(0,n.kt)("div",null,(0,n.kt)(a.iz,{name:"Description",mdxType:"Divider"}),(0,n.kt)("p",null,"Wrap and inject features to the passed in model, optimizer, criterion, lr_scheduler, and dataloader."))),(0,n.kt)(a.Cl,{mdxType:"DocStringContainer"},(0,n.kt)("div",null,(0,n.kt)(a.Dx,{type:"function",name:"enable_lora",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/booster/booster.py#L240",mdxType:"Title"}),(0,n.kt)(a.Pc,{mdxType:"Signature"},"model: Module, pretrained_dir: typing.Optional[str] = None, lora_config: peft.LoraConfig = None, bnb_quantization_config: typing.Optional[colossalai.quantization.bnb_config.BnbQuantizationConfig] = None, quantize = False"),(0,n.kt)(a.aE,{mdxType:"Parameters"},"- **model** (nn.Module) -- The model to be appended with LoRA modules.\n- **pretrained_dir(str,** optional) -- The path to the pretrained directory, can be a local directory\n  or model_id of a PEFT configuration hosted inside a model repo on the Hugging Face Hub.\n  When set to None, create new lora configs and weights for the model using the passed in lora_config. Defaults to None.\n  lora_config -- (peft.LoraConfig, optional): Passed in LoraConfig for peft. Defaults to None.")),(0,n.kt)("div",null,(0,n.kt)(a.iz,{name:"Description",mdxType:"Divider"}),(0,n.kt)("p",null,"Wrap the passed in model with LoRA modules for training. If pretrained directory is provided, lora configs and weights are loaded from that directory.\nLora in ColossalAI is implemented using Huggingface peft library, so the arguments for Lora configuration are same as those of peft."))),(0,n.kt)(a.Cl,{mdxType:"DocStringContainer"},(0,n.kt)("div",null,(0,n.kt)(a.Dx,{type:"function",name:"execute_pipeline",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/booster/booster.py#L185",mdxType:"Title"}),(0,n.kt)(a.Pc,{mdxType:"Signature"},"data_iter: typing.Iterator, model: Module, criterion: typing.Callable[[typing.Any, typing.Any], torch.Tensor], optimizer: typing.Optional[torch.optim.optimizer.Optimizer] = None, return_loss: bool = True, return_outputs: bool = False"),(0,n.kt)(a.aE,{mdxType:"Parameters"},"data_iter(Iterator) -- The iterator for getting the next batch of data. Usually there are two ways to obtain this argument:\n1. wrap the dataloader to iterator through: iter(dataloader)\n2. get the next batch from dataloader, and wrap this batch to iterator: iter([batch])\n- **model** (nn.Module) -- The model to execute forward/backward, it should be a model wrapped by a plugin that supports pipeline.\ncriterion -- (Callable[[Any, Any], torch.Tensor]): Criterion to be used. It should take two arguments: model outputs and inputs, and returns loss tensor.\n'lambda y, x: loss_fn(y)' can turn a normal loss function into a valid two-argument criterion here.\n  - **optimizer** (Optimizer, optional) -- The optimizer for execution of backward. Can be None when only doing forward (i.e. evaluation). Defaults to None.\n- **return_loss** (bool, optional) -- Whether to return loss in the dict returned by this method. Defaults to True.\n- **return_output** (bool, optional) -- Whether to return Huggingface style model outputs in the dict returned by this method. Defaults to False."),(0,n.kt)(a.nT,{name:"Dict[str, Any]",desc:"Output dict in the form of &lcub;'loss': ..., 'outputs': ...}.\nret_dict['loss'] is the loss of forward if return_loss is set to True, else None.\nret_dict['outputs'] is the Huggingface style model outputs during forward if return_output is set to True, else None.",mdxType:"Returns"})),(0,n.kt)("div",null,(0,n.kt)(a.iz,{name:"Description",mdxType:"Divider"}),(0,n.kt)("p",null,"Execute forward & backward when utilizing pipeline parallel.\nReturn loss or Huggingface style model outputs if needed."),(0,n.kt)("p",null,"Warning: This function is tailored for the scenario of pipeline parallel.\nAs a result, please don't do the forward/backward pass in the conventional way (model(input)/loss.backward())\nwhen doing pipeline parallel training with booster, which will cause unexpected errors."))),(0,n.kt)(a.Cl,{mdxType:"DocStringContainer"},(0,n.kt)("div",null,(0,n.kt)(a.Dx,{type:"function",name:"load_lr_scheduler",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/booster/booster.py#L391",mdxType:"Title"}),(0,n.kt)(a.Pc,{mdxType:"Signature"},"lr_scheduler: _LRScheduler, checkpoint: str"),(0,n.kt)(a.aE,{mdxType:"Parameters"},"- **lr_scheduler** (LRScheduler) -- A lr scheduler boosted by Booster.\n- **checkpoint** (str) -- Path to the checkpoint. It must be a local file path.")),(0,n.kt)("div",null,(0,n.kt)(a.iz,{name:"Description",mdxType:"Divider"}),"Load lr scheduler from checkpoint.")),(0,n.kt)(a.Cl,{mdxType:"DocStringContainer"},(0,n.kt)("div",null,(0,n.kt)(a.Dx,{type:"function",name:"load_model",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/booster/booster.py#L291",mdxType:"Title"}),(0,n.kt)(a.Pc,{mdxType:"Signature"},"model: typing.Union[torch.nn.modules.module.Module, colossalai.interface.model.ModelWrapper], checkpoint: str, strict: bool = True"),(0,n.kt)(a.aE,{mdxType:"Parameters"},"- **model** (nn.Module or ModelWrapper) -- A model boosted by Booster.\n- **checkpoint** (str) -- Path to the checkpoint. It must be a local path.\n  It should be a directory path if the checkpoint is sharded. Otherwise, it should be a file path.\n- **strict** (bool, optional) -- whether to strictly enforce that the keys\n  in :attr:*state_dict* match the keys returned by this module's\n  [`~torch.nn.Module.state_dict`] function. Defaults to True.")),(0,n.kt)("div",null,(0,n.kt)(a.iz,{name:"Description",mdxType:"Divider"}),"Load model from checkpoint.")),(0,n.kt)(a.Cl,{mdxType:"DocStringContainer"},(0,n.kt)("div",null,(0,n.kt)(a.Dx,{type:"function",name:"load_optimizer",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/booster/booster.py#L341",mdxType:"Title"}),(0,n.kt)(a.Pc,{mdxType:"Signature"},"optimizer: Optimizer, checkpoint: str"),(0,n.kt)(a.aE,{mdxType:"Parameters"},"- **optimizer** (Optimizer) -- An optimizer boosted by Booster.\n- **checkpoint** (str) -- Path to the checkpoint. It must be a local path.\n  It should be a directory path if the checkpoint is sharded. Otherwise, it should be a file path.\n- **prefix** (str, optional) -- A prefix added to parameter and buffer\n  names to compose the keys in state_dict. Defaults to None.\n- **size_per_shard** (int, optional) -- Maximum size of checkpoint shard file in MB. This is useful only when `shard=True`. Defaults to 1024.")),(0,n.kt)("div",null,(0,n.kt)(a.iz,{name:"Description",mdxType:"Divider"}),"Load optimizer from checkpoint.")),(0,n.kt)(a.Cl,{mdxType:"DocStringContainer"},(0,n.kt)("div",null,(0,n.kt)(a.Dx,{type:"function",name:"no_sync",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/booster/booster.py#L223",mdxType:"Title"}),(0,n.kt)(a.Pc,{mdxType:"Signature"},"model: Module = None, optimizer: OptimizerWrapper = None"),(0,n.kt)(a.aE,{mdxType:"Parameters"},"- **model** (nn.Module) -- The model to be disabled gradient synchronization, for DDP\n- **optimizer** (OptimizerWrapper) -- The optimizer to be disabled gradient synchronization, for ZeRO1-1"),(0,n.kt)(a.nT,{name:"contextmanager",desc:"Context to disable gradient synchronization.",mdxType:"Returns"})),(0,n.kt)("div",null,(0,n.kt)(a.iz,{name:"Description",mdxType:"Divider"}),"Context manager to disable gradient synchronization across DP process groups. Support torch DDP and Low Level ZeRO-1 for now.")),(0,n.kt)(a.Cl,{mdxType:"DocStringContainer"},(0,n.kt)("div",null,(0,n.kt)(a.Dx,{type:"function",name:"save_lora_as_pretrained",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/booster/booster.py#L400",mdxType:"Title"}),(0,n.kt)(a.Pc,{mdxType:"Signature"},"model: typing.Union[torch.nn.modules.module.Module, colossalai.interface.model.ModelWrapper], checkpoint: str, use_safetensors: bool = False"),(0,n.kt)(a.aE,{mdxType:"Parameters"},"- **model** (Union[nn.Module, ModelWrapper]) -- A model boosted by Booster.\n- **checkpoint** (str) -- Path to the checkpoint directory. It must be a local path.\n- **use_safetensors** (bool, optional) -- Whether to use safe tensors when saving. Defaults to False.")),(0,n.kt)("div",null,(0,n.kt)(a.iz,{name:"Description",mdxType:"Divider"}),(0,n.kt)("p",null,"Save the lora adapters and adapter configuration file to a pretrained checkpoint directory."))),(0,n.kt)(a.Cl,{mdxType:"DocStringContainer"},(0,n.kt)("div",null,(0,n.kt)(a.Dx,{type:"function",name:"save_lr_scheduler",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/booster/booster.py#L382",mdxType:"Title"}),(0,n.kt)(a.Pc,{mdxType:"Signature"},"lr_scheduler: _LRScheduler, checkpoint: str"),(0,n.kt)(a.aE,{mdxType:"Parameters"},"- **lr_scheduler** (LRScheduler) -- A lr scheduler boosted by Booster.\n- **checkpoint** (str) -- Path to the checkpoint. It must be a local file path.")),(0,n.kt)("div",null,(0,n.kt)(a.iz,{name:"Description",mdxType:"Divider"}),"Save lr scheduler to checkpoint.")),(0,n.kt)(a.Cl,{mdxType:"DocStringContainer"},(0,n.kt)("div",null,(0,n.kt)(a.Dx,{type:"function",name:"save_model",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/booster/booster.py#L304",mdxType:"Title"}),(0,n.kt)(a.Pc,{mdxType:"Signature"},"model: typing.Union[torch.nn.modules.module.Module, colossalai.interface.model.ModelWrapper], checkpoint: str, shard: bool = False, gather_dtensor: bool = True, prefix: typing.Optional[str] = None, size_per_shard: int = 1024, use_safetensors: bool = False, use_async: bool = False"),(0,n.kt)(a.aE,{mdxType:"Parameters"},"- **model** (nn.Module or ModelWrapper) -- A model boosted by Booster.\n- **checkpoint** (str) -- Path to the checkpoint. It must be a local path.\n  It is a file path if `shard=False`. Otherwise, it is a directory path.\n- **shard** (bool, optional) -- Whether to save checkpoint a sharded way.\n  If true, the checkpoint will be a folder with the same format as Huggingface transformers checkpoint. Otherwise, it will be a single file. Defaults to False.\n- **gather_dtensor** (bool, optional) -- whether to gather the distributed tensor to the first device. Default: True.\n- **prefix** (str, optional) -- A prefix added to parameter and buffer\n  names to compose the keys in state_dict. Defaults to None.\n- **size_per_shard** (int, optional) -- Maximum size of checkpoint shard file in MB. This is useful only when `shard=True`. Defaults to 1024.\n- **use_safetensors** (bool, optional) -- whether to use safe tensors. Default: False. If set to True, the checkpoint will be saved.\n- **use_async** (bool, optional) -- whether to save the state_dict of model asynchronously. Default: False.")),(0,n.kt)("div",null,(0,n.kt)(a.iz,{name:"Description",mdxType:"Divider"}),"Save model to checkpoint.")),(0,n.kt)(a.Cl,{mdxType:"DocStringContainer"},(0,n.kt)("div",null,(0,n.kt)(a.Dx,{type:"function",name:"save_optimizer",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/booster/booster.py#L354",mdxType:"Title"}),(0,n.kt)(a.Pc,{mdxType:"Signature"},"optimizer: Optimizer, checkpoint: str, shard: bool = False, gather_dtensor: bool = True, prefix: typing.Optional[str] = None, size_per_shard: int = 1024, use_async: bool = False"),(0,n.kt)(a.aE,{mdxType:"Parameters"},"- **optimizer** (Optimizer) -- An optimizer boosted by Booster.\n- **checkpoint** (str) -- Path to the checkpoint. It must be a local path.\n  It is a file path if `shard=False`. Otherwise, it is a directory path.\n- **shard** (bool, optional) -- Whether to save checkpoint a sharded way.\n  If true, the checkpoint will be a folder. Otherwise, it will be a single file. Defaults to False.\n- **gather_dtensor** (bool) -- whether to gather the distributed tensor to the first device. Default: True.\n- **prefix** (str, optional) -- A prefix added to parameter and buffer\n  names to compose the keys in state_dict. Defaults to None.\n- **size_per_shard** (int, optional) -- Maximum size of checkpoint shard file in MB. This is useful only when `shard=True`. Defaults to 1024.")),(0,n.kt)("div",null,(0,n.kt)(a.iz,{name:"Description",mdxType:"Divider"}),(0,n.kt)("p",null,"Save optimizer to checkpoint.")))),(0,n.kt)("h2",{id:"usage"},"Usage"),(0,n.kt)("p",null,"In a typical workflow, you should launch distributed environment at the beginning of training script and create objects needed (such as models, optimizers, loss function, data loaders etc.) firstly, then call ",(0,n.kt)("inlineCode",{parentName:"p"},"booster.boost")," to inject features into these objects, After that, you can use our booster APIs and these returned objects to continue the rest of your training processes."),(0,n.kt)("p",null,"A pseudo-code example is like below:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},"import torch\nfrom torch.optim import SGD\nfrom torchvision.models import resnet18\n\nimport colossalai\nfrom colossalai.booster import Booster\nfrom colossalai.booster.plugin import TorchDDPPlugin\n\ndef train():\n    # launch colossalai\n    colossalai.launch(rank=rank, world_size=world_size, port=port, host='localhost')\n\n    # create plugin and objects for training\n    plugin = TorchDDPPlugin()\n    booster = Booster(plugin=plugin)\n    model = resnet18()\n    criterion = lambda x: x.mean()\n    optimizer = SGD((model.parameters()), lr=0.001)\n    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n\n    # use booster.boost to wrap the training objects\n    model, optimizer, criterion, _, scheduler = booster.boost(model, optimizer, criterion, lr_scheduler=scheduler)\n\n    # do training as normal, except that the backward should be called by booster\n    x = torch.randn(4, 3, 224, 224)\n    x = x.to('cuda')\n    output = model(x)\n    loss = criterion(output)\n    booster.backward(loss, optimizer)\n    optimizer.clip_grad_by_norm(1.0)\n    optimizer.step()\n    scheduler.step()\n    optimizer.zero_grad()\n\n    # checkpointing using booster api\n    save_path = \"./model\"\n    booster.save_model(model, save_path, shard=True, size_per_shard=10, use_safetensors=True)\n\n    new_model = resnet18()\n    booster.load_model(new_model, save_path)\n")),(0,n.kt)("p",null,"For more design details please see ",(0,n.kt)("a",{parentName:"p",href:"https://github.com/hpcaitech/ColossalAI/discussions/3046"},"this page"),"."))}m.isMDXComponent=!0}}]);