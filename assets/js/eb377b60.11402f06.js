"use strict";(self.webpackChunkdemo=self.webpackChunkdemo||[]).push([[4915],{3905:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>f});var r=n(7294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,r,a=function(e,t){if(null==e)return{};var n,r,a={},o=Object.keys(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var s=r.createContext({}),p=function(e){var t=r.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},c=function(e){var t=p(e.components);return r.createElement(s.Provider,{value:t},e.children)},m="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},u=r.forwardRef((function(e,t){var n=e.components,a=e.mdxType,o=e.originalType,s=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),m=p(n),u=a,f=m["".concat(s,".").concat(u)]||m[u]||d[u]||o;return n?r.createElement(f,i(i({ref:t},c),{},{components:n})):r.createElement(f,i({ref:t},c))}));function f(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var o=n.length,i=new Array(o);i[0]=u;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l[m]="string"==typeof e?e:a,i[1]=l;for(var p=2;p<o;p++)i[p]=n[p];return r.createElement.apply(null,i)}return r.createElement.apply(null,n)}u.displayName="MDXCreateElement"},9802:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>i,default:()=>d,frontMatter:()=>o,metadata:()=>l,toc:()=>p});var r=n(7462),a=(n(7294),n(3905));const o={},i="NVMe offload",l={unversionedId:"features/nvme_offload",id:"features/nvme_offload",title:"NVMe offload",description:"Author: Hongxin Liu",source:"@site/i18n/en/docusaurus-plugin-content-docs/current/features/nvme_offload.md",sourceDirName:"features",slug:"/features/nvme_offload",permalink:"/docs/features/nvme_offload",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/features/nvme_offload.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Pipeline Parallel",permalink:"/docs/features/pipeline_parallel"},next:{title:"Train ViT Using Pipeline Parallelism",permalink:"/docs/advanced_tutorials/train_vit_using_pipeline_parallelism"}},s={},p=[{value:"Introduction",id:"introduction",level:2},{value:"Usage",id:"usage",level:2}],c={toc:p},m="wrapper";function d(e){let{components:t,...n}=e;return(0,a.kt)(m,(0,r.Z)({},c,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"nvme-offload"},"NVMe offload"),(0,a.kt)("p",null,"Author: Hongxin Liu"),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"Prerequisite:")),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"/docs/features/zero_with_chunk"},"Zero Redundancy Optimizer with chunk-based memory management"))),(0,a.kt)("h2",{id:"introduction"},"Introduction"),(0,a.kt)("p",null,"If a model has ",(0,a.kt)("inlineCode",{parentName:"p"},"N")," parameters, when using Adam, it has ",(0,a.kt)("inlineCode",{parentName:"p"},"8N")," optimizer states. For billion-scale models, optimizer states take at least 32 GB memory. GPU memory limits the model scale we can train, which is called GPU memory wall. If we offload optimizer states to the disk, we can break through GPU memory wall."),(0,a.kt)("p",null,"We implement a user-friendly and efficient asynchronous Tensor I/O library: ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/hpcaitech/TensorNVMe"},"TensorNVMe"),". With this library, we can simply implement NVMe offload."),(0,a.kt)("blockquote",null,(0,a.kt)("p",{parentName:"blockquote"},"This library is compatible with all kinds of disk (HDD, SATA SSD, and NVMe SSD). As I/O bandwidth of HDD or SATA SSD is low, it's recommended to use this lib only on NVMe disk.")),(0,a.kt)("p",null,"When optimizing a parameter, we can divide the optimization process into three stages: read, compute and offload. We perform the optimization process in a pipelined fashion, which can overlap computation and I/O."),(0,a.kt)("figure",{style:{textAlign:"center"}},(0,a.kt)("img",{src:"https://s2.loli.net/2022/08/16/CvRnowrsNyB4hza.jpg"}),(0,a.kt)("figcaption",null,"Optimization process")),(0,a.kt)("h2",{id:"usage"},"Usage"),(0,a.kt)("p",null,"First, please make sure you installed ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/hpcaitech/TensorNVMe"},"TensorNVMe"),":"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell"},"pip install packaging\npip install tensornvme\n")),(0,a.kt)("p",null,"We implement NVMe offload of optimizer states for Adam (",(0,a.kt)("a",{parentName:"p",href:"https://colossalai.readthedocs.io/en/latest/colossalai/colossalai.nn.optimizer.cpu_adam.html"},"CPUAdam")," and ",(0,a.kt)("a",{parentName:"p",href:"https://colossalai.readthedocs.io/en/latest/colossalai/colossalai.nn.optimizer.hybrid_adam.html"},"HybridAdam"),")."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"from colossalai.nn.optimizer import CPUAdam, HybridAdam\n\noptimizer = HybridAdam(model.parameters(), lr=1e-3, nvme_offload_fraction=1.0, nvme_offload_dir='./')\n")),(0,a.kt)("p",null,(0,a.kt)("inlineCode",{parentName:"p"},"nvme_offload_fraction")," is the fraction of optimizer states to be offloaded to NVMe. ",(0,a.kt)("inlineCode",{parentName:"p"},"nvme_offload_dir")," is the directory to save NVMe offload files. If ",(0,a.kt)("inlineCode",{parentName:"p"},"nvme_offload_dir")," is ",(0,a.kt)("inlineCode",{parentName:"p"},"None"),", a random temporary directory will be used."),(0,a.kt)("p",null,"It's compatible with all parallel methods in ColossalAI."))}d.isMDXComponent=!0}}]);