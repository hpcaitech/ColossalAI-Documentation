"use strict";(self.webpackChunkdemo=self.webpackChunkdemo||[]).push([[9215],{6999:(e,a,t)=>{t.d(a,{Cl:()=>s,Dx:()=>m,Pc:()=>o,aE:()=>p,e_:()=>c,iz:()=>l,nT:()=>i});var n=t(7294),r=t(398);t(814);function s(e){return n.createElement("div",{className:"docstring-container"},e.children)}function o(e){return n.createElement("div",{className:"signature"},"(",e.children,")")}function l(e){return n.createElement("div",{class:"divider"},n.createElement("span",{class:"divider-text"},e.name))}function p(e){return n.createElement("div",null,n.createElement(l,{name:"Parameters"}),n.createElement(r.D,null,e.children))}function i(e){return n.createElement("div",null,n.createElement(l,{name:"Returns"}),n.createElement(r.D,null,`${e.name}: ${e.desc}`))}function m(e){return n.createElement("div",{className:"title-container"},n.createElement("div",{className:"title-module"},n.createElement("h5",null,e.type),"\xa0 ",n.createElement("h3",null,e.name)),n.createElement("div",{className:"title-source"},"<",n.createElement("a",{href:e.source,className:"title-source"},"source"),">"))}function c(e){return n.createElement("div",null,n.createElement(l,{name:"Example"}),n.createElement(r.D,null,e.code))}},7808:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>i,contentTitle:()=>l,default:()=>h,frontMatter:()=>o,metadata:()=>p,toc:()=>m});var n=t(7462),r=(t(7294),t(3905)),s=t(6999);const o={},l="Shardformer",p={unversionedId:"features/shardformer",id:"features/shardformer",title:"Shardformer",description:"Author: Baizhou Zhang, Bin Jia",source:"@site/i18n/en/docusaurus-plugin-content-docs/current/features/shardformer.md",sourceDirName:"features",slug:"/features/shardformer",permalink:"/docs/features/shardformer",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/features/shardformer.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Booster Checkpoint",permalink:"/docs/basics/booster_checkpoint"},next:{title:"Auto Mixed Precision Training",permalink:"/docs/features/mixed_precision_training_with_booster"}},i={},m=[{value:"Introduction",id:"introduction",level:2},{value:"Supporting Information",id:"supporting-information",level:2},{value:"Usage",id:"usage",level:2},{value:"Shardformer Configuration",id:"shardformer-configuration",level:3},{value:"Enabling Shardformer",id:"enabling-shardformer",level:3},{value:"1. Enabling Shardformer Through Booster (Recommended)",id:"1-enabling-shardformer-through-booster-recommended",level:4},{value:"2. Enabling Shardformer Through Shardformer APIs (Not Recommended)",id:"2-enabling-shardformer-through-shardformer-apis-not-recommended",level:4},{value:"Precautions",id:"precautions",level:3},{value:"How Shardformer Works",id:"how-shardformer-works",level:2},{value:"Main Idea",id:"main-idea",level:3},{value:"Sequence Parallelism",id:"sequence-parallelism",level:3}],c={toc:m},d="wrapper";function h(e){let{components:a,...t}=e;return(0,r.kt)(d,(0,n.Z)({},c,t,{components:a,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"shardformer"},"Shardformer"),(0,r.kt)("p",null,"Author: ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/Fridge003"},"Baizhou Zhang"),", ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/FoolPlayer"},"Bin Jia")),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Prerequisite")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"/docs/concepts/paradigms_of_parallelism"},"Paradigms of Parallelism")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"/docs/basics/booster_api"},"Booster API")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"/docs/basics/booster_plugins"},"Booster Plugins"))),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Example Code")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://github.com/hpcaitech/ColossalAI/tree/main/colossalai/shardformer/examples"},"Tensor Parallelism with Shardformer")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/bert"},"Enabling Shardformer using HybridPrallelPlugin"))),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Related Paper")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2104.04473"},"Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/1811.06965"},"GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2307.08691"},"FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2105.13120"},"Sequence Parallelism: Long Sequence Training from System Perspective")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2205.05198"},"Reducing Activation Recomputation in Large Transformer Models"))),(0,r.kt)("h2",{id:"introduction"},"Introduction"),(0,r.kt)("p",null,"When training large transformer models such as LLaMa-2 70B or OPT 175B, model parallelism methods that divide a huge model into smaller shards, including tensor parallelism or pipeline parallelism, are essential so as to meet the limitation of GPU memory.\nHowever, manually cutting model and rewriting its forward/backword logic could be difficult for users who are not familiar with distributed training.\nMeanwhile, the Huggingface transformers library has gradually become users' first choice of model source, and most mainstream large models have been open-sourced in Huggingface transformers model library."),(0,r.kt)("p",null,"Out of this motivation, the ColossalAI team develops ",(0,r.kt)("strong",{parentName:"p"},"Shardformer"),", a feature that automatically does preparation of model parallelism (tensor parallelism/pipeline parallelism) for popular transformer models in HuggingFace.\nThis module aims to make parallelization hassle-free for users who are not from the system background.\nWithin a few lines of codes, users can turn a model into a state ready for distributed training.\nAlso, Shardformer contains various optimization tools for acceleration and memory saving during forward/backward pass."),(0,r.kt)("h2",{id:"supporting-information"},"Supporting Information"),(0,r.kt)("p",null,"Model/Feature Compatibility Matrix:"),(0,r.kt)("table",null,(0,r.kt)("tr",null,(0,r.kt)("th",{nowrap:"nowrap"},"Model/Feature"),(0,r.kt)("th",{nowrap:"nowrap",title:"Tensor Parallel"},"Tensor",(0,r.kt)("br",null),"Parallel"),(0,r.kt)("th",{nowrap:"nowrap",align:"center",title:"Pipeline Parallel"},"Pipeline",(0,r.kt)("br",null),"Parallel"),(0,r.kt)("th",{nowrap:"nowrap",align:"center",title:"Lazy Initialization"},"Lazy",(0,r.kt)("br",null),"Initialization"),(0,r.kt)("th",{nowrap:"nowrap",align:"center",title:"xFormers"},"xFormers"),(0,r.kt)("th",{nowrap:"nowrap",align:"center",title:"Flash Attention 2"},"Flash",(0,r.kt)("br",null),"Attention 2"),(0,r.kt)("th",{nowrap:"nowrap",align:"center",title:"JIT Fused Operators"},"JIT Fused",(0,r.kt)("br",null),"Operators"),(0,r.kt)("th",{nowrap:"nowrap",align:"center",title:"Fused LayerNorm"},"Fused",(0,r.kt)("br",null),"LayerNorm"),(0,r.kt)("th",{nowrap:"nowrap",align:"center",title:"Sequence Parallel"},"Sequence",(0,r.kt)("br",null),"Parallel"),(0,r.kt)("th",{nowrap:"nowrap",align:"center",title:"Sequence Overlap"},"Sequence",(0,r.kt)("br",null),"Overlap")),(0,r.kt)("tr",null,(0,r.kt)("td",{nowrap:"nowrap"},"Llama V1/V2"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u274c")),(0,r.kt)("tr",null,(0,r.kt)("td",{nowrap:"nowrap"},"OPT"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u274c"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u274c")),(0,r.kt)("tr",null,(0,r.kt)("td",{nowrap:"nowrap"},"BLOOM"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f")),(0,r.kt)("tr",null,(0,r.kt)("td",{nowrap:"nowrap"},"ChatGLM 2"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f")),(0,r.kt)("tr",null,(0,r.kt)("td",{nowrap:"nowrap"},"BERT"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f")),(0,r.kt)("tr",null,(0,r.kt)("td",{nowrap:"nowrap"},"GPT 2"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f")),(0,r.kt)("tr",null,(0,r.kt)("td",{nowrap:"nowrap"},"T5"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u274c"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u274c")),(0,r.kt)("tr",null,(0,r.kt)("td",{nowrap:"nowrap"},"ViT"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u274c"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u274c"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u274c")),(0,r.kt)("tr",null,(0,r.kt)("td",{nowrap:"nowrap"},"Whisper"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u274c"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u274c"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u274c")),(0,r.kt)("tr",null,(0,r.kt)("td",{nowrap:"nowrap"},"SAM"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u274c"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u274c"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u274c"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u274c")),(0,r.kt)("tr",null,(0,r.kt)("td",{nowrap:"nowrap"},"Blip2"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u274c"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u274c"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u274c"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u274c")),(0,r.kt)("tr",null,(0,r.kt)("td",{nowrap:"nowrap"},"Falcon"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u274c"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u274c"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u274c")),(0,r.kt)("tr",null,(0,r.kt)("td",{colspan:"39"}))),(0,r.kt)("p",null,"List of model families we plan to support in the near future:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"RoBERTa"),(0,r.kt)("li",{parentName:"ul"},"ALBERT"),(0,r.kt)("li",{parentName:"ul"},"ERNIE"),(0,r.kt)("li",{parentName:"ul"},"GPT Neo"),(0,r.kt)("li",{parentName:"ul"},"GPT-J"),(0,r.kt)("li",{parentName:"ul"},"BEiT"),(0,r.kt)("li",{parentName:"ul"},"SwinTransformer V1/V2"),(0,r.kt)("li",{parentName:"ul"},"qwen")),(0,r.kt)("p",null,"The support matrix will grow larger as more models and optimization tools emerge in the future. If you have any suggestions on the models/optimization we should support, please feel free to mention it in ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/hpcaitech/ColossalAI/issues"},"Issues")," section of our project."),(0,r.kt)("h2",{id:"usage"},"Usage"),(0,r.kt)("h3",{id:"shardformer-configuration"},"Shardformer Configuration"),(0,r.kt)("p",null,"The configuration of Shardformer is controlled by class ",(0,r.kt)("inlineCode",{parentName:"p"},"ShardConfig"),":"),(0,r.kt)(s.Cl,{mdxType:"DocStringContainer"},(0,r.kt)("div",null,(0,r.kt)(s.Dx,{type:"class",name:"colossalai.shardformer.ShardConfig",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/shardformer/shard/shard_config.py#L17",mdxType:"Title"}),(0,r.kt)(s.Pc,{mdxType:"Signature"},"tensor_parallel_process_group: typing.Optional[torch.distributed.distributed_c10d.ProcessGroup] = None, sequence_parallel_process_group: typing.Optional[torch.distributed.distributed_c10d.ProcessGroup] = None, pipeline_stage_manager: typing.Optional[colossalai.pipeline.stage_manager.PipelineStageManager] = None, enable_tensor_parallelism: bool = True, enable_all_optimization: bool = False, enable_fused_normalization: bool = False, enable_flash_attention: bool = False, enable_jit_fused: bool = False, enable_sequence_parallelism: bool = False, sequence_parallelism_mode: str = None, enable_sequence_overlap: bool = False, parallel_output: bool = True, make_vocab_size_divisible_by: int = 64, gradient_checkpoint_config: typing.Optional[colossalai.shardformer.shard.grad_ckpt_config.GradientCheckpointConfig] = None, extra_kwargs: typing.Dict[str, typing.Any] = <factory>, inner_ring_size: typing.Optional[int] = None, moe_dp_group: typing.Optional[torch.distributed.distributed_c10d.ProcessGroup] = None, ep_group: typing.Optional[torch.distributed.distributed_c10d.ProcessGroup] = None, fp8_communication: bool = False"),(0,r.kt)(s.aE,{mdxType:"Parameters"},"- **tensor_parallel_process_group** (Optional[ProcessGroup]) -- The process group of tensor parallelism, it's necessary when using tensor parallel. Defaults to None, which is the global process group.\n- **pipeline_stage_manager** (Optional[PipelineStageManager]) -- If using pipeline parallelism, it's necessary to specify a pipeline stage manager for inter-process communication in pipeline parallelism. Defaults to None, which means not using pipeline parallelism.\n- **enable_tensor_parallelism** (bool) -- Whether to use tensor parallelism. Defaults to True.\n- **enable_fused_normalization** (bool) -- Whether to use fused layernorm. Defaults to False.\n- **enable_flash_attention** (bool, optional) -- Whether to switch on flash attention. Defaults to False.\n- **enable_jit_fused** (bool, optional) -- Whether to switch on JIT fused operators. Defaults to False.\n- **enable_sequence_parallelism** (bool) -- Whether to turn on sequence parallelism, which partitions non-tensor-parallel regions along the sequence dimension. Defaults to False.\n- **enable_sequence_overlap** (bool) -- Whether to turn on sequence overlap, which overlap the computation and communication in sequence parallelism. It can only be used when enable_sequence_parallelism is True. Defaults to False.\n- **gradient_checkpoint_config** (Optional[GradientCheckpointConfig]) -- The gradient checkpoint config. Defaults to None.\n- **enable_all_optimization** (bool) -- Whether to turn on all optimization tools including 'fused normalization', 'flash attention', 'JIT fused operators', 'sequence parallelism' and 'sequence overlap'. Defaults to False.\n- **fp8_communication** (bool, optional) -- Whether to enable fp8 communication in model parallelism. Defaults to False.\n- **parallel_output** (bool) -- For TP: whether to use parallelize cross entropy computation along the feature dim.\n  For SP: set to True to NOT gather the output along the seq dim.")),(0,r.kt)("div",null,(0,r.kt)(s.iz,{name:"Description",mdxType:"Divider"}),(0,r.kt)("p",null,"The config for sharding the huggingface model"))),(0,r.kt)("p",null,"If you want to enable Apex Fused Layernorm, please install ",(0,r.kt)("inlineCode",{parentName:"p"},"apex"),".\nIf you want to enable the usage of flash attention, please install ",(0,r.kt)("inlineCode",{parentName:"p"},"flash_attn"),".\nIn addition, xFormers's ",(0,r.kt)("inlineCode",{parentName:"p"},"cutlass_op")," can serve as a backup for flash attention."),(0,r.kt)("h3",{id:"enabling-shardformer"},"Enabling Shardformer"),(0,r.kt)("h4",{id:"1-enabling-shardformer-through-booster-recommended"},"1. Enabling Shardformer Through Booster (Recommended)"),(0,r.kt)("p",null,"Enabling ",(0,r.kt)("inlineCode",{parentName:"p"},"Shardformer")," through ",(0,r.kt)("inlineCode",{parentName:"p"},"Booster")," initialized with ",(0,r.kt)("inlineCode",{parentName:"p"},"HybridParallelPlugin")," is the recommended way to awaken the power of Shardformer.\nThe main reason is that pipeline parallelism cannot successfully work without the calling of ",(0,r.kt)("inlineCode",{parentName:"p"},"execute_pipeline")," method of ",(0,r.kt)("inlineCode",{parentName:"p"},"Booster"),". Besides, ",(0,r.kt)("inlineCode",{parentName:"p"},"HybridParallelPlugin")," provides the capacity to combine the features of ",(0,r.kt)("inlineCode",{parentName:"p"},"Shardformer")," with other useful features, such as mixed precision training or Zero."),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/bert"},"Here")," is an example on how to trigger ",(0,r.kt)("inlineCode",{parentName:"p"},"Shardformer")," through ",(0,r.kt)("inlineCode",{parentName:"p"},"HybridParallelPlugin"),". Move to the root directory of this example, and execute"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},'torchrun --standalone --nproc_per_node 4  finetune.py --target_f1 0.86 --plugin "hybrid_parallel" --model_type "bert"\n')),(0,r.kt)("p",null,"Then you can start finetuning a bert model wrapped by ",(0,r.kt)("inlineCode",{parentName:"p"},"Shardformer"),". The process of wrapping is operated by ",(0,r.kt)("inlineCode",{parentName:"p"},"HybridParallelPlugin"),"."),(0,r.kt)("p",null,"Let's delve into the code of ",(0,r.kt)("inlineCode",{parentName:"p"},"finetune.py"),":"),(0,r.kt)("p",null,"In the ",(0,r.kt)("inlineCode",{parentName:"p"},"main")," function, the plugin is created through the following codes:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'...\nelif args.plugin == "hybrid_parallel":\n    # modify the param accordingly for finetuning test cases\n    plugin = HybridParallelPlugin(\n        tp_size=1,\n        pp_size=2,\n        num_microbatches=None,\n        microbatch_size=1,\n        enable_all_optimization=True,\n        zero_stage=1,\n        precision="fp16",\n        initial_scale=1,\n    )\n')),(0,r.kt)("p",null,"Here you can change the configuration of plugin by setting ",(0,r.kt)("inlineCode",{parentName:"p"},"tp_size"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"pp_size")," or ",(0,r.kt)("inlineCode",{parentName:"p"},"zero_stage")," to other values. More details about plugin configuration can be found in ",(0,r.kt)("a",{parentName:"p",href:"/docs/basics/booster_plugins"},"Booster Plugins Doc"),"."),(0,r.kt)("p",null,"If pipeline parallel is not enabled, just do the training in the same way of other booster plugins(first boost with Booster, then do forward and backward through normal way).\nHowever, if pipeline parallel is enabled, there are several usages different from other normal cases:"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Before doing forward or backward, the criterion function (loss function) is processed to meet the argument demand of running pipeline:"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-python"},"def _criterion(outputs, inputs):\n    outputs = output_transform_fn(outputs)\n    loss = criterion(outputs)\n    return loss\n"))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"In ",(0,r.kt)("inlineCode",{parentName:"p"},"train_epoch")," function, dataloader is converted into ",(0,r.kt)("inlineCode",{parentName:"p"},"Iterator")," class before running pipeline:"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-python"},"train_dataloader_iter = iter(train_dataloader)\n"))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Do forward and backward passing through calling ",(0,r.kt)("inlineCode",{parentName:"p"},"Booster.execute_pipeline")," method:"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-python"},"outputs = booster.execute_pipeline(\n    train_dataloader_iter, model, _criterion, optimizer, return_loss=True\n)\n")),(0,r.kt)("p",{parentName:"li"},"Backward passing has been completed by this method, so there is no need to call ",(0,r.kt)("inlineCode",{parentName:"p"},"loss.backward()")," after executing this method.\nMore details about ",(0,r.kt)("inlineCode",{parentName:"p"},"Booster.execute_pipeline")," can be found in ",(0,r.kt)("a",{parentName:"p",href:"/docs/basics/booster_api"},"Booster API Doc"),"."))),(0,r.kt)("h4",{id:"2-enabling-shardformer-through-shardformer-apis-not-recommended"},"2. Enabling Shardformer Through Shardformer APIs (Not Recommended)"),(0,r.kt)("p",null,"You can also use Shardformer through manually calling Shardformer APIs. However, this usage is not recommended since pipeline parallelism can't run without ",(0,r.kt)("inlineCode",{parentName:"p"},"Booster"),"."),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/shardformer/examples/convergence_benchmark.py"},"Here"),"\nis an example on how to trigger ",(0,r.kt)("inlineCode",{parentName:"p"},"Shardformer")," through calling Shardformer APIs. In the ",(0,r.kt)("inlineCode",{parentName:"p"},"train")," function of example code, the model is wrapped by ",(0,r.kt)("inlineCode",{parentName:"p"},"Shardformer")," through the following few codes:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'...\nif dist.get_world_size() > 1:\n    tp_group = dist.new_group(backend="nccl")\n\n    # First create configuration for Shardformer\n    shard_config = ShardConfig(\n        tensor_parallel_process_group=tp_group,\n        enable_tensor_parallelism=True,\n        enable_all_optimization=True\n    )\n\n    # Then create ShardFormer object with created config\n    shard_former = ShardFormer(shard_config=shard_config)\n\n    # Finally shard the model using ShardFormer.optimize method\n    model, _ = shard_former.optimize(model)\n...\n')),(0,r.kt)("h3",{id:"precautions"},"Precautions"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"When enabling pipeline parallel, please don't do the forward/backward pass in the conventional way (",(0,r.kt)("inlineCode",{parentName:"p"},"model(input)"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"loss.backward()"),"), which will cause unexpected errors. Rather, please do forward/backward pass through calling ",(0,r.kt)("inlineCode",{parentName:"p"},"booster.execute_pipeline")," method.")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"When you use Shardformer to process classification models such as ",(0,r.kt)("inlineCode",{parentName:"p"},"GPT2ForSequenceClassification"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"ViTForImageClassification"),", please ensure that the total number of labels should be integer multiple of tensor parallel size, otherwise Shardformer can't process the classifier layer correctly. A simple fix could be appending dummy labels in transformers config. This bug will be fixed in future version of Shardformer."))),(0,r.kt)("h2",{id:"how-shardformer-works"},"How Shardformer Works"),(0,r.kt)("h3",{id:"main-idea"},"Main Idea"),(0,r.kt)("p",null,"Generally, Shardformer works through the following four kinds of ",(0,r.kt)("em",{parentName:"p"},"replacements"),":"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Replacing original PyTorch module (e.g. ",(0,r.kt)("inlineCode",{parentName:"p"},"nn.Linear"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"nn.Embedding"),") with a crafted distributed module.\nThe distributed module keeps the same attributes as the original module but replaces the original parameters with distributed parameters.\nAlso, new ",(0,r.kt)("inlineCode",{parentName:"p"},"forward")," methods will replace original ones so as to execute distributed computation, such as linear layers' split /gather operations executed under tensor parallelism.\nEach distributed module implements its ",(0,r.kt)("inlineCode",{parentName:"p"},"from_native_module")," static method to convert the PyTorch module to its corresponding distributed module.")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Replacing attributes of original Huggingface Transformers layers with appropriate attributes for distributed training.\nFor example, when training LlaMa-2 with tensor parallel size as 2, the attribute ",(0,r.kt)("inlineCode",{parentName:"p"},"num_heads")," of ",(0,r.kt)("inlineCode",{parentName:"p"},"LlamaDecoderLayer")," (the number of attention heads in each layer) should be replaced with ",(0,r.kt)("inlineCode",{parentName:"p"},"model.config.num_attention_heads // 2"),".")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Replacing the ",(0,r.kt)("inlineCode",{parentName:"p"},"forward")," methods implemented by original Huggingface\nTransformers libraries with our customized ",(0,r.kt)("inlineCode",{parentName:"p"},"forward")," methods.\nThis replacement is essential for pipeline parallelism, where a customized function is needed to pass intermediate hidden states between different pipeline stages.\nAlso, optimization methods such as flash attention or sequence parallel can be injected into the ",(0,r.kt)("inlineCode",{parentName:"p"},"forward")," process through our customized ",(0,r.kt)("inlineCode",{parentName:"p"},"forward")," method.")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Replacing the whole copy of model parameters and optimizer states with incomplete ones controlled by current device (this is why it's called Shardformer).\nBy executing ",(0,r.kt)("inlineCode",{parentName:"p"},"ModelSharder.shard")," method, current device will only keep the part of model parameters it's supposed to take care of.\nTo be specific, they should be the assigned parameter shards when using tensor parallelism, or the parameters belonging to current pipeline stage when using pipeline parallelism, or both of them.\nAll other parameters are released so as to liberate memory usage.\nAs a result, the optimizer will only compute the states corresponding to these part of parameters, causing the usage of memory to be further saved."))),(0,r.kt)("p",null,"All of these replacements are implemented with manually written policies and forward functions.\nIf you want to delve deeper into the design of Shardformer or customize your own Shardformer policies, please refer to our ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/shardformer/README.md"},"Shardformer development document")," and ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/hpcaitech/ColossalAI/discussions/4050"},"pipeline parallelism design")," for more details."),(0,r.kt)("h3",{id:"sequence-parallelism"},"Sequence Parallelism"),(0,r.kt)("p",null,"Sequence parallelism is a special optimization method supported by ",(0,r.kt)("inlineCode",{parentName:"p"},"Shardformer"),". Sequence parallelism in ",(0,r.kt)("inlineCode",{parentName:"p"},"Shardformer")," is a little different from ",(0,r.kt)("a",{parentName:"p",href:"https://colossalai.org/docs/basics/configure_parallelization/#sequence-parallel"},"this one")," which focuses on ring attention. In ",(0,r.kt)("inlineCode",{parentName:"p"},"Shardformer"),", sequence parallelism is only used along with 1D tensor parallelism to further reduce memory occupation of activation tensors during computation."),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"In normal ",(0,r.kt)("a",{parentName:"p",href:"https://colossalai.org/docs/features/1D_tensor_parallel"},"1D tensor parallel"),", there are 2 communication operations, ",(0,r.kt)("span",{parentName:"p",className:"math math-inline"},(0,r.kt)("span",{parentName:"span",className:"katex"},(0,r.kt)("span",{parentName:"span",className:"katex-mathml"},(0,r.kt)("math",{parentName:"span",xmlns:"http://www.w3.org/1998/Math/MathML"},(0,r.kt)("semantics",{parentName:"math"},(0,r.kt)("mrow",{parentName:"semantics"},(0,r.kt)("mi",{parentName:"mrow"},"g")),(0,r.kt)("annotation",{parentName:"semantics",encoding:"application/x-tex"},"g")))),(0,r.kt)("span",{parentName:"span",className:"katex-html","aria-hidden":"true"},(0,r.kt)("span",{parentName:"span",className:"base"},(0,r.kt)("span",{parentName:"span",className:"strut",style:{height:"0.625em",verticalAlign:"-0.19444em"}}),(0,r.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.03588em"}},"g")))))," and ",(0,r.kt)("span",{parentName:"p",className:"math math-inline"},(0,r.kt)("span",{parentName:"span",className:"katex"},(0,r.kt)("span",{parentName:"span",className:"katex-mathml"},(0,r.kt)("math",{parentName:"span",xmlns:"http://www.w3.org/1998/Math/MathML"},(0,r.kt)("semantics",{parentName:"math"},(0,r.kt)("mrow",{parentName:"semantics"},(0,r.kt)("mover",{parentName:"mrow",accent:"true"},(0,r.kt)("mi",{parentName:"mover"},"g"),(0,r.kt)("mo",{parentName:"mover"},"\u20d7"))),(0,r.kt)("annotation",{parentName:"semantics",encoding:"application/x-tex"},"\\vec{g}")))),(0,r.kt)("span",{parentName:"span",className:"katex-html","aria-hidden":"true"},(0,r.kt)("span",{parentName:"span",className:"base"},(0,r.kt)("span",{parentName:"span",className:"strut",style:{height:"0.9084399999999999em",verticalAlign:"-0.19444em"}}),(0,r.kt)("span",{parentName:"span",className:"mord accent"},(0,r.kt)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,r.kt)("span",{parentName:"span",className:"vlist-r"},(0,r.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.714em"}},(0,r.kt)("span",{parentName:"span",style:{top:"-3em"}},(0,r.kt)("span",{parentName:"span",className:"pstrut",style:{height:"3em"}}),(0,r.kt)("span",{parentName:"span",className:"mord"},(0,r.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.03588em"}},"g"))),(0,r.kt)("span",{parentName:"span",style:{top:"-3em"}},(0,r.kt)("span",{parentName:"span",className:"pstrut",style:{height:"3em"}}),(0,r.kt)("span",{parentName:"span",className:"accent-body",style:{left:"-0.20772em"}},(0,r.kt)("span",{parentName:"span",className:"overlay",style:{height:"0.714em",width:"0.471em"}},(0,r.kt)("svg",{parentName:"span",width:"0.471em",height:"0.714em",style:{width:"0.471em"},viewBox:"0 0 471 714",preserveAspectRatio:"xMinYMin"},(0,r.kt)("path",{parentName:"svg",d:"M377 20c0-5.333 1.833-10 5.5-14S391 0 397 0c4.667 0 8.667 1.667 12 5\n3.333 2.667 6.667 9 10 19 6.667 24.667 20.333 43.667 41 57 7.333 4.667 11\n10.667 11 18 0 6-1 10-3 12s-6.667 5-14 9c-28.667 14.667-53.667 35.667-75 63\n-1.333 1.333-3.167 3.5-5.5 6.5s-4 4.833-5 5.5c-1 .667-2.5 1.333-4.5 2s-4.333 1\n-7 1c-4.667 0-9.167-1.833-13.5-5.5S337 184 337 178c0-12.667 15.667-32.333 47-59\nH213l-171-1c-8.667-6-13-12.333-13-19 0-4.667 4.333-11.333 13-20h359\nc-16-25.333-24-45-24-59z"})))))),(0,r.kt)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,r.kt)("span",{parentName:"span",className:"vlist-r"},(0,r.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.19444em"}},(0,r.kt)("span",{parentName:"span"}))))))))),", ",(0,r.kt)("span",{parentName:"p",className:"math math-inline"},(0,r.kt)("span",{parentName:"span",className:"katex"},(0,r.kt)("span",{parentName:"span",className:"katex-mathml"},(0,r.kt)("math",{parentName:"span",xmlns:"http://www.w3.org/1998/Math/MathML"},(0,r.kt)("semantics",{parentName:"math"},(0,r.kt)("mrow",{parentName:"semantics"},(0,r.kt)("mi",{parentName:"mrow"},"g")),(0,r.kt)("annotation",{parentName:"semantics",encoding:"application/x-tex"},"g")))),(0,r.kt)("span",{parentName:"span",className:"katex-html","aria-hidden":"true"},(0,r.kt)("span",{parentName:"span",className:"base"},(0,r.kt)("span",{parentName:"span",className:"strut",style:{height:"0.625em",verticalAlign:"-0.19444em"}}),(0,r.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.03588em"}},"g")))))," will do one time All-Reduce in backward to get all gradients from all the devices and ",(0,r.kt)("span",{parentName:"p",className:"math math-inline"},(0,r.kt)("span",{parentName:"span",className:"katex"},(0,r.kt)("span",{parentName:"span",className:"katex-mathml"},(0,r.kt)("math",{parentName:"span",xmlns:"http://www.w3.org/1998/Math/MathML"},(0,r.kt)("semantics",{parentName:"math"},(0,r.kt)("mrow",{parentName:"semantics"},(0,r.kt)("mover",{parentName:"mrow",accent:"true"},(0,r.kt)("mi",{parentName:"mover"},"g"),(0,r.kt)("mo",{parentName:"mover"},"\u20d7"))),(0,r.kt)("annotation",{parentName:"semantics",encoding:"application/x-tex"},"\\vec{g}")))),(0,r.kt)("span",{parentName:"span",className:"katex-html","aria-hidden":"true"},(0,r.kt)("span",{parentName:"span",className:"base"},(0,r.kt)("span",{parentName:"span",className:"strut",style:{height:"0.9084399999999999em",verticalAlign:"-0.19444em"}}),(0,r.kt)("span",{parentName:"span",className:"mord accent"},(0,r.kt)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,r.kt)("span",{parentName:"span",className:"vlist-r"},(0,r.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.714em"}},(0,r.kt)("span",{parentName:"span",style:{top:"-3em"}},(0,r.kt)("span",{parentName:"span",className:"pstrut",style:{height:"3em"}}),(0,r.kt)("span",{parentName:"span",className:"mord"},(0,r.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.03588em"}},"g"))),(0,r.kt)("span",{parentName:"span",style:{top:"-3em"}},(0,r.kt)("span",{parentName:"span",className:"pstrut",style:{height:"3em"}}),(0,r.kt)("span",{parentName:"span",className:"accent-body",style:{left:"-0.20772em"}},(0,r.kt)("span",{parentName:"span",className:"overlay",style:{height:"0.714em",width:"0.471em"}},(0,r.kt)("svg",{parentName:"span",width:"0.471em",height:"0.714em",style:{width:"0.471em"},viewBox:"0 0 471 714",preserveAspectRatio:"xMinYMin"},(0,r.kt)("path",{parentName:"svg",d:"M377 20c0-5.333 1.833-10 5.5-14S391 0 397 0c4.667 0 8.667 1.667 12 5\n3.333 2.667 6.667 9 10 19 6.667 24.667 20.333 43.667 41 57 7.333 4.667 11\n10.667 11 18 0 6-1 10-3 12s-6.667 5-14 9c-28.667 14.667-53.667 35.667-75 63\n-1.333 1.333-3.167 3.5-5.5 6.5s-4 4.833-5 5.5c-1 .667-2.5 1.333-4.5 2s-4.333 1\n-7 1c-4.667 0-9.167-1.833-13.5-5.5S337 184 337 178c0-12.667 15.667-32.333 47-59\nH213l-171-1c-8.667-6-13-12.333-13-19 0-4.667 4.333-11.333 13-20h359\nc-16-25.333-24-45-24-59z"})))))),(0,r.kt)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,r.kt)("span",{parentName:"span",className:"vlist-r"},(0,r.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.19444em"}},(0,r.kt)("span",{parentName:"span"})))))))))," will do one time All-Reduce in forward to get whole outputs from all the devices.")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"When using sequence parallelism, ",(0,r.kt)("span",{parentName:"p",className:"math math-inline"},(0,r.kt)("span",{parentName:"span",className:"katex"},(0,r.kt)("span",{parentName:"span",className:"katex-mathml"},(0,r.kt)("math",{parentName:"span",xmlns:"http://www.w3.org/1998/Math/MathML"},(0,r.kt)("semantics",{parentName:"math"},(0,r.kt)("mrow",{parentName:"semantics"},(0,r.kt)("mover",{parentName:"mrow",accent:"true"},(0,r.kt)("mi",{parentName:"mover"},"g"),(0,r.kt)("mo",{parentName:"mover"},"\u20d7"))),(0,r.kt)("annotation",{parentName:"semantics",encoding:"application/x-tex"},"\\vec{g}")))),(0,r.kt)("span",{parentName:"span",className:"katex-html","aria-hidden":"true"},(0,r.kt)("span",{parentName:"span",className:"base"},(0,r.kt)("span",{parentName:"span",className:"strut",style:{height:"0.9084399999999999em",verticalAlign:"-0.19444em"}}),(0,r.kt)("span",{parentName:"span",className:"mord accent"},(0,r.kt)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,r.kt)("span",{parentName:"span",className:"vlist-r"},(0,r.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.714em"}},(0,r.kt)("span",{parentName:"span",style:{top:"-3em"}},(0,r.kt)("span",{parentName:"span",className:"pstrut",style:{height:"3em"}}),(0,r.kt)("span",{parentName:"span",className:"mord"},(0,r.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.03588em"}},"g"))),(0,r.kt)("span",{parentName:"span",style:{top:"-3em"}},(0,r.kt)("span",{parentName:"span",className:"pstrut",style:{height:"3em"}}),(0,r.kt)("span",{parentName:"span",className:"accent-body",style:{left:"-0.20772em"}},(0,r.kt)("span",{parentName:"span",className:"overlay",style:{height:"0.714em",width:"0.471em"}},(0,r.kt)("svg",{parentName:"span",width:"0.471em",height:"0.714em",style:{width:"0.471em"},viewBox:"0 0 471 714",preserveAspectRatio:"xMinYMin"},(0,r.kt)("path",{parentName:"svg",d:"M377 20c0-5.333 1.833-10 5.5-14S391 0 397 0c4.667 0 8.667 1.667 12 5\n3.333 2.667 6.667 9 10 19 6.667 24.667 20.333 43.667 41 57 7.333 4.667 11\n10.667 11 18 0 6-1 10-3 12s-6.667 5-14 9c-28.667 14.667-53.667 35.667-75 63\n-1.333 1.333-3.167 3.5-5.5 6.5s-4 4.833-5 5.5c-1 .667-2.5 1.333-4.5 2s-4.333 1\n-7 1c-4.667 0-9.167-1.833-13.5-5.5S337 184 337 178c0-12.667 15.667-32.333 47-59\nH213l-171-1c-8.667-6-13-12.333-13-19 0-4.667 4.333-11.333 13-20h359\nc-16-25.333-24-45-24-59z"})))))),(0,r.kt)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,r.kt)("span",{parentName:"span",className:"vlist-r"},(0,r.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.19444em"}},(0,r.kt)("span",{parentName:"span"})))))))))," needs to do All-Gather to gather the inputs along sequence dimension during forward, and Reduce-Scatter to split the gradient during backward. ",(0,r.kt)("span",{parentName:"p",className:"math math-inline"},(0,r.kt)("span",{parentName:"span",className:"katex"},(0,r.kt)("span",{parentName:"span",className:"katex-mathml"},(0,r.kt)("math",{parentName:"span",xmlns:"http://www.w3.org/1998/Math/MathML"},(0,r.kt)("semantics",{parentName:"math"},(0,r.kt)("mrow",{parentName:"semantics"},(0,r.kt)("mover",{parentName:"mrow",accent:"true"},(0,r.kt)("mi",{parentName:"mover"},"g"),(0,r.kt)("mo",{parentName:"mover"},"\u20d7"))),(0,r.kt)("annotation",{parentName:"semantics",encoding:"application/x-tex"},"\\vec{g}")))),(0,r.kt)("span",{parentName:"span",className:"katex-html","aria-hidden":"true"},(0,r.kt)("span",{parentName:"span",className:"base"},(0,r.kt)("span",{parentName:"span",className:"strut",style:{height:"0.9084399999999999em",verticalAlign:"-0.19444em"}}),(0,r.kt)("span",{parentName:"span",className:"mord accent"},(0,r.kt)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,r.kt)("span",{parentName:"span",className:"vlist-r"},(0,r.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.714em"}},(0,r.kt)("span",{parentName:"span",style:{top:"-3em"}},(0,r.kt)("span",{parentName:"span",className:"pstrut",style:{height:"3em"}}),(0,r.kt)("span",{parentName:"span",className:"mord"},(0,r.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.03588em"}},"g"))),(0,r.kt)("span",{parentName:"span",style:{top:"-3em"}},(0,r.kt)("span",{parentName:"span",className:"pstrut",style:{height:"3em"}}),(0,r.kt)("span",{parentName:"span",className:"accent-body",style:{left:"-0.20772em"}},(0,r.kt)("span",{parentName:"span",className:"overlay",style:{height:"0.714em",width:"0.471em"}},(0,r.kt)("svg",{parentName:"span",width:"0.471em",height:"0.714em",style:{width:"0.471em"},viewBox:"0 0 471 714",preserveAspectRatio:"xMinYMin"},(0,r.kt)("path",{parentName:"svg",d:"M377 20c0-5.333 1.833-10 5.5-14S391 0 397 0c4.667 0 8.667 1.667 12 5\n3.333 2.667 6.667 9 10 19 6.667 24.667 20.333 43.667 41 57 7.333 4.667 11\n10.667 11 18 0 6-1 10-3 12s-6.667 5-14 9c-28.667 14.667-53.667 35.667-75 63\n-1.333 1.333-3.167 3.5-5.5 6.5s-4 4.833-5 5.5c-1 .667-2.5 1.333-4.5 2s-4.333 1\n-7 1c-4.667 0-9.167-1.833-13.5-5.5S337 184 337 178c0-12.667 15.667-32.333 47-59\nH213l-171-1c-8.667-6-13-12.333-13-19 0-4.667 4.333-11.333 13-20h359\nc-16-25.333-24-45-24-59z"})))))),(0,r.kt)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,r.kt)("span",{parentName:"span",className:"vlist-r"},(0,r.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.19444em"}},(0,r.kt)("span",{parentName:"span"})))))))))," needs to do Reduce-Scatter to split the output of ",(0,r.kt)("inlineCode",{parentName:"p"},"Row Linear")," layer of tensor parallel to all devices along sequence dimension, and All-Gather to get the whole gradient during backward.")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"NCCL's implementation of All-Reduce adopts the ",(0,r.kt)("inlineCode",{parentName:"p"},"Ring All-Reduce")," approach, which consists of a Reduce-Scatter operation and an All-Gather operation with equal costs. Therefore, compared with sequence parallelism and tensor parallelism, it does not introduce additional communication overhead.")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"One important thing to note is that when using sequence parallelism along with ",(0,r.kt)("inlineCode",{parentName:"p"},"Column Linear")," module of tensor parallelism, the complete input needs to be obtained during the backward computation of gradients. During the forward pass, only the portion of the input that is split along the sequence dimension is retained, in the shape of ",(0,r.kt)("span",{parentName:"p",className:"math math-inline"},(0,r.kt)("span",{parentName:"span",className:"katex"},(0,r.kt)("span",{parentName:"span",className:"katex-mathml"},(0,r.kt)("math",{parentName:"span",xmlns:"http://www.w3.org/1998/Math/MathML"},(0,r.kt)("semantics",{parentName:"math"},(0,r.kt)("mrow",{parentName:"semantics"},(0,r.kt)("mo",{parentName:"mrow",stretchy:"false"},"("),(0,r.kt)("mi",{parentName:"mrow"},"b"),(0,r.kt)("mi",{parentName:"mrow"},"a"),(0,r.kt)("mi",{parentName:"mrow"},"t"),(0,r.kt)("mi",{parentName:"mrow"},"c"),(0,r.kt)("mi",{parentName:"mrow"},"h"),(0,r.kt)("mo",{parentName:"mrow",separator:"true"},","),(0,r.kt)("mi",{parentName:"mrow"},"s"),(0,r.kt)("mi",{parentName:"mrow"},"e"),(0,r.kt)("mi",{parentName:"mrow"},"q"),(0,r.kt)("mi",{parentName:"mrow"},"u"),(0,r.kt)("mi",{parentName:"mrow"},"e"),(0,r.kt)("mi",{parentName:"mrow"},"n"),(0,r.kt)("mi",{parentName:"mrow"},"c"),(0,r.kt)("msub",{parentName:"mrow"},(0,r.kt)("mi",{parentName:"msub"},"e"),(0,r.kt)("mi",{parentName:"msub"},"l")),(0,r.kt)("mi",{parentName:"mrow"},"e"),(0,r.kt)("mi",{parentName:"mrow"},"n"),(0,r.kt)("mi",{parentName:"mrow",mathvariant:"normal"},"/"),(0,r.kt)("mi",{parentName:"mrow"},"k"),(0,r.kt)("mo",{parentName:"mrow",separator:"true"},","),(0,r.kt)("mi",{parentName:"mrow"},"h"),(0,r.kt)("mi",{parentName:"mrow"},"i"),(0,r.kt)("mi",{parentName:"mrow"},"d"),(0,r.kt)("mi",{parentName:"mrow"},"d"),(0,r.kt)("mi",{parentName:"mrow"},"e"),(0,r.kt)("msub",{parentName:"mrow"},(0,r.kt)("mi",{parentName:"msub"},"n"),(0,r.kt)("mi",{parentName:"msub"},"s")),(0,r.kt)("mi",{parentName:"mrow"},"t"),(0,r.kt)("mi",{parentName:"mrow"},"a"),(0,r.kt)("mi",{parentName:"mrow"},"t"),(0,r.kt)("mi",{parentName:"mrow"},"e"),(0,r.kt)("mi",{parentName:"mrow"},"s"),(0,r.kt)("mo",{parentName:"mrow",stretchy:"false"},")")),(0,r.kt)("annotation",{parentName:"semantics",encoding:"application/x-tex"},"(batch, sequence_len/k, hidden_states)")))),(0,r.kt)("span",{parentName:"span",className:"katex-html","aria-hidden":"true"},(0,r.kt)("span",{parentName:"span",className:"base"},(0,r.kt)("span",{parentName:"span",className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,r.kt)("span",{parentName:"span",className:"mopen"},"("),(0,r.kt)("span",{parentName:"span",className:"mord mathnormal"},"b"),(0,r.kt)("span",{parentName:"span",className:"mord mathnormal"},"a"),(0,r.kt)("span",{parentName:"span",className:"mord mathnormal"},"t"),(0,r.kt)("span",{parentName:"span",className:"mord mathnormal"},"c"),(0,r.kt)("span",{parentName:"span",className:"mord mathnormal"},"h"),(0,r.kt)("span",{parentName:"span",className:"mpunct"},","),(0,r.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.16666666666666666em"}}),(0,r.kt)("span",{parentName:"span",className:"mord mathnormal"},"s"),(0,r.kt)("span",{parentName:"span",className:"mord mathnormal"},"e"),(0,r.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.03588em"}},"q"),(0,r.kt)("span",{parentName:"span",className:"mord mathnormal"},"u"),(0,r.kt)("span",{parentName:"span",className:"mord mathnormal"},"e"),(0,r.kt)("span",{parentName:"span",className:"mord mathnormal"},"n"),(0,r.kt)("span",{parentName:"span",className:"mord mathnormal"},"c"),(0,r.kt)("span",{parentName:"span",className:"mord"},(0,r.kt)("span",{parentName:"span",className:"mord mathnormal"},"e"),(0,r.kt)("span",{parentName:"span",className:"msupsub"},(0,r.kt)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,r.kt)("span",{parentName:"span",className:"vlist-r"},(0,r.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.33610799999999996em"}},(0,r.kt)("span",{parentName:"span",style:{top:"-2.5500000000000003em",marginLeft:"0em",marginRight:"0.05em"}},(0,r.kt)("span",{parentName:"span",className:"pstrut",style:{height:"2.7em"}}),(0,r.kt)("span",{parentName:"span",className:"sizing reset-size6 size3 mtight"},(0,r.kt)("span",{parentName:"span",className:"mord mathnormal mtight",style:{marginRight:"0.01968em"}},"l")))),(0,r.kt)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,r.kt)("span",{parentName:"span",className:"vlist-r"},(0,r.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.15em"}},(0,r.kt)("span",{parentName:"span"})))))),(0,r.kt)("span",{parentName:"span",className:"mord mathnormal"},"e"),(0,r.kt)("span",{parentName:"span",className:"mord mathnormal"},"n"),(0,r.kt)("span",{parentName:"span",className:"mord"},"/"),(0,r.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.03148em"}},"k"),(0,r.kt)("span",{parentName:"span",className:"mpunct"},","),(0,r.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.16666666666666666em"}}),(0,r.kt)("span",{parentName:"span",className:"mord mathnormal"},"h"),(0,r.kt)("span",{parentName:"span",className:"mord mathnormal"},"i"),(0,r.kt)("span",{parentName:"span",className:"mord mathnormal"},"d"),(0,r.kt)("span",{parentName:"span",className:"mord mathnormal"},"d"),(0,r.kt)("span",{parentName:"span",className:"mord mathnormal"},"e"),(0,r.kt)("span",{parentName:"span",className:"mord"},(0,r.kt)("span",{parentName:"span",className:"mord mathnormal"},"n"),(0,r.kt)("span",{parentName:"span",className:"msupsub"},(0,r.kt)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,r.kt)("span",{parentName:"span",className:"vlist-r"},(0,r.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.151392em"}},(0,r.kt)("span",{parentName:"span",style:{top:"-2.5500000000000003em",marginLeft:"0em",marginRight:"0.05em"}},(0,r.kt)("span",{parentName:"span",className:"pstrut",style:{height:"2.7em"}}),(0,r.kt)("span",{parentName:"span",className:"sizing reset-size6 size3 mtight"},(0,r.kt)("span",{parentName:"span",className:"mord mathnormal mtight"},"s")))),(0,r.kt)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,r.kt)("span",{parentName:"span",className:"vlist-r"},(0,r.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.15em"}},(0,r.kt)("span",{parentName:"span"})))))),(0,r.kt)("span",{parentName:"span",className:"mord mathnormal"},"t"),(0,r.kt)("span",{parentName:"span",className:"mord mathnormal"},"a"),(0,r.kt)("span",{parentName:"span",className:"mord mathnormal"},"t"),(0,r.kt)("span",{parentName:"span",className:"mord mathnormal"},"e"),(0,r.kt)("span",{parentName:"span",className:"mord mathnormal"},"s"),(0,r.kt)("span",{parentName:"span",className:"mclose"},")"))))),". Therefore, an additional All-Gather operation is required to obtain the complete input for gradient computation. However, it is possible to overlap the gradient computation with the All-Gather communication operation in our implementation, which would not introduce additional communication overhead (corresponding to the ",(0,r.kt)("inlineCode",{parentName:"p"},"enable_sequence_overlap")," parameter in ",(0,r.kt)("inlineCode",{parentName:"p"},"Shardformer"),")."))))}h.isMDXComponent=!0}}]);