"use strict";(self.webpackChunkagile_docs=self.webpackChunkagile_docs||[]).push([[3545],{3905:function(e,t,n){n.d(t,{Zo:function(){return p},kt:function(){return d}});var a=n(7294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},o=Object.keys(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var l=a.createContext({}),c=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},p=function(e){var t=c(e.components);return a.createElement(l.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},h=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,o=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),h=c(n),d=r,m=h["".concat(l,".").concat(d)]||h[d]||u[d]||o;return n?a.createElement(m,i(i({ref:t},p),{},{components:n})):a.createElement(m,i({ref:t},p))}));function d(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=n.length,i=new Array(o);i[0]=h;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:r,i[1]=s;for(var c=2;c<o;c++)i[c]=n[c];return a.createElement.apply(null,i)}return a.createElement.apply(null,n)}h.displayName="MDXCreateElement"},1766:function(e,t,n){n.r(t),n.d(t,{frontMatter:function(){return s},contentTitle:function(){return l},metadata:function(){return c},toc:function(){return p},default:function(){return h}});var a=n(3117),r=n(102),o=(n(7294),n(3905)),i=["components"],s={},l="Build an online OPT service using Colossal-AI in 5 minutes",c={unversionedId:"advanced_tutorials/opt_service",id:"version-v0.2.2/advanced_tutorials/opt_service",title:"Build an online OPT service using Colossal-AI in 5 minutes",description:"Introduction",source:"@site/i18n/en/docusaurus-plugin-content-docs/version-v0.2.2/advanced_tutorials/opt_service.md",sourceDirName:"advanced_tutorials",slug:"/advanced_tutorials/opt_service",permalink:"/docs/advanced_tutorials/opt_service",tags:[],version:"v0.2.2",frontMatter:{}},p=[{value:"Introduction",id:"introduction",children:[],level:2},{value:"Colossal-AI Inference Overview",id:"colossal-ai-inference-overview",children:[],level:2},{value:"Basic Usage:",id:"basic-usage",children:[],level:2},{value:"Advance Features Usage:",id:"advance-features-usage",children:[],level:2}],u={toc:p};function h(e){var t=e.components,n=(0,r.Z)(e,i);return(0,o.kt)("wrapper",(0,a.Z)({},u,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"build-an-online-opt-service-using-colossal-ai-in-5-minutes"},"Build an online OPT service using Colossal-AI in 5 minutes"),(0,o.kt)("h2",{id:"introduction"},"Introduction"),(0,o.kt)("p",null,"This tutorial shows how to build your own service with OPT with the help of ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/hpcaitech/ColossalAI"},"Colossal-AI"),"."),(0,o.kt)("h2",{id:"colossal-ai-inference-overview"},"Colossal-AI Inference Overview"),(0,o.kt)("p",null,"Colossal-AI provides an inference subsystem ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/hpcaitech/EnergonAI"},"Energon-AI"),", a serving system built upon Colossal-AI, which has the following characteristics:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Parallelism for Large-scale Models:")," With the help of tensor parallel operations, pipeline parallel strategies from Colossal-AI, Colossal-AI inference enables efficient parallel inference for large-scale models."),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Pre-built large models:")," There are pre-built implementations for popular models, such as OPT. It supports a caching technique for the generation task and checkpoints loading."),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Engine encapsulation\uff1a")," There has an abstraction layer called an engine. It encapsulates the single instance multiple devices (SIMD) execution with the remote procedure call, making it act as the single instance single device (SISD) execution."),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"An online service system:")," Based on FastAPI, users can launch a web service of a distributed inference quickly. The online service makes special optimizations for the generation task. It adopts both left padding and bucket batching techniques to improve efficiency.")),(0,o.kt)("h2",{id:"basic-usage"},"Basic Usage:"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},"Download OPT model")),(0,o.kt)("p",null,"To launch the distributed inference service quickly, you can download the OPT-125M from ",(0,o.kt)("a",{parentName:"p",href:"https://huggingface.co/patrickvonplaten/opt_metaseq_125m/blob/main/model/restored.pt"},"here"),". You can get details for loading other sizes of models ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/hpcaitech/EnergonAI/tree/main/examples/opt/script"},"here"),"."),(0,o.kt)("ol",{start:2},(0,o.kt)("li",{parentName:"ol"},"Prepare a prebuilt service image")),(0,o.kt)("p",null,"Pull a docker image from dockerhub installed with Colossal-AI inference."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"docker pull hpcaitech/energon-ai:latest\n")),(0,o.kt)("ol",{start:3},(0,o.kt)("li",{parentName:"ol"},"Launch an HTTP service")),(0,o.kt)("p",null,"To launch a service, we need to provide python scripts to describe the model type and related configurations, and settings for the HTTP service.\nWe have provided a set of ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/hpcaitech/EnergonAI/tree/main/examples%5D"},"examples"),". We will use the ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/hpcaitech/EnergonAI/tree/main/examples/opt"},"OPT example")," in this tutorial.\nThe entrance of the service is a bash script server.sh.\nThe config of the service is at opt_config.py, which defines the model type, the checkpoint file path, the parallel strategy, and http settings. You can adapt it for your own case.\nFor example, set the model class as opt_125M and set the correct checkpoint path as follows."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"model_class = opt_125M\ncheckpoint = 'your_file_path'\n")),(0,o.kt)("p",null,"Set the tensor parallelism degree the same as your gpu number."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"tp_init_size = #gpu\n")),(0,o.kt)("p",null,"Now, we can launch a service using docker. You can map the path of the checkpoint and directory containing configs to local disk path ",(0,o.kt)("inlineCode",{parentName:"p"},"/model_checkpoint")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"/config"),"."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},'export CHECKPOINT_DIR="your_opt_checkpoint_path"\n# the ${CONFIG_DIR} must contain a server.sh file as the entry of service\nexport CONFIG_DIR="config_file_path"\n\ndocker run --gpus all  --rm -it -p 8020:8020 -v ${CHECKPOINT_DIR}:/model_checkpoint -v ${CONFIG_DIR}:/config --ipc=host energonai:lastest\n')),(0,o.kt)("p",null,"Then open ",(0,o.kt)("inlineCode",{parentName:"p"},"https://[IP-ADDRESS]:8020/docs#")," in your browser to try out!"),(0,o.kt)("h2",{id:"advance-features-usage"},"Advance Features Usage:"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},"Batching Optimization")),(0,o.kt)("p",null,"To use our advanced batching technique to collect multiple queries in batches to serve, you can set the executor_max_batch_size as the max batch size. Note, that only the decoder task with the same top_k, top_p and temperature can be batched together."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"executor_max_batch_size = 16\n")),(0,o.kt)("p",null,"All queries are submitted to a FIFO queue. All consecutive queries whose number of decoding steps is less than or equal to that of the head of the queue can be batched together. Left padding is applied to ensure correctness. executor_max_batch_size should not be too large. This ensures batching won't increase latency. For opt-30b, ",(0,o.kt)("inlineCode",{parentName:"p"},"executor_max_batch_size=16")," may be a good choice, while for opt-175b, ",(0,o.kt)("inlineCode",{parentName:"p"},"executor_max_batch_size=4")," may be better."),(0,o.kt)("ol",{start:2},(0,o.kt)("li",{parentName:"ol"},"Cache Optimization.")),(0,o.kt)("p",null,"You can cache several recently served query results for each independent serving process. Set the cache_size and cache_list_size in config.py. The cache size is the number of queries cached. The cache_list_size is the number of results stored for each query. And a random cached result will be returned. When the cache is full, LRU is applied to evict cached queries. cache_size=0means no cache is applied."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"cache_size = 50\ncache_list_size = 2\n")))}h.isMDXComponent=!0}}]);