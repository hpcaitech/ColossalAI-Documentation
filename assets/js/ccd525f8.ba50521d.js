"use strict";(self.webpackChunkdemo=self.webpackChunkdemo||[]).push([[8344],{3905:(e,n,t)=>{t.d(n,{Zo:()=>m,kt:()=>h});var a=t(7294);function i(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function r(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,a)}return t}function o(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?r(Object(t),!0).forEach((function(n){i(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):r(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function s(e,n){if(null==e)return{};var t,a,i=function(e,n){if(null==e)return{};var t,a,i={},r=Object.keys(e);for(a=0;a<r.length;a++)t=r[a],n.indexOf(t)>=0||(i[t]=e[t]);return i}(e,n);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)t=r[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(i[t]=e[t])}return i}var l=a.createContext({}),p=function(e){var n=a.useContext(l),t=n;return e&&(t="function"==typeof e?e(n):o(o({},n),e)),t},m=function(e){var n=p(e.components);return a.createElement(l.Provider,{value:n},e.children)},d="mdxType",c={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},u=a.forwardRef((function(e,n){var t=e.components,i=e.mdxType,r=e.originalType,l=e.parentName,m=s(e,["components","mdxType","originalType","parentName"]),d=p(t),u=i,h=d["".concat(l,".").concat(u)]||d[u]||c[u]||r;return t?a.createElement(h,o(o({ref:n},m),{},{components:t})):a.createElement(h,o({ref:n},m))}));function h(e,n){var t=arguments,i=n&&n.mdxType;if("string"==typeof e||i){var r=t.length,o=new Array(r);o[0]=u;var s={};for(var l in n)hasOwnProperty.call(n,l)&&(s[l]=n[l]);s.originalType=e,s[d]="string"==typeof e?e:i,o[1]=s;for(var p=2;p<r;p++)o[p]=t[p];return a.createElement.apply(null,o)}return a.createElement.apply(null,t)}u.displayName="MDXCreateElement"},3376:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>c,frontMatter:()=>r,metadata:()=>s,toc:()=>p});var a=t(7462),i=(t(7294),t(3905));const r={},o="Zero Redundancy Optimizer with chunk-based memory management",s={unversionedId:"features/zero_with_chunk",id:"version-v0.2.4/features/zero_with_chunk",title:"Zero Redundancy Optimizer with chunk-based memory management",description:"Author: Hongxiu Liu, Jiarui Fang, Zijian Ye",source:"@site/i18n/en/docusaurus-plugin-content-docs/version-v0.2.4/features/zero_with_chunk.md",sourceDirName:"features",slug:"/features/zero_with_chunk",permalink:"/docs/features/zero_with_chunk",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/versioned_docs/version-v0.2.4/features/zero_with_chunk.md",tags:[],version:"v0.2.4",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Gradient Handler",permalink:"/docs/features/gradient_handler"},next:{title:"1D Tensor Parallelism",permalink:"/docs/features/1D_tensor_parallel"}},l={},p=[{value:"Introduction",id:"introduction",level:2},{value:"Usage",id:"usage",level:2},{value:"GeminiDDP",id:"geminiddp",level:3},{value:"Train GPT",id:"train-gpt",level:3}],m={toc:p},d="wrapper";function c(e){let{components:n,...t}=e;return(0,i.kt)(d,(0,a.Z)({},m,t,{components:n,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"zero-redundancy-optimizer-with-chunk-based-memory-management"},"Zero Redundancy Optimizer with chunk-based memory management"),(0,i.kt)("p",null,"Author: ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/ver217"},"Hongxiu Liu"),", ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/feifeibear"},"Jiarui Fang"),", ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/ZijianYY"},"Zijian Ye"),"\n",(0,i.kt)("strong",{parentName:"p"},"Prerequisite:")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"/docs/basics/define_your_config"},"Define Your Configuration"))),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Example Code")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/gpt"},"Train GPT with Colossal-AI"))),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Related Paper")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/1910.02054"},"ZeRO: Memory Optimizations Toward Training Trillion Parameter Models")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2101.06840"},"ZeRO-Offload: Democratizing Billion-Scale Model Training")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2104.07857"},"ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2108.05818"},"PatrickStar: Parallel Training of Pre-trained Models via Chunk-based Memory Management"))),(0,i.kt)("h2",{id:"introduction"},"Introduction"),(0,i.kt)("p",null,"The Zero Redundancy Optimizer (ZeRO) removes the memory redundancies across data-parallel processes by partitioning three\nmodel states (optimizer states, gradients, and parameters) instead of replicating them.\nBy doing so, memory efficiency is boosted drastically compared to classic data parallelism, while the computational granularity\nand communication efficiency is retained."),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("strong",{parentName:"li"},"Shard Optimizer States"),": The optimizer states (e.g., for ",(0,i.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/1412.6980"},"Adam optimizer"),", 32-bit weights,\nand the first and second momentum estimates) are partitioned across the processes, so that each process updates only its partition.")),(0,i.kt)("ol",{start:2},(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("strong",{parentName:"p"},"Shard Gradient"),": After reduction inside data parallel process group, gradient tensors are also partitioned such that each process only stores the gradients corresponding to its partition of the optimizer states. Note, Colossal converts gradient into fp32 format to participate in parameter updating.")),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("strong",{parentName:"p"},"Shard Parameter"),": The 16-bit model parameters are partitioned across the processes of a data parallel group.")),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("strong",{parentName:"p"},(0,i.kt)("a",{parentName:"strong",href:"/docs/advanced_tutorials/meet_gemini"},"Gemini")),": Dynamic heterogeneous memory space manager for paramters, gradients and optimizer states."))),(0,i.kt)("p",null,"Besides, this article will introduce the Zero Redundancy Optimizer with chunk-based memory management."),(0,i.kt)("p",null,"When using ZeRO, we distributed the model by sharding the parameters. The advantage of this method is that the memory of each node is load balanced. But this approach has two significiant disadvantages. First, during communication, a temporary memory buffer needs to be allocated and released afterwards, leading to the memory fragmentation problem. Secondly, using tensor as the granularity for communication will cause the network bandwidth underutilized. Generally, the longer the transmitted message length, the higher the bandwidth utilization."),(0,i.kt)("p",null,"Using the Chunk mechanism introduced in ColossalAI v0.1.8, we can improve the efficiency of ZeRO. We store a continuous set of parameters in initialization order into a Chunk (a chunk is a continuous memory space), and each Chunk has the same size. Organizing memory in chunks can lead to efficient use of network bandwidth between PCI-e and GPU-GPU, reduce the number of communications, and avoid potential memory fragmentation."),(0,i.kt)("p",null,"Before v0.1.8, ZeRO had a high communication cost for parameter communications. If a parameter was used multiple times in several consecutive operators, there will be repeated communications operations, and the efficiency was highly damaged. This situation is very common when using the Gradient Checkpoint technique, and the parameter will recompute the forward propagation during backward propagation."),(0,i.kt)("p",null,"Taking GPT as an example, its Checkpoint will be applied to each GPT Block, and each GPT Block contains a Self-Attention layer and an MLP layer. During the backward pass, the forward of the Self-Attention layer and the MLP layer will be computed in turn, and then the backward of the MLP layer and the Self-Attention layer will be computed in turn."),(0,i.kt)("p",null,"In addition, due to the communication and memory movement of small Tensors, the bandwidth of NVLINK and PCI-E cannot be fully utilized, and each communication and memory movement has the overhead of kernel launch. After using Chunk, multiple small Tensor communication and memory movement can be changed into one large Tensor communication and memory movement, which not only improves bandwidth utilization but also reduces the overhead of kernel launch."),(0,i.kt)("p",null,"We also provide a lightweight chunk search mechanism to help users automatically find the chunk size with the smallest memory fragmentation."),(0,i.kt)("h2",{id:"usage"},"Usage"),(0,i.kt)("h3",{id:"geminiddp"},"GeminiDDP"),(0,i.kt)("p",null,"We will use ",(0,i.kt)("inlineCode",{parentName:"p"},"GeminiDDP")," to use ZeRO with chunk-based memory management. This is our new torch.Module wrapper which uses ZeRO-DP and Gemini. ZeRO is for parallelism and Gemini is for memory management."),(0,i.kt)("p",null,"Also Make sure that your model is initialized under the context of ColoInitContext."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"with ColoInitContext(device='cpu', default_dist_spec=default_dist_spec, default_pg=default_pg):\n  model = gpt2_medium(checkpoint=True)\n")),(0,i.kt)("p",null,"Define the model parameters as follows:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"chunk_manager = init_chunk_manager(model=module,\n                                           init_device=device,\n                                           hidden_dim=hidden_dim,\n                                           search_range_mb=search_range_mb,\n                                           min_chunk_size_mb=min_chunk_size_mb)\ngemini_manager = GeminiManager(placement_policy, chunk_manager)\n")),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"hidden_dim")," is the hidden dimension of DNN. Users can provide this argument to speed up searching. If users do not know this argument before training, it is ok. We will use a default value 1024. ",(0,i.kt)("inlineCode",{parentName:"p"},"min_chunk_size_mb")," is the the minimum chunk size in MegaByte. If the aggregate size of parameters is still samller than the minimum chunk size, all parameters will be compacted into one small chunk."),(0,i.kt)("p",null,"Initialization of the optimizer."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"optimizer = GeminiAdamOptimizer(model, lr=1e-3, initial_scale=2**5)\n")),(0,i.kt)("p",null,"Training"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"optimizer.zero_grad()\noutputs = model(input_ids, attn_mask)\nloss = criterion(outputs, input_ids)\noptimizer.backward(loss)\noptimizer.step()\n")),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"\u26a0\ufe0f Note: Please do not use ",(0,i.kt)("inlineCode",{parentName:"p"},"loss.backward()"),", the standard way of writing is ",(0,i.kt)("inlineCode",{parentName:"p"},"optimizer.backward(loss)"),".")),(0,i.kt)("h3",{id:"train-gpt"},"Train GPT"),(0,i.kt)("p",null,"In this example, we use ",(0,i.kt)("inlineCode",{parentName:"p"},"Hugging Face Transformers"),". You have to install ",(0,i.kt)("inlineCode",{parentName:"p"},"transformers")," before running this example. We will take ",(0,i.kt)("inlineCode",{parentName:"p"},"GPT2 Medium")," as an example here."),(0,i.kt)("p",null,"For simplicity, we just use randomly generated data here."),(0,i.kt)("p",null,"First we only need to import ",(0,i.kt)("inlineCode",{parentName:"p"},"GPT2LMHeadModel")," from ",(0,i.kt)("inlineCode",{parentName:"p"},"Huggingface transformers")," to define our model, which does not require users to define or modify the model, so that users can use it more conveniently."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"class GPTLMModel(nn.Module):\n\n    def __init__(self,\n                 hidden_size=768,\n                 num_layers=12,\n                 num_attention_heads=12,\n                 max_seq_len=1024,\n                 vocab_size=50257,\n                 checkpoint=False):\n        super().__init__()\n        self.checkpoint = checkpoint\n        self.model = GPT2LMHeadModel(\n            GPT2Config(n_embd=hidden_size,\n                       n_layer=num_layers,\n                       n_head=num_attention_heads,\n                       n_positions=max_seq_len,\n                       n_ctx=max_seq_len,\n                       vocab_size=vocab_size))\n        if checkpoint:\n            self.model.gradient_checkpointing_enable()\n\n    def forward(self, input_ids, attention_mask):\n        return self.model(input_ids=input_ids, attention_mask=attention_mask, use_cache=not self.checkpoint)[0]\n\ndef gpt2_medium(checkpoint=False):\n    return GPTLMModel(hidden_size=1024, num_layers=24, num_attention_heads=16, checkpoint=checkpoint)\n")),(0,i.kt)("p",null,"Define our loss function:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"class GPTLMLoss(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.loss_fn = nn.CrossEntropyLoss()\n\n    def forward(self, logits, labels):\n        shift_logits = logits[..., :-1, :].contiguous()\n        shift_labels = labels[..., 1:].contiguous()\n        return self.loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n")),(0,i.kt)("p",null,"Define tensor parallel and parameter sharding strategies for tensor parallelism:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"def tensor_parallelize(model: torch.nn.Module, pg: ProcessGroup):\n    for mn, module in model.named_modules():\n        for pn, param in module.named_parameters(recurse=False):\n            if hasattr(param, 'visited'):\n                continue\n            param.set_dist_spec(ReplicaSpec())\n            if 'mlp.c_fc' in mn:\n                if 'weight' in pn or 'bias' in pn:\n                    split_param_col_tp1d(param, pg)\n                    param.compute_spec.set_output_replicate(False)\n                else:\n                    param.set_dist_spec(ReplicaSpec())\n            elif 'mlp.c_proj' in mn:\n                if 'weight' in pn:\n                    split_param_row_tp1d(param, pg)\n                else:\n                    param.set_dist_spec(ReplicaSpec())\n            elif 'wte' in mn or 'wpe' in mn:\n                split_param_col_tp1d(param, pg)\n            elif 'c_attn' in mn or 'c_proj' in mn:\n                split_param_col_tp1d(param, pg)\n            else:\n                param.set_dist_spec(ReplicaSpec())\n\n            param.visited = True\ndef split_param_single_dim_tp1d(dim: int, param: ColoParameter, pg: ProcessGroup):\n    spec = (ShardSpec([dim], [pg.tp_world_size()]), ComputeSpec(ComputePattern.TP1D))\n    param.set_tensor_spec(*spec)\n\n\ndef split_param_row_tp1d(param: ColoParameter, pg: ProcessGroup):\n    split_param_single_dim_tp1d(0, param, pg)\n\n\ndef split_param_col_tp1d(param: ColoParameter, pg: ProcessGroup):\n    split_param_single_dim_tp1d(-1, param, pg)\n")),(0,i.kt)("p",null,"Define a model which uses Gemini + ZeRO DDP:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'def gemini_zero_dpp(model: torch.nn.Module, pg: ProcessGroup, placememt_policy: str = "auto"):\n    cai_version = colossalai.__version__\n    if version.parse(cai_version) > version.parse("0.1.10"):\n        from colossalai.nn.parallel import GeminiDDP\n        model = GeminiDDP(model,\n                          device=get_current_device(),\n                          placement_policy=placememt_policy,\n                          pin_memory=True,\n                          search_range_mb=32)\n    elif version.parse(cai_version) <= version.parse("0.1.10") and version.parse(cai_version) >= version.parse("0.1.9"):\n        from colossalai.gemini import ChunkManager, GeminiManager\n        chunk_size = ChunkManager.search_chunk_size(model, 64 * 1024**2, 32)\n        gemini_manager = GeminiManager(placememt_policy, chunk_manager)\n        chunk_manager = ChunkManager(chunk_size,\n                                     pg,\n                                     enable_distributed_storage=True,\n                                     init_device=GeminiManager.get_default_device(placememt_policy))\n        model = ZeroDDP(model, gemini_manager)\n    else:\n        raise NotImplemented(f"CAI version {cai_version} is not supported")\n    return model\n')),(0,i.kt)("p",null,"As we pre-train GPT in this example, we just use a simple language model loss."),(0,i.kt)("p",null,"Write a function to get random inputs:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"def get_data(batch_size, seq_len, vocab_size):\n    input_ids = torch.randint(0, vocab_size, (batch_size, seq_len), device=torch.cuda.current_device())\n    attention_mask = torch.ones_like(input_ids)\n    return input_ids, attention_mask\n")),(0,i.kt)("p",null,"Finally, we can define our training loop:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"def main():\n    args = parse_args()\n    BATCH_SIZE = 8\n    SEQ_LEN = 1024\n    VOCAB_SIZE = 50257\n    NUM_STEPS = 10\n    colossalai.launch_from_torch(config={})\n\n    # build criterion\n    criterion = GPTLMLoss()\n\n    torch.manual_seed(123)\n    default_pg = ProcessGroup(tp_degree=args.tp_degree)\n    default_dist_spec = ShardSpec([-1], [args.tp_degree]) if args.shardinit else None\n    # build GPT model\n    with ColoInitContext(device='cpu', default_dist_spec=default_dist_spec, default_pg=default_pg):\n      model = gpt2_medium(checkpoint=True)\n    pg = default_pg\n    # Tensor Parallelism (TP)\n    tensor_parallelize(model, pg)\n    # Gemini + ZeRO DP, Note it must be used after TP\n    model = gemini_zero_dpp(model, pg, args.placement)\n    # build optimizer\n    optimizer = GeminiAdamOptimizer(model, lr=1e-3, initial_scale=2**5)\n    numel = sum([p.numel() for p in model.parameters()])\n    get_tflops_func = partial(get_tflops, numel, BATCH_SIZE, SEQ_LEN)\n    torch.cuda.synchronize()\n    model.train()\n    for n in range(NUM_STEPS):\n        # we just use randomly generated data here\n        input_ids, attn_mask = get_data(BATCH_SIZE, SEQ_LEN, VOCAB_SIZE)\n        optimizer.zero_grad()\n        outputs = model(input_ids, attn_mask)\n        loss = criterion(outputs, input_ids)\n        optimizer.backward(loss)\n        optimizer.step()\n\n    torch.cuda.synchronize()\n")),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"\u26a0\ufe0f Note: If you want to use the Gemini module, please do not use the ",(0,i.kt)("a",{parentName:"p",href:"/docs/features/gradient_accumulation"},"Gradient Accumulation")," we mentioned before\u3002\nThe complete example can be found on ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/gpt"},"Train GPT with Colossal-AI"),".")))}c.isMDXComponent=!0}}]);