"use strict";(self.webpackChunkdemo=self.webpackChunkdemo||[]).push([[8185],{3905:(e,t,n)=>{n.d(t,{Zo:()=>d,kt:()=>h});var r=n(7294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function l(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function o(e,t){if(null==e)return{};var n,r,a=function(e,t){if(null==e)return{};var n,r,a={},i=Object.keys(e);for(r=0;r<i.length;r++)n=i[r],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)n=i[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var s=r.createContext({}),p=function(e){var t=r.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):l(l({},t),e)),n},d=function(e){var t=p(e.components);return r.createElement(s.Provider,{value:t},e.children)},u="mdxType",c={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},m=r.forwardRef((function(e,t){var n=e.components,a=e.mdxType,i=e.originalType,s=e.parentName,d=o(e,["components","mdxType","originalType","parentName"]),u=p(n),m=a,h=u["".concat(s,".").concat(m)]||u[m]||c[m]||i;return n?r.createElement(h,l(l({ref:t},d),{},{components:n})):r.createElement(h,l({ref:t},d))}));function h(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var i=n.length,l=new Array(i);l[0]=m;var o={};for(var s in t)hasOwnProperty.call(t,s)&&(o[s]=t[s]);o.originalType=e,o[u]="string"==typeof e?e:a,l[1]=o;for(var p=2;p<i;p++)l[p]=n[p];return r.createElement.apply(null,l)}return r.createElement.apply(null,n)}m.displayName="MDXCreateElement"},8362:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>l,default:()=>c,frontMatter:()=>i,metadata:()=>o,toc:()=>p});var r=n(7462),a=(n(7294),n(3905));const i={},l="Pipeline Parallel",o={unversionedId:"features/pipeline_parallel",id:"features/pipeline_parallel",title:"Pipeline Parallel",description:"Author: Guangyang Lu, Hongxin Liu, Yongbin Li, Mingyan Jiang",source:"@site/i18n/en/docusaurus-plugin-content-docs/current/features/pipeline_parallel.md",sourceDirName:"features",slug:"/features/pipeline_parallel",permalink:"/docs/features/pipeline_parallel",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/features/pipeline_parallel.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"3D Tensor Parallelism",permalink:"/docs/features/3D_tensor_parallel"},next:{title:"NVMe offload",permalink:"/docs/features/nvme_offload"}},s={},p=[{value:"Quick introduction",id:"quick-introduction",level:2},{value:"Table Of Content",id:"table-of-content",level:2},{value:"Introduction of 1F1B pipeline",id:"introduction-of-1f1b-pipeline",level:2},{value:"Non-interleaved Schedule",id:"non-interleaved-schedule",level:3},{value:"Interleaved Schedule",id:"interleaved-schedule",level:3},{value:"Colossal-AI&#39;s Implementation",id:"colossal-ais-implementation",level:2},{value:"Fine-tune Bert with pipeline",id:"fine-tune-bert-with-pipeline",level:2}],d={toc:p},u="wrapper";function c(e){let{components:t,...n}=e;return(0,a.kt)(u,(0,r.Z)({},d,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"pipeline-parallel"},"Pipeline Parallel"),(0,a.kt)("p",null,"Author: Guangyang Lu, Hongxin Liu, Yongbin Li, Mingyan Jiang"),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"Prerequisite")),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"/docs/concepts/paradigms_of_parallelism"},"Paradigms of Parallelism")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"/docs/basics/booster_api"},"Use Booster to Training")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"/docs/features/shardformer"},"Shardformer")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"/docs/basics/booster_plugins"},"Plugin of Booster"))),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"Example Code")),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"https://github.com/hpcaitech/ColossalAI/blob/main/examples/language/bert/finetune.py"},"Fine-tune Bert with pipeline"))),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"Related Paper")),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2110.14883"},"Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2104.04473"},"Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/1811.06965"},"GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism"))),(0,a.kt)("h2",{id:"quick-introduction"},"Quick introduction"),(0,a.kt)("p",null,"In this tutorial, you will learn how to use pipeline parallel. In Colossal-AI, we use 1F1B pipeline, introduced by Nvidia. In this case, ViT and Imagenet are too large to use. Therefore, here we use bert model and glue dataset as example."),(0,a.kt)("h2",{id:"table-of-content"},"Table Of Content"),(0,a.kt)("p",null,"In this tutorial we will cover:"),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},"Introduction of 1F1B pipeline."),(0,a.kt)("li",{parentName:"ol"},"Usage of non-interleaved and interleaved schedule."),(0,a.kt)("li",{parentName:"ol"},"Finetune Bert with pipeline.")),(0,a.kt)("h2",{id:"introduction-of-1f1b-pipeline"},"Introduction of 1F1B pipeline"),(0,a.kt)("p",null,"First of all, we will introduce you GPipe for your better understanding."),(0,a.kt)("figure",{style:{textAlign:"center"}},(0,a.kt)("img",{src:"https://s2.loli.net/2022/01/28/OAucPF6mWYynUtV.png"}),(0,a.kt)("figcaption",null,"Figure1: GPipe. This figure is from ",(0,a.kt)("a",{href:"https://arxiv.org/pdf/2104.04473.pdf"},"Megatron-LM")," paper.")),(0,a.kt)("p",null,"As you can see, for GPipe, only when the forward passes of all microbatches in a batch finish, the backward passes would be executed."),(0,a.kt)("p",null,"In general, 1F1B(one forward pass followed by one backward pass) is more efficient than GPipe(in memory or both memory and time). There are two schedules of 1F1B pipeline, the non-interleaved and the interleaved. The figures are shown below."),(0,a.kt)("figure",{style:{textAlign:"center"}},(0,a.kt)("img",{src:"https://s2.loli.net/2022/01/28/iJrVkp2HLcahjsT.png"}),(0,a.kt)("figcaption",null,"Figure2: This figure is from ",(0,a.kt)("a",{href:"https://arxiv.org/pdf/2104.04473.pdf"},"Megatron-LM")," paper. The top part shows the default non-interleaved schedule. And the bottom part shows the interleaved schedule.")),(0,a.kt)("h3",{id:"non-interleaved-schedule"},"Non-interleaved Schedule"),(0,a.kt)("p",null,"The non-interleaved schedule can be divided into three stages. The first stage is the warm-up stage, where workers perform differing numbers of forward passes. At the following stage, workers perform one forward pass followed by one backward pass. Workers will finish backward passes at the last stage."),(0,a.kt)("p",null,"This mode is more memory-efficient than GPipe. However, it would take the same time to finish a turn of passes as GPipe."),(0,a.kt)("h3",{id:"interleaved-schedule"},"Interleaved Schedule"),(0,a.kt)("p",null,"This schedule requires ",(0,a.kt)("strong",{parentName:"p"},"the number of microbatches to be an integer multiple of the stage of pipeline"),"."),(0,a.kt)("p",null,"In this schedule, each device can perform computation for multiple subsets of layers(called a model chunk) instead of a single contiguous set of layers. i.e. Before device 1 had layer 1-4; device 2 had layer 5-8; and so on. But now device 1 has layer 1,2,9,10; device 2 has layer 3,4,11,12; and so on. With this scheme, each device in the pipeline is assigned multiple pipeline stages and each pipeline stage has less computation."),(0,a.kt)("p",null,"This mode is both memory-efficient and time-efficient."),(0,a.kt)("h2",{id:"colossal-ais-implementation"},"Colossal-AI's Implementation"),(0,a.kt)("p",null,"In Colossal-AI, pipeline parallelism relies on the ",(0,a.kt)("inlineCode",{parentName:"p"},"scheduler")," and ",(0,a.kt)("a",{parentName:"p",href:"/docs/features/shardformer"},(0,a.kt)("inlineCode",{parentName:"a"},"Shardformer")),". We provide both non-interleaved (",(0,a.kt)("inlineCode",{parentName:"p"},"OneForwardOneBackwardSchedule"),") and interleaved (",(0,a.kt)("inlineCode",{parentName:"p"},"InterleavedSchedule"),") schedules. While ",(0,a.kt)("inlineCode",{parentName:"p"},"Shardformer")," implements layer splitting for models and replaces the ",(0,a.kt)("inlineCode",{parentName:"p"},"forward")," function of the model to make it compatible with the scheduler."),(0,a.kt)("p",null,"In Colossal-AI, the ",(0,a.kt)("inlineCode",{parentName:"p"},"HybridParallelPlugin")," encapsulates pipeline execution strategies. It manages pipeline parallel communication groups and a scheduler. When boosting the model with this plugin, the model's layers are split by calling the ",(0,a.kt)("inlineCode",{parentName:"p"},"shardformer.optimize")," function, and then ",(0,a.kt)("inlineCode",{parentName:"p"},"execute_pipeline")," is called to execute the model in segments using ",(0,a.kt)("inlineCode",{parentName:"p"},"OneForwardOneBackwardSchedule")," which is default scheduler used in ",(0,a.kt)("inlineCode",{parentName:"p"},"HybridParallelPlugin"),", and ",(0,a.kt)("inlineCode",{parentName:"p"},"InterleavedSchedule")," will be integrated later."),(0,a.kt)("p",null,"You can customize your parallel strategy by setting parameters for the ",(0,a.kt)("inlineCode",{parentName:"p"},"HybridParallelPlugin"),"."),(0,a.kt)("p",null,"For more usage details, please refer to the ",(0,a.kt)("a",{parentName:"p",href:"/docs/basics/booster_plugins"},"documentation")," for ",(0,a.kt)("inlineCode",{parentName:"p"},"HybridParallelPlugin"),"."),(0,a.kt)("h2",{id:"fine-tune-bert-with-pipeline"},"Fine-tune Bert with pipeline"),(0,a.kt)("p",null,"First, we define the necessary training components, including model, dataloader, optimizer, lr_scheduler, criterion:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'import argparse\nfrom typing import Callable, List, Union\n\nimport torch\nimport torch.nn as nn\nfrom data import GLUEDataBuilder\nfrom torch.optim import Adam, Optimizer\nfrom torch.optim.lr_scheduler import _LRScheduler as LRScheduler\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom transformers import (\n    AlbertForSequenceClassification,\n    AutoConfig,\n    BertForSequenceClassification,\n    get_linear_schedule_with_warmup,\n)\n\nimport colossalai\nfrom colossalai.booster import Booster\nfrom colossalai.booster.plugin import HybridParallelPlugin\nfrom colossalai.cluster import DistCoordinator\nfrom colossalai.nn.optimizer import HybridAdam\n\n# Define some config\nNUM_EPOCHS = 3\nBATCH_SIZE = 32\nLEARNING_RATE = 2.4e-5\nWEIGHT_DECAY = 0.01\nWARMUP_FRACTION = 0.1\n\ncoordinator = DistCoordinator()\n\ndef move_to_cuda(batch):\n    return {k: v.cuda() for k, v in batch.items()}\n\n\n# Define \'criterion\' function with two inputs, which will be passed to \'execute_pipeline\'.\ndef _criterion(outputs, inputs):\n    return outputs.loss\n\n# Define optimizer\nlr = LEARNING_RATE\nno_decay = ["bias", "LayerNorm.weight"]\noptimizer_grouped_parameters = [\n    {\n        "params": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n        "weight_decay": WEIGHT_DECAY,\n    },\n    {\n        "params": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n        "weight_decay": 0.0,\n    },\n]\n\noptimizer = HybridAdam(optimizer_grouped_parameters, lr=lr, eps=1e-8)\n\n\n# Define lr_scheduler\ntotal_steps = len(train_dataloader) * NUM_EPOCHS\nnum_warmup_steps = int(WARMUP_FRACTION * total_steps)\nlr_scheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=num_warmup_steps,\n    num_training_steps=total_steps,\n)\n\n\n# Define Bert model\nmodel = BertForSequenceClassification.from_pretrained("bert-base-uncased", config=cfg).cuda()\n\n# Define a dataloader\ndata_builder = GLUEDataBuilder(model_name,\n                                plugin,\n                                args.task,\n                                train_batch_size=BATCH_SIZE,\n                                eval_batch_size=BATCH_SIZE)\ntrain_dataloader = data_builder.train_dataloader()\n')),(0,a.kt)("p",null,"Define a booster with the ",(0,a.kt)("inlineCode",{parentName:"p"},"HybridParallelPlugin"),"."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"plugin = HybridParallelPlugin(tp_size=1,\n                                pp_size=2,\n                                num_microbatches=None,\n                                microbatch_size=1,\n                                enable_all_optimization=True,\n                                zero_stage=1,\n                                precision='fp16',\n                                initial_scale=1)\nbooster = Booster(plugin=plugin)\n")),(0,a.kt)("p",null,"Boost these train components with the booster created."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"model, optimizer, _criterion, _, lr_scheduler = booster.boost(model,\n                                                                optimizer,\n                                                                criterion=_criterion,\n                                                                lr_scheduler=lr_scheduler)\n")),(0,a.kt)("p",null,"Train the model at last."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"# Define a train function\ndef train_epoch(epoch: int, model: nn.Module, optimizer: Optimizer, _criterion: Callable, lr_scheduler: LRScheduler,\n                train_dataloader: DataLoader, booster: Booster, coordinator: DistCoordinator):\n\n    is_pp_last_stage = booster.plugin.stage_manager.is_last_stage()\n    total_step = len(train_dataloader)\n\n    model.train()\n    optimizer.zero_grad()\n    # convert train_dataloader to a iterator\n    train_dataloader_iter = iter(train_dataloader)\n    with tqdm(range(total_step),\n              desc=f'Epoch [{epoch + 1}/{NUM_EPOCHS}]',\n              disable=not (is_pp_last_stage)) as pbar:\n        # Forward pass\n        for _ in pbar:\n            outputs = booster.execute_pipeline(train_dataloader_iter,\n                                                model,\n                                                _criterion,\n                                                optimizer,\n                                                return_loss=True)\n            # Backward and optimize\n            if is_pp_last_stage:\n                loss = outputs['loss']\n                pbar.set_postfix({'loss': loss.item()})\n\n            optimizer.step()\n            optimizer.zero_grad()\n            lr_scheduler.step()\n\n# Train model\nfor epoch in range(NUM_EPOCHS):\n    train_epoch(epoch, model, optimizer, _criterion, lr_scheduler, train_dataloader, booster, coordinator)\n")),(0,a.kt)("p",null,"We use ",(0,a.kt)("inlineCode",{parentName:"p"},"2")," pipeline stages and the micro batches is 1. (these parameters can be configured to an appropriate value)"))}c.isMDXComponent=!0}}]);