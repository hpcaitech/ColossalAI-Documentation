"use strict";(self.webpackChunkdemo=self.webpackChunkdemo||[]).push([[618],{3905:(e,t,n)=>{n.d(t,{Zo:()=>d,kt:()=>g});var a=n(7294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},i=Object.keys(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var s=a.createContext({}),p=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},d=function(e){var t=p(e.components);return a.createElement(s.Provider,{value:t},e.children)},u="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},c=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,i=e.originalType,s=e.parentName,d=l(e,["components","mdxType","originalType","parentName"]),u=p(n),c=r,g=u["".concat(s,".").concat(c)]||u[c]||m[c]||i;return n?a.createElement(g,o(o({ref:t},d),{},{components:n})):a.createElement(g,o({ref:t},d))}));function g(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=n.length,o=new Array(i);o[0]=c;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l[u]="string"==typeof e?e:r,o[1]=l;for(var p=2;p<i;p++)o[p]=n[p];return a.createElement.apply(null,o)}return a.createElement.apply(null,n)}c.displayName="MDXCreateElement"},9285:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>o,default:()=>m,frontMatter:()=>i,metadata:()=>l,toc:()=>p});var a=n(7462),r=(n(7294),n(3905));const i={},o="Fine-tune GPT-2 Using Hybrid Parallelism",l={unversionedId:"advanced_tutorials/train_gpt_using_hybrid_parallelism",id:"advanced_tutorials/train_gpt_using_hybrid_parallelism",title:"Fine-tune GPT-2 Using Hybrid Parallelism",description:"Author: Hongxin Liu, Yongbin Li, Mingyan Jiang",source:"@site/i18n/en/docusaurus-plugin-content-docs/current/advanced_tutorials/train_gpt_using_hybrid_parallelism.md",sourceDirName:"advanced_tutorials",slug:"/advanced_tutorials/train_gpt_using_hybrid_parallelism",permalink:"/docs/advanced_tutorials/train_gpt_using_hybrid_parallelism",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/advanced_tutorials/train_gpt_using_hybrid_parallelism.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Step By Step: Accelerate ViT Training With Colossal-AI (From Data Parallel to Hybrid Parallel)",permalink:"/docs/advanced_tutorials/train_vit_with_hybrid_parallelism"},next:{title:"Meet Gemini:The Heterogeneous Memory Manager of Colossal-AI",permalink:"/docs/advanced_tutorials/meet_gemini"}},s={},p=[{value:"Introduction",id:"introduction",level:2},{value:"Table of content",id:"table-of-content",level:2},{value:"Import libraries",id:"import-libraries",level:2},{value:"Define Plugin",id:"define-plugin",level:2},{value:"Define GPT-2&#39;s Training Components",id:"define-gpt-2s-training-components",level:2},{value:"Boost the GPT-2 Model",id:"boost-the-gpt-2-model",level:2},{value:"Training GPT-2 using hybrid parallelism",id:"training-gpt-2-using-hybrid-parallelism",level:2}],d={toc:p},u="wrapper";function m(e){let{components:t,...n}=e;return(0,r.kt)(u,(0,a.Z)({},d,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"fine-tune-gpt-2-using-hybrid-parallelism"},"Fine-tune GPT-2 Using Hybrid Parallelism"),(0,r.kt)("p",null,"Author: Hongxin Liu, Yongbin Li, Mingyan Jiang"),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Prerequisite:")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"/docs/basics/booster_plugins"},"parallelism plugin")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"/docs/basics/booster_api"},"booster API"))),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Example Code")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://github.com/hpcaitech/ColossalAI/blob/main/examples/language/gpt/hybridparallelism/finetune.py"},"ColossalAI-Examples GPT"))),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Related Paper")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2110.14883"},"Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2104.04473"},"Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM"))),(0,r.kt)("h2",{id:"introduction"},"Introduction"),(0,r.kt)("p",null,"In the previous tutorial, we introduce how to train ViT with pipeline. In this tutorial, you will learn a more complex scenario -- fine-tune GPT-2 with hybrid parallelism. In this case, GPT-2 is so large that CPU memory cannot fit it as well. Therefore, you must split the model."),(0,r.kt)("h2",{id:"table-of-content"},"Table of content"),(0,r.kt)("p",null,"In this tutorial we will cover:"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"Initialize the hybrid parallelism plugin."),(0,r.kt)("li",{parentName:"ol"},"Defining the Training Components of the GPT-2 Model"),(0,r.kt)("li",{parentName:"ol"},"Boost the GPT-2 Model with ",(0,r.kt)("a",{parentName:"li",href:"/docs/basics/booster_plugins"},(0,r.kt)("inlineCode",{parentName:"a"},"HybridParallelPlugin"))),(0,r.kt)("li",{parentName:"ol"},"Training GPT-2 using hybrid parallelism")),(0,r.kt)("h2",{id:"import-libraries"},"Import libraries"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"from typing import Callable, List, Union\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\nfrom torch.optim import Optimizer\nfrom torch.optim.lr_scheduler import _LRScheduler as LRScheduler\nfrom tqdm import tqdm\nfrom transformers import AutoConfig, GPT2ForSequenceClassification, get_linear_schedule_with_warmup\nfrom transformers import AutoTokenizer\n\nimport colossalai\nfrom colossalai.booster import Booster\nfrom colossalai.booster.plugin import GeminiPlugin, HybridParallelPlugin, LowLevelZeroPlugin, TorchDDPPlugin\nfrom colossalai.cluster import DistCoordinator\nfrom colossalai.nn.optimizer import HybridAdam\n")),(0,r.kt)("h2",{id:"define-plugin"},"Define Plugin"),(0,r.kt)("p",null,"Create a ",(0,r.kt)("inlineCode",{parentName:"p"},"HybridParallelPlugin")," object and specify the desired parallelism strategies to be used. In this example, both pipeline parallelism and ZeRO-1 are used simultaneously."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'plugin = HybridParallelPlugin(\n    tp_size=1,\n    pp_size=2,\n    num_microbatches=None,\n    microbatch_size=1,\n    enable_all_optimization=True,\n    zero_stage=1,\n    precision="fp16",\n    initial_scale=1,\n)\n')),(0,r.kt)("h2",{id:"define-gpt-2s-training-components"},"Define GPT-2's Training Components"),(0,r.kt)("p",null,"Before using hybrid parallelism, you need to define the components used for training."),(0,r.kt)("p",null,"Define hyperparameters"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"NUM_EPOCHS = 3\nBATCH_SIZE = 32\nLEARNING_RATE = 2.4e-5\nWEIGHT_DECAY = 0.01\nWARMUP_FRACTION = 0.1\n")),(0,r.kt)("p",null,"we create a distributed environment."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"# Launch ColossalAI\ncolossalai.launch_from_torch( seed=42)\ncoordinator = DistCoordinator()\n")),(0,r.kt)("p",null,"prepare the dataset. You can use ",(0,r.kt)("inlineCode",{parentName:"p"},"plugin.prepare_dataloader")," to generate a dataloader or customize your own dataloader."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'def tokenize_batch(batch, tokenizer: Optional[AutoTokenizer] = None, max_length: int = 2048):\n    texts = [sample["sentence1"] + sample["sentence2"] for sample in batch]\n    data = tokenizer(texts, return_tensors="pt", padding="max_length", truncation=True, max_length=max_length)\n    data = {k: v.cuda() for k, v in data.items()}\n    data["labels"] = data["input_ids"].clone()\n    return data\n\ntokenizer = AutoTokenizer.from_pretrained("gpt2")\ndataset = datasets.load_dataset("glue", "mrpc")\ntrain_dataloader = plugin.prepare_dataloader(\n    dataset["train"],\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    drop_last=True,\n    collate_fn=partial(tokenize_batch, tokenizer=tokenizer, max_length=512),\n)\n')),(0,r.kt)("p",null,"Prepare gpt-2 model"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'cfg = AutoConfig.from_pretrained("gpt2", num_labels=2)\nmodel = GPT2ForSequenceClassification.from_pretrained("gpt2", config=cfg).cuda()\n\n')),(0,r.kt)("p",null,"prepare optimizer"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'lr = LEARNING_RATE * coordinator.world_size\nno_decay = ["bias", "LayerNorm.weight"]\noptimizer_grouped_parameters = [\n    {\n        "params": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n        "weight_decay": WEIGHT_DECAY,\n    },\n    {\n        "params": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n        "weight_decay": 0.0,\n    },\n]\noptimizer = HybridAdam(optimizer_grouped_parameters, lr=lr, eps=1e-8)\n')),(0,r.kt)("p",null,"Prepare the lr_scheduler and criterion, and it's important to note that when hybrid parallelism with pipeline parallelism is used, a criterion function should also be defined. This function should take the input and output of the model's forward pass as parameters and return the loss."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"# lr scheduler\ntotal_steps = len(train_dataloader) * NUM_EPOCHS\nnum_warmup_steps = int(WARMUP_FRACTION * total_steps)\nlr_scheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=num_warmup_steps,\n    num_training_steps=total_steps,\n)\n\ndef _criterion(outputs, inputs):\n    return outputs.loss\n")),(0,r.kt)("h2",{id:"boost-the-gpt-2-model"},"Boost the GPT-2 Model"),(0,r.kt)("p",null,"Define a booster with ",(0,r.kt)("inlineCode",{parentName:"p"},"HybridParallelPlugin"),". Based on the configured plugin parameters, the booster will inject one or more parallel strategies into the model. In this example, pipeline parallelism, zero1, and mixed-precision training optimizations are utilized."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"booster = Booster(plugin=plugin)\n")),(0,r.kt)("p",null,"Boost these components with the defined booster."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"model, optimizer, _criterion, _, lr_scheduler = booster.boost(\n    model, optimizer, criterion=_criterion, lr_scheduler=lr_scheduler\n)\n")),(0,r.kt)("h2",{id:"training-gpt-2-using-hybrid-parallelism"},"Training GPT-2 using hybrid parallelism"),(0,r.kt)("p",null,"In the previous tutorial, We've explained how to inject various parallelism features into the model and its training components using the Booster and ",(0,r.kt)("inlineCode",{parentName:"p"},"HybridParallelPlugin"),". Now we can start model training.\nDefine a training function. When pipeline parallelism is used, you need to call ",(0,r.kt)("inlineCode",{parentName:"p"},"booster.execute_pipeline")," to schedule the stages of model training."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'def train_epoch(\n    epoch: int,\n    model: nn.Module,\n    optimizer: Optimizer,\n    _criterion: Callable,\n    lr_scheduler: LRScheduler,\n    train_dataloader: DataLoader,\n    booster: Booster,\n    coordinator: DistCoordinator,\n):\n    use_pipeline = isinstance(booster.plugin, HybridParallelPlugin) and booster.plugin.pp_size > 1\n    is_pp_last_stage = use_pipeline and booster.plugin.stage_manager.is_last_stage()\n    print_flag = (not use_pipeline and coordinator.is_master()) or (use_pipeline and is_pp_last_stage)\n    total_step = len(train_dataloader)\n\n    model.train()\n    optimizer.zero_grad()\n    train_dataloader_iter = iter(train_dataloader)\n    with tqdm(\n        range(total_step),\n        desc=f"Epoch [{epoch + 1}/{NUM_EPOCHS}]",\n        disable=not print_flag,\n    ) as pbar:\n        # Forward pass\n        for _ in pbar:\n            if use_pipeline:\n                outputs = booster.execute_pipeline(\n                    train_dataloader_iter, model, _criterion, optimizer, return_loss=True\n                )\n                # Backward and optimize\n                if is_pp_last_stage:\n                    loss = outputs["loss"]\n                    pbar.set_postfix({"loss": loss.item()})\n            else:\n                data = next(train_dataloader_iter)\n                data = move_to_cuda(data)\n                outputs = model(**data)\n                loss = _criterion(outputs, None)\n                # Backward\n                booster.backward(loss, optimizer)\n                pbar.set_postfix({"loss": loss.item()})\n\n            optimizer.step()\n            optimizer.zero_grad()\n            lr_scheduler.step()\n\n')),(0,r.kt)("p",null,"Training the gpt-2 model"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"for epoch in range(NUM_EPOCHS):\n    train_epoch(epoch, model, optimizer, _criterion, lr_scheduler, train_dataloader, booster, coordinator)\n")))}m.isMDXComponent=!0}}]);