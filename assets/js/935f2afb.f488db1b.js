"use strict";(self.webpackChunkdemo=self.webpackChunkdemo||[]).push([[53],{1109:e=>{e.exports=JSON.parse('{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"category","label":"Get started","collapsed":true,"items":[{"type":"link","label":"Setup","href":"/docs/get_started/installation","docId":"get_started/installation"},{"type":"link","label":"Quick Demo","href":"/docs/get_started/run_demo","docId":"get_started/run_demo"},{"type":"link","label":"Reading Roadmap","href":"/docs/get_started/reading_roadmap","docId":"get_started/reading_roadmap"}],"collapsible":true},{"type":"category","label":"Concepts","collapsed":true,"items":[{"type":"link","label":"Distributed Training","href":"/docs/concepts/distributed_training","docId":"concepts/distributed_training"},{"type":"link","label":"Paradigms of Parallelism","href":"/docs/concepts/paradigms_of_parallelism","docId":"concepts/paradigms_of_parallelism"},{"type":"link","label":"Colossal-AI Overview","href":"/docs/concepts/colossalai_overview","docId":"concepts/colossalai_overview"}],"collapsible":true},{"type":"category","label":"Basics","collapsed":true,"items":[{"type":"link","label":"Command Line Tool","href":"/docs/basics/command_line_tool","docId":"basics/command_line_tool"},{"type":"link","label":"Launch Colossal-AI","href":"/docs/basics/launch_colossalai","docId":"basics/launch_colossalai"},{"type":"link","label":"Booster API","href":"/docs/basics/booster_api","docId":"basics/booster_api"},{"type":"link","label":"Booster Plugins","href":"/docs/basics/booster_plugins","docId":"basics/booster_plugins"},{"type":"link","label":"Booster Checkpoint","href":"/docs/basics/booster_checkpoint","docId":"basics/booster_checkpoint"}],"collapsible":true},{"type":"category","label":"Features","collapsed":true,"items":[{"type":"link","label":"Shardformer","href":"/docs/features/shardformer","docId":"features/shardformer"},{"type":"link","label":"Auto Mixed Precision Training","href":"/docs/features/mixed_precision_training_with_booster","docId":"features/mixed_precision_training_with_booster"},{"type":"link","label":"Gradient Accumulation","href":"/docs/features/gradient_accumulation_with_booster","docId":"features/gradient_accumulation_with_booster"},{"type":"link","label":"Gradient Clipping","href":"/docs/features/gradient_clipping_with_booster","docId":"features/gradient_clipping_with_booster"},{"type":"link","label":"Zero Redundancy Optimizer with chunk-based memory management","href":"/docs/features/zero_with_chunk","docId":"features/zero_with_chunk"},{"type":"category","label":"Tensor Parallel","collapsed":true,"items":[{"type":"link","label":"1D Tensor Parallelism","href":"/docs/features/1D_tensor_parallel","docId":"features/1D_tensor_parallel"},{"type":"link","label":"2D Tensor Parallelism","href":"/docs/features/2D_tensor_parallel","docId":"features/2D_tensor_parallel"},{"type":"link","label":"2.5D Tensor Parallelism","href":"/docs/features/2p5D_tensor_parallel","docId":"features/2p5D_tensor_parallel"},{"type":"link","label":"3D Tensor Parallelism","href":"/docs/features/3D_tensor_parallel","docId":"features/3D_tensor_parallel"}],"collapsible":true},{"type":"link","label":"Pipeline Parallel","href":"/docs/features/pipeline_parallel","docId":"features/pipeline_parallel"},{"type":"link","label":"NVMe offload","href":"/docs/features/nvme_offload","docId":"features/nvme_offload"},{"type":"link","label":"Lazy initialization","href":"/docs/features/lazy_init","docId":"features/lazy_init"},{"type":"link","label":"Cluster Utilities","href":"/docs/features/cluster_utils","docId":"features/cluster_utils"}],"collapsible":true},{"type":"category","label":"Advanced Tutorials","collapsed":true,"items":[{"type":"link","label":"Step By Step: Accelerate ViT Training With Colossal-AI (From Data Parallel to Hybrid Parallel)","href":"/docs/advanced_tutorials/train_vit_with_hybrid_parallelism","docId":"advanced_tutorials/train_vit_with_hybrid_parallelism"},{"type":"link","label":"Fine-tune GPT-2 Using Hybrid Parallelism","href":"/docs/advanced_tutorials/train_gpt_using_hybrid_parallelism","docId":"advanced_tutorials/train_gpt_using_hybrid_parallelism"},{"type":"link","label":"Meet Gemini:The Heterogeneous Memory Manager of Colossal-AI","href":"/docs/advanced_tutorials/meet_gemini","docId":"advanced_tutorials/meet_gemini"},{"type":"link","label":"Integrate Mixture-of-Experts Into Your Model","href":"/docs/advanced_tutorials/integrate_mixture_of_experts_into_your_model","docId":"advanced_tutorials/integrate_mixture_of_experts_into_your_model"},{"type":"link","label":"Build an online OPT service using Colossal-AI in 5 minutes","href":"/docs/advanced_tutorials/opt_service","docId":"advanced_tutorials/opt_service"}],"collapsible":true}]},"docs":{"advanced_tutorials/integrate_mixture_of_experts_into_your_model":{"id":"advanced_tutorials/integrate_mixture_of_experts_into_your_model","title":"Integrate Mixture-of-Experts Into Your Model","description":"Author: Haichen Huang","sidebar":"tutorialSidebar"},"advanced_tutorials/meet_gemini":{"id":"advanced_tutorials/meet_gemini","title":"Meet Gemini:The Heterogeneous Memory Manager of Colossal-AI","description":"Author: Jiarui Fang, Yang You","sidebar":"tutorialSidebar"},"advanced_tutorials/opt_service":{"id":"advanced_tutorials/opt_service","title":"Build an online OPT service using Colossal-AI in 5 minutes","description":"Introduction","sidebar":"tutorialSidebar"},"advanced_tutorials/train_gpt_using_hybrid_parallelism":{"id":"advanced_tutorials/train_gpt_using_hybrid_parallelism","title":"Fine-tune GPT-2 Using Hybrid Parallelism","description":"Author: Hongxin Liu, Yongbin Li, Mingyan Jiang","sidebar":"tutorialSidebar"},"advanced_tutorials/train_vit_with_hybrid_parallelism":{"id":"advanced_tutorials/train_vit_with_hybrid_parallelism","title":"Step By Step: Accelerate ViT Training With Colossal-AI (From Data Parallel to Hybrid Parallel)","description":"Author: Yuxuan Lou, Mingyan Jiang","sidebar":"tutorialSidebar"},"basics/booster_api":{"id":"basics/booster_api","title":"Booster API","description":"Author: Mingyan Jiang, Jianghai Chen, Baizhou Zhang","sidebar":"tutorialSidebar"},"basics/booster_checkpoint":{"id":"basics/booster_checkpoint","title":"Booster Checkpoint","description":"Author: Hongxin Liu","sidebar":"tutorialSidebar"},"basics/booster_plugins":{"id":"basics/booster_plugins","title":"Booster Plugins","description":"Author: Hongxin Liu, Baizhou Zhang, Pengtai Xu","sidebar":"tutorialSidebar"},"basics/command_line_tool":{"id":"basics/command_line_tool","title":"Command Line Tool","description":"Author: Shenggui Li","sidebar":"tutorialSidebar"},"basics/launch_colossalai":{"id":"basics/launch_colossalai","title":"Launch Colossal-AI","description":"Author: Chuanrui Wang, Shenggui Li, Siqi Mai","sidebar":"tutorialSidebar"},"Colossal-Auto/feature/auto_checkpoint":{"id":"Colossal-Auto/feature/auto_checkpoint","title":"auto_checkpoint","description":""},"Colossal-Auto/feature/device_mesh":{"id":"Colossal-Auto/feature/device_mesh","title":"device_mesh","description":""},"Colossal-Auto/feature/layout_converting_management":{"id":"Colossal-Auto/feature/layout_converting_management","title":"layout_converting_management","description":"When a tensor is required to have different sharding specs in upstream and downstream operators, we need to perform layout conversion processing, which can also be called redistribution. There are currently two mainstream methods, enumeration conversion, and dimension-by-dimension conversion. enumeration conversion is to enumerate all possible situations, and then find the corresponding conversion scheme in the table when conversion is required. However, it has a big problem. That is, as the dimension of the device mesh increases, the scale of this problem is so inflated that it cannot be solved by enumerating tables. Dimension-by-dimension conversion is for a sharding spec of an N-D tensor, X0X1...Xn-1, sharding spec is converted from 0 to n-1 dimension by dimension, so that no matter how many dimensions the device mesh and tensor have, with only one-time Scanning, a feasible conversion operation sequence is generated, the problem is that the conversion efficiency will be very poor."},"Colossal-Auto/feature/tracer":{"id":"Colossal-Auto/feature/tracer","title":"tracer","description":""},"Colossal-Auto/get_started/installation":{"id":"Colossal-Auto/get_started/installation","title":"Setup","description":"Announcement"},"Colossal-Auto/get_started/introduction":{"id":"Colossal-Auto/get_started/introduction","title":"Introduction","description":"In recent years, the deployment of large-scale machine learning models has become increasingly important. However, distributed training systems often require manual parallelization plans, which can be complex and require expert knowledge in system engineering and configuration. This can be a challenge for most AI developers without the necessary skills. The need for manual parallelization can make deploying large-scale machine learning models difficult and expensive."},"Colossal-Auto/get_started/run_demo":{"id":"Colossal-Auto/get_started/run_demo","title":"Quick Demo","description":"Colossal-Auto simplifies the process of deploying large-scale machine learning models for AI developers. Compared to other solutions that require manual configuration of complex parallel policies and model modification, Colossal-Auto only requires one line of code from the user, along with cluster information and model configurations, to enable distributed training. Quick demos showing how to use Colossal-Auto are given below."},"concepts/colossalai_overview":{"id":"concepts/colossalai_overview","title":"Colossal-AI Overview","description":"Author: Shenggui Li, Siqi Mai","sidebar":"tutorialSidebar"},"concepts/distributed_training":{"id":"concepts/distributed_training","title":"Distributed Training","description":"Author: Shenggui Li, Siqi Mai","sidebar":"tutorialSidebar"},"concepts/paradigms_of_parallelism":{"id":"concepts/paradigms_of_parallelism","title":"Paradigms of Parallelism","description":"Author: Shenggui Li, Siqi Mai","sidebar":"tutorialSidebar"},"features/1D_tensor_parallel":{"id":"features/1D_tensor_parallel","title":"1D Tensor Parallelism","description":"Author: Zhengda Bian, Yongbin Li","sidebar":"tutorialSidebar"},"features/2D_tensor_parallel":{"id":"features/2D_tensor_parallel","title":"2D Tensor Parallelism","description":"Author: Zhengda Bian, Yongbin Li","sidebar":"tutorialSidebar"},"features/2p5D_tensor_parallel":{"id":"features/2p5D_tensor_parallel","title":"2.5D Tensor Parallelism","description":"Author: Zhengda Bian, Yongbin Li","sidebar":"tutorialSidebar"},"features/3D_tensor_parallel":{"id":"features/3D_tensor_parallel","title":"3D Tensor Parallelism","description":"Author: Zhengda Bian, Yongbin Li","sidebar":"tutorialSidebar"},"features/cluster_utils":{"id":"features/cluster_utils","title":"Cluster Utilities","description":"Author: Hongxin Liu","sidebar":"tutorialSidebar"},"features/gradient_accumulation_with_booster":{"id":"features/gradient_accumulation_with_booster","title":"Gradient Accumulation","description":"Author: Mingyan Jiang, Baizhou Zhang","sidebar":"tutorialSidebar"},"features/gradient_clipping_with_booster":{"id":"features/gradient_clipping_with_booster","title":"Gradient Clipping","description":"Author: Mingyan Jiang","sidebar":"tutorialSidebar"},"features/lazy_init":{"id":"features/lazy_init","title":"Lazy initialization","description":"Author: Hongxiu Liu","sidebar":"tutorialSidebar"},"features/mixed_precision_training_with_booster":{"id":"features/mixed_precision_training_with_booster","title":"Auto Mixed Precision Training","description":"Author: Mingyan Jiang","sidebar":"tutorialSidebar"},"features/nvme_offload":{"id":"features/nvme_offload","title":"NVMe offload","description":"Author: Hongxin Liu","sidebar":"tutorialSidebar"},"features/pipeline_parallel":{"id":"features/pipeline_parallel","title":"Pipeline Parallel","description":"Author: Guangyang Lu, Hongxin Liu, Yongbin Li, Mingyan Jiang","sidebar":"tutorialSidebar"},"features/shardformer":{"id":"features/shardformer","title":"Shardformer","description":"Author: Baizhou Zhang, Bin Jia","sidebar":"tutorialSidebar"},"features/zero_with_chunk":{"id":"features/zero_with_chunk","title":"Zero Redundancy Optimizer with chunk-based memory management","description":"Author: Hongxiu Liu, Jiarui Fang, Zijian Ye","sidebar":"tutorialSidebar"},"get_started/installation":{"id":"get_started/installation","title":"Setup","description":"Requirements:","sidebar":"tutorialSidebar"},"get_started/reading_roadmap":{"id":"get_started/reading_roadmap","title":"Reading Roadmap","description":"Colossal-AI provides a collection of parallel training components for you. We aim to support you with your development","sidebar":"tutorialSidebar"},"get_started/run_demo":{"id":"get_started/run_demo","title":"Quick Demo","description":"Colossal-AI is an integrated large-scale deep learning system with efficient parallelization techniques. The system can","sidebar":"tutorialSidebar"}}}')}}]);