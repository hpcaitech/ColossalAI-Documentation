"use strict";(self.webpackChunkdemo=self.webpackChunkdemo||[]).push([[9793],{3905:(e,t,a)=>{a.d(t,{Zo:()=>u,kt:()=>h});var n=a(7294);function i(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function l(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function r(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?l(Object(a),!0).forEach((function(t){i(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):l(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function o(e,t){if(null==e)return{};var a,n,i=function(e,t){if(null==e)return{};var a,n,i={},l=Object.keys(e);for(n=0;n<l.length;n++)a=l[n],t.indexOf(a)>=0||(i[a]=e[a]);return i}(e,t);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);for(n=0;n<l.length;n++)a=l[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(i[a]=e[a])}return i}var s=n.createContext({}),p=function(e){var t=n.useContext(s),a=t;return e&&(a="function"==typeof e?e(t):r(r({},t),e)),a},u=function(e){var t=p(e.components);return n.createElement(s.Provider,{value:t},e.children)},d="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},c=n.forwardRef((function(e,t){var a=e.components,i=e.mdxType,l=e.originalType,s=e.parentName,u=o(e,["components","mdxType","originalType","parentName"]),d=p(a),c=i,h=d["".concat(s,".").concat(c)]||d[c]||m[c]||l;return a?n.createElement(h,r(r({ref:t},u),{},{components:a})):n.createElement(h,r({ref:t},u))}));function h(e,t){var a=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var l=a.length,r=new Array(l);r[0]=c;var o={};for(var s in t)hasOwnProperty.call(t,s)&&(o[s]=t[s]);o.originalType=e,o[d]="string"==typeof e?e:i,r[1]=o;for(var p=2;p<l;p++)r[p]=a[p];return n.createElement.apply(null,r)}return n.createElement.apply(null,a)}c.displayName="MDXCreateElement"},1404:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>s,contentTitle:()=>r,default:()=>m,frontMatter:()=>l,metadata:()=>o,toc:()=>p});var n=a(7462),i=(a(7294),a(3905));const l={},r="Step By Step: Accelerate ViT Training With Colossal-AI (From Data Parallel to Hybrid Parallel)",o={unversionedId:"advanced_tutorials/train_vit_with_hybrid_parallelism",id:"advanced_tutorials/train_vit_with_hybrid_parallelism",title:"Step By Step: Accelerate ViT Training With Colossal-AI (From Data Parallel to Hybrid Parallel)",description:"Author: Yuxuan Lou, Mingyan Jiang",source:"@site/i18n/en/docusaurus-plugin-content-docs/current/advanced_tutorials/train_vit_with_hybrid_parallelism.md",sourceDirName:"advanced_tutorials",slug:"/advanced_tutorials/train_vit_with_hybrid_parallelism",permalink:"/docs/advanced_tutorials/train_vit_with_hybrid_parallelism",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/advanced_tutorials/train_vit_with_hybrid_parallelism.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Cluster Utilities",permalink:"/docs/features/cluster_utils"},next:{title:"Fine-tune GPT-2 Using Hybrid Parallelism",permalink:"/docs/advanced_tutorials/train_gpt_using_hybrid_parallelism"}},s={},p=[{value:"Introduction",id:"introduction",level:2},{value:"Table of Contents",id:"table-of-contents",level:2},{value:"Colossal-AI Installation",id:"colossal-ai-installation",level:2},{value:"Import libraries",id:"import-libraries",level:2},{value:"Define the Vision Transformer (VIT) model.",id:"define-the-vision-transformer-vit-model",level:2},{value:"Boost the VIT Model",id:"boost-the-vit-model",level:2},{value:"Training with AMP",id:"training-with-amp",level:3},{value:"Tensor parallelism",id:"tensor-parallelism",level:3},{value:"Pipeline Parallelism",id:"pipeline-parallelism",level:3},{value:"Data Parallelism",id:"data-parallelism",level:3},{value:"Hybrid Parallelism",id:"hybrid-parallelism",level:3},{value:"Train ViT using hybrid parallelism.",id:"train-vit-using-hybrid-parallelism",level:2}],u={toc:p},d="wrapper";function m(e){let{components:t,...a}=e;return(0,i.kt)(d,(0,n.Z)({},u,a,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"step-by-step-accelerate-vit-training-with-colossal-ai-from-data-parallel-to-hybrid-parallel"},"Step By Step: Accelerate ViT Training With Colossal-AI (From Data Parallel to Hybrid Parallel)"),(0,i.kt)("p",null,"Author: Yuxuan Lou, Mingyan Jiang"),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Prerequisite:")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"/docs/basics/booster_plugins"},"parallelism plugin")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"/docs/basics/booster_api"},"booster API"))),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Example Code")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://github.com/hpcaitech/ColossalAI/blob/main/examples/images/vit/vit_train_demo.py"},"Colossal-AI Examples ViT on ",(0,i.kt)("inlineCode",{parentName:"a"},"beans")))),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Related Paper")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://arxiv.org/pdf/2010.11929.pdf"},"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"))),(0,i.kt)("h2",{id:"introduction"},"Introduction"),(0,i.kt)("p",null,"In this example for ViT model, Colossal-AI provides three different parallelism techniques which accelerate model training: data parallelism, pipeline parallelism and tensor parallelism.\nWe will show you how to train ViT on ",(0,i.kt)("inlineCode",{parentName:"p"},"beans")," dataset with these parallelism techniques. To run this example, you will need 2-4 GPUs."),(0,i.kt)("h2",{id:"table-of-contents"},"Table of Contents"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"Colossal-AI installation"),(0,i.kt)("li",{parentName:"ol"},"Define the ViT model and related training components."),(0,i.kt)("li",{parentName:"ol"},"Boost the VIT Model with ",(0,i.kt)("a",{parentName:"li",href:"/docs/basics/booster_plugins"},(0,i.kt)("inlineCode",{parentName:"a"},"HybridParallelPlugin"))),(0,i.kt)("li",{parentName:"ol"},"Train the VIT model using data parallelism, pipeline parallelism, and tensor parallelism.")),(0,i.kt)("h2",{id:"colossal-ai-installation"},"Colossal-AI Installation"),(0,i.kt)("p",null,"You can install Colossal-AI package and its dependencies with PyPI."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"pip install colossalai\n")),(0,i.kt)("h2",{id:"import-libraries"},"Import libraries"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"from typing import Any, Callable, Iterator\n\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\nimport transformers\nfrom data import BeansDataset, beans_collator\nfrom torch.optim import Optimizer\nfrom torch.optim.lr_scheduler import _LRScheduler as LRScheduler\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom transformers import ViTConfig, ViTForImageClassification, ViTImageProcessor\n\nimport colossalai\nfrom colossalai.booster import Booster\nfrom colossalai.booster.plugin import GeminiPlugin, HybridParallelPlugin, LowLevelZeroPlugin, TorchDDPPlugin\nfrom colossalai.cluster import DistCoordinator\nfrom colossalai.logging import disable_existing_loggers, get_dist_logger\nfrom colossalai.nn.lr_scheduler import CosineAnnealingWarmupLR\nfrom colossalai.nn.optimizer import HybridAdam\n")),(0,i.kt)("h2",{id:"define-the-vision-transformer-vit-model"},"Define the Vision Transformer (VIT) model."),(0,i.kt)("p",null,"Define hyperparameters."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'SEED = 42\nMODEL_PATH = "google/vit-base-patch16-224"\nLEARNING_RATE = 5e-5\nWEIGHT_DECAY = 0.0\nNUM_EPOCH = 3\nWARMUP_RATIO = 0.3\nTP_SIZE = 2\nPP_SIZE = 2\n')),(0,i.kt)("p",null,"Create a distributed environment."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"# Launch ColossalAI\ncolossalai.launch_from_torch( seed=SEED\xe5)\ncoordinator = DistCoordinator()\nworld_size = coordinator.world_size\n")),(0,i.kt)("p",null,"Before training, you can define the relevant components of the model training process as usual, such as defining the model, data loaders, optimizer, and so on. It's important to note that when using pipeline parallelism, you also need to define a criterion function. This function takes the input and output of the model forward pass as inputs and returns the loss.\nPrepare the dataset. BeansDataset is defined in ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/hpcaitech/ColossalAI/blob/main/examples/images/vit/data.py"},"data.py"),"."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'image_processor = ViTImageProcessor.from_pretrained(MODEL_PATH)\ntrain_dataset = BeansDataset(image_processor, TP_SIZE, split="train")\neval_dataset = BeansDataset(image_processor, RP_SIZE, split="validation")\nnum_labels = train_dataset.num_labels\n')),(0,i.kt)("p",null,"Define the VIT model:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"config = ViTConfig.from_pretrained(MODEL_PATH)\nconfig.num_labels = num_labels\nconfig.id2label = {str(i): c for i, c in enumerate(train_dataset.label_names)}\nconfig.label2id = {c: str(i) for i, c in enumerate(train_dataset.label_names)}\nmodel = ViTForImageClassification.from_pretrained(\n    MODEL_PATH, config=config, ignore_mismatched_sizes=True\n)\n")),(0,i.kt)("p",null,"Define the optimizer:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"optimizer = HybridAdam(model.parameters(), lr=(LEARNING_RATE * world_size), weight_decay=WEIGHT_DECAY)\n")),(0,i.kt)("p",null,"Define the learning rate scheduler:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"total_steps = len(train_dataloader) * NUM_EPOCH\nnum_warmup_steps = int(WARMUP_RATIO * total_steps)\nlr_scheduler = CosineAnnealingWarmupLR(\n        optimizer=optimizer, total_steps=(len(train_dataloader) * NUM_EPOCH), warmup_steps=num_warmup_steps\n    )\n")),(0,i.kt)("p",null,"Define the criterion function:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"def _criterion(outputs, inputs):\n    return outputs.loss\n")),(0,i.kt)("h2",{id:"boost-the-vit-model"},"Boost the VIT Model"),(0,i.kt)("p",null,"We begin using ColossalAI's hybrid parallelism strategy to enhance the model. First, let's define an object of ",(0,i.kt)("inlineCode",{parentName:"p"},"HybridParallelPlugin"),". ",(0,i.kt)("inlineCode",{parentName:"p"},"HybridParallelPlugin")," encapsulates various parallelism strategies in ColossalAI. Afterward, we use the ",(0,i.kt)("inlineCode",{parentName:"p"},"HybridParallelPlugin")," object to initialize the booster and boost the VIT model."),(0,i.kt)("h3",{id:"training-with-amp"},"Training with AMP"),(0,i.kt)("p",null,"In the HybridParallelPlugin plugin, you can determine the training precision by setting the precision parameter, which supports three types: 'fp16', 'bf16', and 'fp32'. 'fp16' and 'bf16' are half-precision types. Half-precision is used in two scenarios in the HybridParallelPlugin:"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"When using zero-data parallelism, you should set it to half-precision."),(0,i.kt)("li",{parentName:"ol"},"When specifying the use of AMP (Automatic Mixed Precision) for training.\nYou can set related parameters when using half-precision.\n",(0,i.kt)("inlineCode",{parentName:"li"},"initial_scale")," (float, optional): Initial loss scaling factor for AMP. Default value is 2",(0,i.kt)("strong",{parentName:"li"},"16.\n",(0,i.kt)("inlineCode",{parentName:"strong"},"min_scale")," (float, optional): Minimum loss scaling factor for AMP. Default value is 1.\n",(0,i.kt)("inlineCode",{parentName:"strong"},"growth_factor")," (float, optional): Multiplicative factor used to increase the loss scaling factor when using AMP. Default value is 2.\n",(0,i.kt)("inlineCode",{parentName:"strong"},"backoff_factor")," (float, optional): Multiplicative factor used to decrease the loss scaling factor when using AMP. Default value is 0.5.\n",(0,i.kt)("inlineCode",{parentName:"strong"},"growth_interval")," (integer, optional): Number of steps to increase the loss scaling factor when using AMP, in cases where there is no overflow. Default value is 1000.\n",(0,i.kt)("inlineCode",{parentName:"strong"},"hysteresis")," (integer, optional): Number of overflows required before reducing the loss scaling factor when using AMP. Default value is 2.\n",(0,i.kt)("inlineCode",{parentName:"strong"},"max_scale")," (float, optional): Maximum loss scaling factor for AMP. Default value is 2"),"32.\nPlugin example when using amp:")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'plugin = HybridParallelPlugin(\n            precision="fp16",\n            initial_scale=1,\n        )\n')),(0,i.kt)("h3",{id:"tensor-parallelism"},"Tensor parallelism"),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"HybridParallelPlugin")," achieves tensor parallelism through Shardformer. In this plugin, you can set the ",(0,i.kt)("inlineCode",{parentName:"p"},"tp_size")," to determine the size of tensor parallel groups. Additionally, there are multiple parameters that can be configured to optimize tensor parallelism features when using this plugin:\n",(0,i.kt)("inlineCode",{parentName:"p"},"enable_all_optimization")," (boolean, optional): Whether to enable all optimization methods supported by Shardformer. Currently, all optimization methods include fused normalization, flash attention, and JIT. Default is False.\n",(0,i.kt)("inlineCode",{parentName:"p"},"enable_fused_normalization")," (boolean, optional): Whether to enable fused normalization in Shardformer. Default is False.\n",(0,i.kt)("inlineCode",{parentName:"p"},"enable_flash_attention")," (boolean, optional): Whether to enable flash attention in Shardformer. Default is False.\n",(0,i.kt)("inlineCode",{parentName:"p"},"enable_jit_fused")," (boolean, optional): Whether to enable JIT (Just-In-Time) fusion in Shardformer. Default is False.\n",(0,i.kt)("inlineCode",{parentName:"p"},"enable_sequence_parallelism")," (boolean): Whether to enable sequence parallelism in Shardformer. Default is False.\n",(0,i.kt)("inlineCode",{parentName:"p"},"enable_sequence_overlap")," (boolean): Whether to enable sequence overlap in Shardformer. Default is False.\nExample of a tensor parallelism plugin:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"plugin = HybridParallelPlugin(\n            tp_size=4,\n            enable_all_optimization=True\n        )\n")),(0,i.kt)("h3",{id:"pipeline-parallelism"},"Pipeline Parallelism"),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"HybridParallelPlugin")," determines the size of pipeline parallelism groups by setting ",(0,i.kt)("inlineCode",{parentName:"p"},"pp_size"),". ",(0,i.kt)("inlineCode",{parentName:"p"},"num_microbatches")," is used to specify the number of microbatches into which the entire batch is divided during pipeline parallelism, and ",(0,i.kt)("inlineCode",{parentName:"p"},"microbatch_size")," can be set to define the size of these microbatches. The plugin will prioritize using ",(0,i.kt)("inlineCode",{parentName:"p"},"num_microbatches")," to determine the microbatch configuration.\nExample of a plugin for pipeline parallelism:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"plugin = HybridParallelPlugin(\n            pp_size=4,\n            num_microbatches=None,\n            microbatch_size=1\n        )\n")),(0,i.kt)("h3",{id:"data-parallelism"},"Data Parallelism"),(0,i.kt)("p",null,"The ",(0,i.kt)("inlineCode",{parentName:"p"},"HybridParallelPlugin"),"'s data parallelism includes both the zero-dp series and Torch DDP. When ",(0,i.kt)("inlineCode",{parentName:"p"},"zero_stage")," is set to 0 (the default), it means using Torch DDP. Please note that Torch DDP conflicts with pipeline parallelism and cannot be used together. When ",(0,i.kt)("inlineCode",{parentName:"p"},"zero_stage")," is set to 1, it indicates the use of the zero1 strategy. When ",(0,i.kt)("inlineCode",{parentName:"p"},"zero_stage")," is set to 2, it implies the use of the zero2 strategy. The zero2 strategy also cannot be used together with pipeline parallelism. If you want to use zero3, please use the ",(0,i.kt)("a",{parentName:"p",href:"/docs/basics/booster_plugins"},(0,i.kt)("inlineCode",{parentName:"a"},"GeminiPlugin")),".\nWhen using data parallelism with the zero series, please set the training precision to half-precision. If you haven't specified the use of zero or pipeline parallelism, and if ",(0,i.kt)("inlineCode",{parentName:"p"},"world_size//(tp_size*pp_size)")," is greater than 1, the HybridParallelPlugin will automatically enable the Torch DDP parallel strategy for you.\nHere are some related parameters for configuring Torch DDP:\n",(0,i.kt)("inlineCode",{parentName:"p"},"broadcast_buffers")," (boolean, optional): Whether to broadcast buffers at the beginning of training when using DDP. Default is True.\n",(0,i.kt)("inlineCode",{parentName:"p"},"ddp_bucket_cap_mb")," (integer, optional): Size of the bucket (in MB) when using DDP. Default is 25.\n",(0,i.kt)("inlineCode",{parentName:"p"},"find_unused_parameters")," (boolean, optional): Whether to search for unused parameters when using DDP. Default is False.\n",(0,i.kt)("inlineCode",{parentName:"p"},"check_reduction")," (boolean, optional): Whether to check the reduction operation when using DDP. Default is False.\n",(0,i.kt)("inlineCode",{parentName:"p"},"gradient_as_bucket_view")," (boolean, optional): Whether to use gradients as bucket views when using DDP. Default is False.\n",(0,i.kt)("inlineCode",{parentName:"p"},"static_graph")," (boolean, optional): Whether to use a static graph when using DDP. Default is False.\nExample of a plugin for Torch DDP."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'plugin = HybridParallelPlugin(\n            tp_size=2,\n            pp_size=1,\n            zero_stage=0,\n            precision="fp16",\n            initial_scale=1,\n        )\n')),(0,i.kt)("p",null,"If there are 4 parallel processes, the parallel group size for Torch DDP is 2.\nZeRO-related parameters:\n",(0,i.kt)("inlineCode",{parentName:"p"},"zero_bucket_size_in_m")," (integer, optional): The bucket size for gradient reduction in megabytes when using ZeRO. Default is 12.\n",(0,i.kt)("inlineCode",{parentName:"p"},"cpu_offload")," (boolean, optional): Whether to enable cpu_offload when using ZeRO. Default is False.\n",(0,i.kt)("inlineCode",{parentName:"p"},"communication_dtype")," (torch data type, optional): The data type for communication when using ZeRO. If not specified, the data type of the parameters will be used. Default is None.\n",(0,i.kt)("inlineCode",{parentName:"p"},"overlap_communication")," (boolean, optional): Whether to overlap communication and computation when using ZeRO. Default is True.\nExample of a plugin for ZERO1."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'plugin = HybridParallelPlugin(\n            tp_size=1,\n            pp_size=1,\n            zero_stage=1,\n            cpu_offload=True,\n            precision="fp16",\n            initial_scale=1,\n        )\n')),(0,i.kt)("h3",{id:"hybrid-parallelism"},"Hybrid Parallelism"),(0,i.kt)("p",null,"You can refer to the above-mentioned strategies to customize an appropriate hybrid parallelism strategy. And use this plugin to define a booster."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'plugin = HybridParallelPlugin(\n            tp_size=TP_SIZE,\n            pp_size=PP_SIZE,\n            num_microbatches=None,\n            microbatch_size=1,\n            enable_all_optimization=True,\n            precision="fp16",\n            initial_scale=1,\n        )\nbooster = Booster(plugin=plugin)\n')),(0,i.kt)("p",null,"Next, we use ",(0,i.kt)("inlineCode",{parentName:"p"},"booster.boost")," to inject the features encapsulated by the plugin into the model training components."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"model, optimizer, _criterion, train_dataloader, lr_scheduler = booster.boost(\n        model=model, optimizer=optimizer, criterion=criterion, dataloader=train_dataloader, lr_scheduler=lr_scheduler\n    )\n")),(0,i.kt)("h2",{id:"train-vit-using-hybrid-parallelism"},"Train ViT using hybrid parallelism."),(0,i.kt)("p",null,"Finally, we can use the hybrid parallelism strategy to train the model. Let's first define a training function that describes the training process. It's important to note that if the pipeline parallelism strategy is used, you should call ",(0,i.kt)("inlineCode",{parentName:"p"},"booster.execute_pipeline")," to perform the model training. This function will invoke the ",(0,i.kt)("inlineCode",{parentName:"p"},"scheduler")," to manage the model's forward and backward operations."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'def run_forward_backward(\n    model: nn.Module,\n    optimizer: Optimizer,\n    criterion: Callable[[Any, Any], torch.Tensor],\n    data_iter: Iterator,\n    booster: Booster,\n):\n    if optimizer is not None:\n        optimizer.zero_grad()\n    if isinstance(booster.plugin, HybridParallelPlugin) and booster.plugin.pp_size > 1:\n        # run pipeline forward backward when enabling pp in hybrid parallel plugin\n        output_dict = booster.execute_pipeline(\n            data_iter, model, criterion, optimizer, return_loss=True\n        )\n        loss, outputs = output_dict["loss"], output_dict["outputs"]\n    else:\n        batch = next(data_iter)\n        batch = move_to_cuda(batch, torch.cuda.current_device())\n        outputs = model(**batch)\n        loss = criterion(outputs, None)\n        if optimizer is not None:\n            booster.backward(loss, optimizer)\n\ndef train_epoch(\n    epoch: int,\n    model: nn.Module,\n    optimizer: Optimizer,\n    criterion: Callable[[Any, Any], torch.Tensor],\n    lr_scheduler: LRScheduler,\n    dataloader: DataLoader,\n    booster: Booster,\n    coordinator: DistCoordinator,\n):\n    torch.cuda.synchronize()\n\n    num_steps = len(dataloader)\n    data_iter = iter(dataloader)\n    enable_pbar = coordinator.is_master()\n    if isinstance(booster.plugin, HybridParallelPlugin) and booster.plugin.pp_size > 1:\n        # when using pp, only the last stage of master pipeline (dp_rank and tp_rank are both zero) shows pbar\n        tp_rank = dist.get_rank(booster.plugin.tp_group)\n        dp_rank = dist.get_rank(booster.plugin.dp_group)\n        enable_pbar = tp_rank == 0 and dp_rank == 0 and booster.plugin.stage_manager.is_last_stage()\n    model.train()\n\n    with tqdm(range(num_steps), desc=f"Epoch [{epoch + 1}]", disable=not enable_pbar) as pbar:\n        for _ in pbar:\n            loss, _ = run_forward_backward(model, optimizer, criterion, data_iter, booster)\n            optimizer.step()\n            lr_scheduler.step()\n\n            # Print batch loss\n            if enable_pbar:\n                pbar.set_postfix({"loss": loss.item()})\n')),(0,i.kt)("p",null,"Start training the model."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"for epoch in range(NUM_EPOCH):\n    train_epoch(epoch, model, optimizer, criterion, lr_scheduler, train_dataloader, booster, coordinator)\n")))}m.isMDXComponent=!0}}]);