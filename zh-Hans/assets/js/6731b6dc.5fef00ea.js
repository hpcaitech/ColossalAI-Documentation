"use strict";(self.webpackChunkdemo=self.webpackChunkdemo||[]).push([[6649],{6999:(e,t,o)=>{o.d(t,{Cl:()=>a,Dx:()=>p,Pc:()=>r,aE:()=>s,e_:()=>d,iz:()=>l,nT:()=>c});var n=o(7294),i=o(398);o(814);function a(e){return n.createElement("div",{className:"docstring-container"},e.children)}function r(e){return n.createElement("div",{className:"signature"},"(",e.children,")")}function l(e){return n.createElement("div",{class:"divider"},n.createElement("span",{class:"divider-text"},e.name))}function s(e){return n.createElement("div",null,n.createElement(l,{name:"Parameters"}),n.createElement(i.D,null,e.children))}function c(e){return n.createElement("div",null,n.createElement(l,{name:"Returns"}),n.createElement(i.D,null,`${e.name}: ${e.desc}`))}function p(e){return n.createElement("div",{className:"title-container"},n.createElement("div",{className:"title-module"},n.createElement("h5",null,e.type),"\xa0 ",n.createElement("h3",null,e.name)),n.createElement("div",{className:"title-source"},"<",n.createElement("a",{href:e.source,className:"title-source"},"source"),">"))}function d(e){return n.createElement("div",null,n.createElement(l,{name:"Example"}),n.createElement(i.D,null,e.code))}},2191:(e,t,o)=>{o.r(t),o.d(t,{assets:()=>c,contentTitle:()=>l,default:()=>m,frontMatter:()=>r,metadata:()=>s,toc:()=>p});var n=o(7462),i=(o(7294),o(3905)),a=o(6999);const r={},l="Booster Checkpoint",s={unversionedId:"basics/booster_checkpoint",id:"basics/booster_checkpoint",title:"Booster Checkpoint",description:"\u4f5c\u8005: Hongxin Liu",source:"@site/i18n/zh-Hans/docusaurus-plugin-content-docs/current/basics/booster_checkpoint.md",sourceDirName:"basics",slug:"/basics/booster_checkpoint",permalink:"/zh-Hans/docs/basics/booster_checkpoint",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/basics/booster_checkpoint.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Booster \u63d2\u4ef6",permalink:"/zh-Hans/docs/basics/booster_plugins"},next:{title:"Shardformer",permalink:"/zh-Hans/docs/features/shardformer"}},c={},p=[{value:"\u5f15\u8a00",id:"\u5f15\u8a00",level:2},{value:"\u6a21\u578b Checkpoint",id:"\u6a21\u578b-checkpoint",level:2},{value:"\u4f18\u5316\u5668 Checkpoint",id:"\u4f18\u5316\u5668-checkpoint",level:2},{value:"\u5b66\u4e60\u7387\u8c03\u5ea6\u5668 Checkpoint",id:"\u5b66\u4e60\u7387\u8c03\u5ea6\u5668-checkpoint",level:2},{value:"Checkpoint \u8bbe\u8ba1",id:"checkpoint-\u8bbe\u8ba1",level:2}],d={toc:p},h="wrapper";function m(e){let{components:t,...o}=e;return(0,i.kt)(h,(0,n.Z)({},d,o,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"booster-checkpoint"},"Booster Checkpoint"),(0,i.kt)("p",null,"\u4f5c\u8005: ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/ver217"},"Hongxin Liu")),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"\u524d\u7f6e\u6559\u7a0b:")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"/zh-Hans/docs/basics/booster_api"},"Booster API"))),(0,i.kt)("h2",{id:"\u5f15\u8a00"},"\u5f15\u8a00"),(0,i.kt)("p",null,"\u6211\u4eec\u5728\u4e4b\u524d\u7684\u6559\u7a0b\u4e2d\u4ecb\u7ecd\u4e86 ",(0,i.kt)("a",{parentName:"p",href:"/zh-Hans/docs/basics/booster_api"},"Booster API"),"\u3002\u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u4ecb\u7ecd\u5982\u4f55\u4f7f\u7528 booster \u4fdd\u5b58\u548c\u52a0\u8f7d checkpoint\u3002"),(0,i.kt)("h2",{id:"\u6a21\u578b-checkpoint"},"\u6a21\u578b Checkpoint"),(0,i.kt)(a.Cl,{mdxType:"DocStringContainer"},(0,i.kt)("div",null,(0,i.kt)(a.Dx,{type:"function",name:"colossalai.booster.Booster.save_model",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/booster/booster.py#L315",mdxType:"Title"}),(0,i.kt)(a.Pc,{mdxType:"Signature"},"model: typing.Union[torch.nn.modules.module.Module, colossalai.interface.model.ModelWrapper], checkpoint: str, shard: bool = False, gather_dtensor: bool = True, prefix: typing.Optional[str] = None, size_per_shard: int = 1024, use_safetensors: bool = False, use_async: bool = False"),(0,i.kt)(a.aE,{mdxType:"Parameters"},"- **model** (nn.Module or ModelWrapper) -- A model boosted by Booster.\n- **checkpoint** (str) -- Path to the checkpoint. It must be a local path.\n  It is a file path if `shard=False`. Otherwise, it is a directory path.\n- **shard** (bool, optional) -- Whether to save checkpoint a sharded way.\n  If true, the checkpoint will be a folder with the same format as Huggingface transformers checkpoint. Otherwise, it will be a single file. Defaults to False.\n- **gather_dtensor** (bool, optional) -- whether to gather the distributed tensor to the first device. Default: True.\n- **prefix** (str, optional) -- A prefix added to parameter and buffer\n  names to compose the keys in state_dict. Defaults to None.\n- **size_per_shard** (int, optional) -- Maximum size of checkpoint shard file in MB. This is useful only when `shard=True`. Defaults to 1024.\n- **use_safetensors** (bool, optional) -- whether to use safe tensors. Default: False. If set to True, the checkpoint will be saved.\n- **use_async** (bool, optional) -- whether to save the state_dict of model asynchronously. Default: False.")),(0,i.kt)("div",null,(0,i.kt)(a.iz,{name:"Description",mdxType:"Divider"}),"Save model to checkpoint.")),(0,i.kt)("p",null,"\u6a21\u578b\u5728\u4fdd\u5b58\u524d\u5fc5\u987b\u88ab ",(0,i.kt)("inlineCode",{parentName:"p"},"colossalai.booster.Booster")," \u5c01\u88c5\u3002 ",(0,i.kt)("inlineCode",{parentName:"p"},"checkpoint")," \u662f\u8981\u4fdd\u5b58\u7684 checkpoint \u7684\u8def\u5f84\u3002 \u5982\u679c ",(0,i.kt)("inlineCode",{parentName:"p"},"shard=False"),"\uff0c\u5b83\u5c31\u662f\u6587\u4ef6\u3002 \u5426\u5219, \u5b83\u5c31\u662f\u6587\u4ef6\u5939\u3002\u5982\u679c ",(0,i.kt)("inlineCode",{parentName:"p"},"shard=True"),"\uff0ccheckpoint \u5c06\u4ee5\u5206\u7247\u65b9\u5f0f\u4fdd\u5b58\uff0c\u5728 checkpoint \u592a\u5927\u800c\u65e0\u6cd5\u4fdd\u5b58\u5728\u5355\u4e2a\u6587\u4ef6\u4e2d\u65f6\u4f1a\u5f88\u5b9e\u7528\u3002\u6211\u4eec\u7684\u5206\u7247 checkpoint \u683c\u5f0f\u4e0e ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/huggingface/transformers"},"huggingface/transformers")," \u517c\u5bb9\uff0c\u6240\u4ee5\u7528\u6237\u53ef\u4ee5\u4f7f\u7528huggingface\u7684",(0,i.kt)("inlineCode",{parentName:"p"},"from_pretrained"),"\u65b9\u6cd5\u4ece\u5206\u7247checkpoint\u52a0\u8f7d\u6a21\u578b\u3002"),(0,i.kt)(a.Cl,{mdxType:"DocStringContainer"},(0,i.kt)("div",null,(0,i.kt)(a.Dx,{type:"function",name:"colossalai.booster.Booster.load_model",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/booster/booster.py#L291",mdxType:"Title"}),(0,i.kt)(a.Pc,{mdxType:"Signature"},"model: typing.Union[torch.nn.modules.module.Module, colossalai.interface.model.ModelWrapper], checkpoint: str, strict: bool = True, low_cpu_mem_mode: bool = True, num_threads: int = 1"),(0,i.kt)(a.aE,{mdxType:"Parameters"},"- **model** (nn.Module or ModelWrapper) -- A model boosted by Booster.\n- **checkpoint** (str) -- Path to the checkpoint. It must be a local path.\n  It should be a directory path if the checkpoint is sharded. Otherwise, it should be a file path.\n- **strict** (bool, optional) -- whether to strictly enforce that the keys\n  in :attr:*state_dict* match the keys returned by this module's\n  [`~torch.nn.Module.state_dict`] function. Defaults to True.\n- **low_cpu_mem_mode** (bool) -- whether to load the model in low cpu memory mode. If false, it will use RAM cache to accelerate loading. Default: True.\n- **num_threads** (int) -- number of threads to use when loading the model. Only useful when disabling low cpu mem mode. Default: 1.")),(0,i.kt)("div",null,(0,i.kt)(a.iz,{name:"Description",mdxType:"Divider"}),"Load model from checkpoint.")),(0,i.kt)("p",null,"\u6a21\u578b\u5728\u52a0\u8f7d\u524d\u5fc5\u987b\u88ab ",(0,i.kt)("inlineCode",{parentName:"p"},"colossalai.booster.Booster")," \u5c01\u88c5\u3002\u5b83\u4f1a\u81ea\u52a8\u68c0\u6d4b checkpoint \u683c\u5f0f\uff0c\u5e76\u4ee5\u76f8\u5e94\u7684\u65b9\u5f0f\u52a0\u8f7d\u3002"),(0,i.kt)("p",null,"\u5982\u679c\u60a8\u60f3\u4eceHuggingface\u52a0\u8f7d\u9884\u8bad\u7ec3\u597d\u7684\u6a21\u578b\uff0c\u4f46\u6a21\u578b\u592a\u5927\u4ee5\u81f3\u4e8e\u65e0\u6cd5\u5728\u5355\u4e2a\u8bbe\u5907\u4e0a\u901a\u8fc7\u201cfrom_pretrained\u201d\u76f4\u63a5\u52a0\u8f7d\uff0c\u63a8\u8350\u7684\u65b9\u6cd5\u662f\u5c06\u9884\u8bad\u7ec3\u7684\u6a21\u578b\u6743\u91cd\u4e0b\u8f7d\u5230\u672c\u5730\uff0c\u5e76\u5728\u5c01\u88c5\u6a21\u578b\u540e\u4f7f\u7528",(0,i.kt)("inlineCode",{parentName:"p"},"booster.load"),"\u76f4\u63a5\u4ece\u672c\u5730\u8def\u5f84\u52a0\u8f7d\u3002\u4e3a\u4e86\u907f\u514d\u5185\u5b58\u4e0d\u8db3\uff0c\u6a21\u578b\u9700\u8981\u5728",(0,i.kt)("inlineCode",{parentName:"p"},"Lazy Initialization"),"\u7684\u73af\u5883\u4e0b\u521d\u59cb\u5316\u3002\u4ee5\u4e0b\u662f\u793a\u4f8b\u4f2a\u4ee3\u7801\uff1a"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'from colossalai.lazy import LazyInitContext\nfrom huggingface_hub import snapshot_download\n...\n\n# Initialize model under lazy init context\ninit_ctx = LazyInitContext(default_device=get_current_device)\nwith init_ctx:\n     model = LlamaForCausalLM(config)\n\n...\n\n# Wrap the model through Booster.boost\nmodel, optimizer, _, _, _ = booster.boost(model, optimizer)\n\n# download huggingface pretrained model to local directory.\nmodel_dir = snapshot_download(repo_id="lysandre/arxiv-nlp")\n\n# load model using booster.load\nbooster.load(model, model_dir)\n...\n')),(0,i.kt)("h2",{id:"\u4f18\u5316\u5668-checkpoint"},"\u4f18\u5316\u5668 Checkpoint"),(0,i.kt)(a.Cl,{mdxType:"DocStringContainer"},(0,i.kt)("div",null,(0,i.kt)(a.Dx,{type:"function",name:"colossalai.booster.Booster.save_optimizer",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/booster/booster.py#L372",mdxType:"Title"}),(0,i.kt)(a.Pc,{mdxType:"Signature"},"optimizer: Optimizer, checkpoint: str, shard: bool = False, gather_dtensor: bool = True, prefix: typing.Optional[str] = None, size_per_shard: int = 1024, use_async: bool = False"),(0,i.kt)(a.aE,{mdxType:"Parameters"},"- **optimizer** (Optimizer) -- An optimizer boosted by Booster.\n- **checkpoint** (str) -- Path to the checkpoint. It must be a local path.\n  It is a file path if `shard=False`. Otherwise, it is a directory path.\n- **shard** (bool, optional) -- Whether to save checkpoint a sharded way.\n  If true, the checkpoint will be a folder. Otherwise, it will be a single file. Defaults to False.\n- **gather_dtensor** (bool) -- whether to gather the distributed tensor to the first device. Default: True.\n- **prefix** (str, optional) -- A prefix added to parameter and buffer\n  names to compose the keys in state_dict. Defaults to None.\n- **size_per_shard** (int, optional) -- Maximum size of checkpoint shard file in MB. This is useful only when `shard=True`. Defaults to 1024.")),(0,i.kt)("div",null,(0,i.kt)(a.iz,{name:"Description",mdxType:"Divider"}),(0,i.kt)("p",null,"Save optimizer to checkpoint."))),(0,i.kt)("p",null,"\u4f18\u5316\u5668\u5728\u4fdd\u5b58\u524d\u5fc5\u987b\u88ab ",(0,i.kt)("inlineCode",{parentName:"p"},"colossalai.booster.Booster")," \u5c01\u88c5\u3002"),(0,i.kt)(a.Cl,{mdxType:"DocStringContainer"},(0,i.kt)("div",null,(0,i.kt)(a.Dx,{type:"function",name:"colossalai.booster.Booster.load_optimizer",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/booster/booster.py#L352",mdxType:"Title"}),(0,i.kt)(a.Pc,{mdxType:"Signature"},"optimizer: Optimizer, checkpoint: str, low_cpu_mem_mode: bool = True, num_threads: int = 1"),(0,i.kt)(a.aE,{mdxType:"Parameters"},"- **optimizer** (Optimizer) -- An optimizer boosted by Booster.\n- **checkpoint** (str) -- Path to the checkpoint. It must be a local path.\n  It should be a directory path if the checkpoint is sharded. Otherwise, it should be a file path.\n- **low_cpu_mem_mode** (bool) -- whether to load the model in low cpu memory mode. If false, it will use RAM cache to accelerate loading. Default: True.\n- **num_threads** (int) -- number of threads to use when loading the model. Only useful when disabling low cpu mem mode. Default: 1.")),(0,i.kt)("div",null,(0,i.kt)(a.iz,{name:"Description",mdxType:"Divider"}),"Load optimizer from checkpoint.")),(0,i.kt)("p",null,"\u4f18\u5316\u5668\u5728\u52a0\u8f7d\u524d\u5fc5\u987b\u88ab ",(0,i.kt)("inlineCode",{parentName:"p"},"colossalai.booster.Booster")," \u5c01\u88c5\u3002"),(0,i.kt)("h2",{id:"\u5b66\u4e60\u7387\u8c03\u5ea6\u5668-checkpoint"},"\u5b66\u4e60\u7387\u8c03\u5ea6\u5668 Checkpoint"),(0,i.kt)(a.Cl,{mdxType:"DocStringContainer"},(0,i.kt)("div",null,(0,i.kt)(a.Dx,{type:"function",name:"colossalai.booster.Booster.save_lr_scheduler",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/booster/booster.py#L400",mdxType:"Title"}),(0,i.kt)(a.Pc,{mdxType:"Signature"},"lr_scheduler: _LRScheduler, checkpoint: str"),(0,i.kt)(a.aE,{mdxType:"Parameters"},"- **lr_scheduler** (LRScheduler) -- A lr scheduler boosted by Booster.\n- **checkpoint** (str) -- Path to the checkpoint. It must be a local file path.")),(0,i.kt)("div",null,(0,i.kt)(a.iz,{name:"Description",mdxType:"Divider"}),"Save lr scheduler to checkpoint.")),(0,i.kt)("p",null,"\u5b66\u4e60\u7387\u8c03\u5ea6\u5668\u5728\u4fdd\u5b58\u524d\u5fc5\u987b\u88ab ",(0,i.kt)("inlineCode",{parentName:"p"},"colossalai.booster.Booster")," \u5c01\u88c5\u3002 ",(0,i.kt)("inlineCode",{parentName:"p"},"checkpoint")," \u662f checkpoint \u6587\u4ef6\u7684\u672c\u5730\u8def\u5f84."),(0,i.kt)(a.Cl,{mdxType:"DocStringContainer"},(0,i.kt)("div",null,(0,i.kt)(a.Dx,{type:"function",name:"colossalai.booster.Booster.load_lr_scheduler",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/booster/booster.py#L409",mdxType:"Title"}),(0,i.kt)(a.Pc,{mdxType:"Signature"},"lr_scheduler: _LRScheduler, checkpoint: str"),(0,i.kt)(a.aE,{mdxType:"Parameters"},"- **lr_scheduler** (LRScheduler) -- A lr scheduler boosted by Booster.\n- **checkpoint** (str) -- Path to the checkpoint. It must be a local file path.")),(0,i.kt)("div",null,(0,i.kt)(a.iz,{name:"Description",mdxType:"Divider"}),"Load lr scheduler from checkpoint.")),(0,i.kt)("p",null,"\u5b66\u4e60\u7387\u8c03\u5ea6\u5668\u5728\u52a0\u8f7d\u524d\u5fc5\u987b\u88ab ",(0,i.kt)("inlineCode",{parentName:"p"},"colossalai.booster.Booster")," \u5c01\u88c5\u3002 ",(0,i.kt)("inlineCode",{parentName:"p"},"checkpoint")," \u662f checkpoint \u6587\u4ef6\u7684\u672c\u5730\u8def\u5f84."),(0,i.kt)("h2",{id:"checkpoint-\u8bbe\u8ba1"},"Checkpoint \u8bbe\u8ba1"),(0,i.kt)("p",null,"\u6709\u5173 Checkpoint \u8bbe\u8ba1\u7684\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1\u6211\u4eec\u7684\u8ba8\u8bba ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/hpcaitech/ColossalAI/discussions/3339"},"A Unified Checkpoint System Design"),"."))}m.isMDXComponent=!0}}]);