"use strict";(self.webpackChunkdemo=self.webpackChunkdemo||[]).push([[8430],{6999:(e,t,n)=>{n.d(t,{Cl:()=>i,Dx:()=>m,Pc:()=>l,aE:()=>s,e_:()=>d,iz:()=>r,nT:()=>p});var a=n(7294),o=n(398);n(814);function i(e){return a.createElement("div",{className:"docstring-container"},e.children)}function l(e){return a.createElement("div",{className:"signature"},"(",e.children,")")}function r(e){return a.createElement("div",{class:"divider"},a.createElement("span",{class:"divider-text"},e.name))}function s(e){return a.createElement("div",null,a.createElement(r,{name:"Parameters"}),a.createElement(o.D,null,e.children))}function p(e){return a.createElement("div",null,a.createElement(r,{name:"Returns"}),a.createElement(o.D,null,`${e.name}: ${e.desc}`))}function m(e){return a.createElement("div",{className:"title-container"},a.createElement("div",{className:"title-module"},a.createElement("h5",null,e.type),"\xa0 ",a.createElement("h3",null,e.name)),a.createElement("div",{className:"title-source"},"<",a.createElement("a",{href:e.source,className:"title-source"},"source"),">"))}function d(e){return a.createElement("div",null,a.createElement(r,{name:"Example"}),a.createElement(o.D,null,e.code))}},7631:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>p,contentTitle:()=>r,default:()=>c,frontMatter:()=>l,metadata:()=>s,toc:()=>m});var a=n(7462),o=(n(7294),n(3905)),i=n(6999);const l={},r="NVMe offload",s={unversionedId:"features/nvme_offload",id:"features/nvme_offload",title:"NVMe offload",description:"\u4f5c\u8005: Hongxin Liu",source:"@site/i18n/zh-Hans/docusaurus-plugin-content-docs/current/features/nvme_offload.md",sourceDirName:"features",slug:"/features/nvme_offload",permalink:"/zh-Hans/docs/features/nvme_offload",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/features/nvme_offload.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"\u6d41\u6c34\u5e76\u884c",permalink:"/zh-Hans/docs/features/pipeline_parallel"},next:{title:"\u96c6\u7fa4\u5b9e\u7528\u7a0b\u5e8f",permalink:"/zh-Hans/docs/features/cluster_utils"}},p={},m=[{value:"\u5f15\u8a00",id:"\u5f15\u8a00",level:2},{value:"\u4f7f\u7528",id:"\u4f7f\u7528",level:2},{value:"Examples",id:"examples",level:2},{value:"API \u53c2\u8003",id:"api-\u53c2\u8003",level:2}],d={toc:m},u="wrapper";function c(e){let{components:t,...n}=e;return(0,o.kt)(u,(0,a.Z)({},d,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"nvme-offload"},"NVMe offload"),(0,o.kt)("p",null,"\u4f5c\u8005: Hongxin Liu"),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"\u524d\u7f6e\u6559\u7a0b:")),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"/zh-Hans/docs/features/zero_with_chunk"},"\u57fa\u4e8eChunk\u5185\u5b58\u7ba1\u7406\u7684\u96f6\u5197\u4f59\u4f18\u5316\u5668 (ZeRO)"))),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"\u76f8\u5173\u8bba\u6587")),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2101.06840"},"ZeRO-Offload: Democratizing Billion-Scale Model Training")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2104.07857"},"ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning"))),(0,o.kt)("h2",{id:"\u5f15\u8a00"},"\u5f15\u8a00"),(0,o.kt)("p",null,"\u5982\u679c\u6a21\u578b\u5177\u6709",(0,o.kt)("inlineCode",{parentName:"p"},"N"),"\u4e2a\u53c2\u6570\uff0c\u5728\u4f7f\u7528 Adam \u65f6\uff0c\u4f18\u5316\u5668\u72b6\u6001\u5177\u6709",(0,o.kt)("inlineCode",{parentName:"p"},"8N"),"\u4e2a\u53c2\u6570\u3002\u5bf9\u4e8e\u5341\u4ebf\u89c4\u6a21\u7684\u6a21\u578b\uff0c\u4f18\u5316\u5668\u72b6\u6001\u81f3\u5c11\u9700\u8981 32 GB \u5185\u5b58\u3002 GPU\u663e\u5b58\u9650\u5236\u4e86\u6211\u4eec\u53ef\u4ee5\u8bad\u7ec3\u7684\u6a21\u578b\u89c4\u6a21\uff0c\u8fd9\u79f0\u4e3aGPU\u663e\u5b58\u5899\u3002\u5982\u679c\u6211\u4eec\u5c06\u4f18\u5316\u5668\u72b6\u6001 offload \u5230\u78c1\u76d8\uff0c\u6211\u4eec\u53ef\u4ee5\u7a81\u7834 GPU \u5185\u5b58\u5899\u3002"),(0,o.kt)("p",null,"\u6211\u4eec\u5b9e\u73b0\u4e86\u4e00\u4e2a\u7528\u6237\u53cb\u597d\u4e14\u9ad8\u6548\u7684\u5f02\u6b65 Tensor I/O \u5e93\uff1a",(0,o.kt)("a",{parentName:"p",href:"https://github.com/hpcaitech/TensorNVMe"},"TensorNVMe"),"\u3002\u6709\u4e86\u8fd9\u4e2a\u5e93\uff0c\u6211\u4eec\u53ef\u4ee5\u7b80\u5355\u5730\u5b9e\u73b0 NVMe offload\u3002"),(0,o.kt)("blockquote",null,(0,o.kt)("p",{parentName:"blockquote"},"\u8be5\u5e93\u4e0e\u5404\u79cd\u78c1\u76d8\uff08HDD\u3001SATA SSD \u548c NVMe SSD\uff09\u517c\u5bb9\u3002\u7531\u4e8e HDD \u6216 SATA SSD \u7684 I/O \u5e26\u5bbd\u8f83\u4f4e\uff0c\u5efa\u8bae\u4ec5\u5728 NVMe \u78c1\u76d8\u4e0a\u4f7f\u7528\u6b64\u5e93\u3002")),(0,o.kt)("p",null,"\u5728\u4f18\u5316\u53c2\u6570\u65f6\uff0c\u6211\u4eec\u53ef\u4ee5\u5c06\u4f18\u5316\u8fc7\u7a0b\u5206\u4e3a\u4e09\u4e2a\u9636\u6bb5\uff1a\u8bfb\u53d6\u3001\u8ba1\u7b97\u548c offload\u3002\u6211\u4eec\u4ee5\u6d41\u6c34\u7ebf\u7684\u65b9\u5f0f\u6267\u884c\u4f18\u5316\u8fc7\u7a0b\uff0c\u8fd9\u53ef\u4ee5\u91cd\u53e0\u8ba1\u7b97\u548c I/O\u3002"),(0,o.kt)("figure",{style:{textAlign:"center"}},(0,o.kt)("img",{src:"https://s2.loli.net/2022/08/16/CvRnowrsNyB4hza.jpg"}),(0,o.kt)("figcaption",null,"\u4f18\u5316\u8fc7\u7a0b")),(0,o.kt)("h2",{id:"\u4f7f\u7528"},"\u4f7f\u7528"),(0,o.kt)("p",null,"\u9996\u5148\uff0c\u8bf7\u786e\u4fdd\u60a8\u5b89\u88c5\u4e86 ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/hpcaitech/TensorNVMe"},"TensorNVMe"),":"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"pip install packaging\npip install tensornvme\n")),(0,o.kt)("p",null,"\u6211\u4eec\u4e3a Adam (",(0,o.kt)("a",{parentName:"p",href:"https://colossalai.readthedocs.io/en/latest/colossalai/colossalai.nn.optimizer.cpu_adam.html"},"CPUAdam")," \u548c ",(0,o.kt)("a",{parentName:"p",href:"https://colossalai.readthedocs.io/en/latest/colossalai/colossalai.nn.optimizer.hybrid_adam.html"},"HybridAdam"),") \u5b9e\u73b0\u4e86\u4f18\u5316\u5668\u72b6\u6001\u7684 NVMe offload\u3002"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from colossalai.nn.optimizer import CPUAdam, HybridAdam\n\noptimizer = HybridAdam(model.parameters(), lr=1e-3, nvme_offload_fraction=1.0, nvme_offload_dir='./')\n")),(0,o.kt)("p",null,(0,o.kt)("inlineCode",{parentName:"p"},"nvme_offload_fraction")," \u662f\u8981 offload \u5230 NVMe \u7684\u4f18\u5316\u5668\u72b6\u6001\u7684\u6bd4\u4f8b\u3002 ",(0,o.kt)("inlineCode",{parentName:"p"},"nvme_offload_dir")," \u662f\u4fdd\u5b58 NVMe offload \u6587\u4ef6\u7684\u76ee\u5f55\u3002\u5982\u679c ",(0,o.kt)("inlineCode",{parentName:"p"},"nvme_offload_dir")," \u4e3a ",(0,o.kt)("inlineCode",{parentName:"p"},"None"),"\uff0c\u5c06\u4f7f\u7528\u968f\u673a\u4e34\u65f6\u76ee\u5f55\u3002"),(0,o.kt)("p",null,"\u5b83\u4e0e ColossalAI \u4e2d\u7684\u6240\u6709\u5e76\u884c\u65b9\u6cd5\u517c\u5bb9\u3002"),(0,o.kt)("blockquote",null,(0,o.kt)("p",{parentName:"blockquote"},"\u26a0 \u5b83\u53ea\u4f1a\u5378\u8f7d\u5728 CPU \u4e0a\u7684\u4f18\u5316\u5668\u72b6\u6001\u3002\u8fd9\u610f\u5473\u7740\u5b83\u53ea\u4f1a\u5f71\u54cd CPU \u8bad\u7ec3\u6216\u8005\u4f7f\u7528\u5378\u8f7d\u7684 Zero/Gemini\u3002")),(0,o.kt)("h2",{id:"examples"},"Examples"),(0,o.kt)("p",null,"\u9996\u5148\u8ba9\u6211\u4eec\u4ece\u4e24\u4e2a\u7b80\u5355\u7684\u4f8b\u5b50\u5f00\u59cb -- \u7528\u4e0d\u540c\u7684\u65b9\u6cd5\u8bad\u7ec3 GPT\u3002\u8fd9\u4e9b\u4f8b\u5b50\u4f9d\u8d56",(0,o.kt)("inlineCode",{parentName:"p"},"transformers"),"\u3002"),(0,o.kt)("p",null,"\u6211\u4eec\u9996\u5148\u5e94\u8be5\u5b89\u88c5\u4f9d\u8d56\uff1a"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"pip install psutil transformers\n")),(0,o.kt)("p",null,"\u9996\u5148\uff0c\u6211\u4eec\u5bfc\u5165\u5fc5\u8981\u7684\u5305\u548c\u6a21\u5757\uff1a"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"import os\nimport time\nfrom typing import Dict, Optional\nimport psutil\nimport torch\nimport torch.nn as nn\nfrom transformers.models.gpt2.configuration_gpt2 import GPT2Config\nfrom transformers.models.gpt2.modeling_gpt2 import GPT2LMHeadModel\nimport colossalai\nfrom colossalai.nn.optimizer import HybridAdam\nfrom colossalai.utils.model.colo_init_context import ColoInitContext\nfrom colossalai.booster import Booster\nfrom colossalai.booster.plugin import GeminiPlugin\n")),(0,o.kt)("p",null,"\u7136\u540e\u6211\u4eec\u5b9a\u4e49\u4e00\u4e2a\u635f\u5931\u51fd\u6570\uff1a"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"class GPTLMLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.loss_fn = nn.CrossEntropyLoss()\n    def forward(self, logits, labels):\n        shift_logits = logits[..., :-1, :].contiguous()\n        shift_labels = labels[..., 1:].contiguous()\n        # Flatten the tokens\n        return self.loss_fn(shift_logits.view(-1, shift_logits.size(-1)),\n                            shift_labels.view(-1))\n")),(0,o.kt)("p",null,"\u6211\u4eec\u5b9a\u4e49\u4e00\u4e9b\u5de5\u5177\u51fd\u6570\uff0c\u7528\u6765\u751f\u6210\u968f\u673a\u6570\u636e\u3001\u8ba1\u7b97\u6a21\u578b\u53c2\u6570\u91cf\u548c\u83b7\u53d6\u5f53\u524d\u8fdb\u7a0b\u5185\u5b58\u5360\u7528\uff1a"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"def get_data(batch_size: int, seq_len: int,\n             vocab_size: int, device: Optional[str] = None) -> Dict[str, torch.Tensor]:\n    device = torch.cuda.current_device() if device is None else device\n    input_ids = torch.randint(vocab_size, (batch_size, seq_len),\n                              device=device)\n    attn_mask = torch.ones_like(input_ids)\n    return dict(input_ids=input_ids, attention_mask=attn_mask)\ndef get_model_numel(model: nn.Module) -> int:\n    return sum(p.numel() for p in model.parameters())\ndef get_mem_usage() -> int:\n    proc = psutil.Process(os.getpid())\n    return proc.memory_info().rss\n")),(0,o.kt)("p",null,"\u6211\u4eec\u9996\u5148\u5c1d\u8bd5\u5728 CPU \u4e0a\u8bad\u7ec3 GPT \u6a21\u578b\uff1a"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"def train_cpu(nvme_offload_fraction: float = 0.0):\n    config = GPT2Config()\n    model = GPT2LMHeadModel(config)\n    criterion = GPTLMLoss()\n    optimizer = HybridAdam(model.parameters(), nvme_offload_fraction=nvme_offload_fraction)\n    print(f'Model numel: {get_model_numel(model) / 1024**3:.3f} B')\n    start = time.time()\n    for step in range(3):\n        data = get_data(4, 128, config.vocab_size, device='cpu')\n        outputs = model(**data)\n        loss = criterion(outputs.logits, data['input_ids'])\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        print(f'[{step}] loss: {loss.item():.3f}')\n    print(f'Time: {time.time() - start:.3f} s')\n    print(f'Mem usage: {get_mem_usage() / 1024**2:.3f} MB')\n")),(0,o.kt)("p",null,"\u4e0d\u4f7f\u7528 NVME \u5378\u8f7d\uff1a"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"train_cpu(0.0)\n")),(0,o.kt)("p",null,"\u6211\u4eec\u53ef\u80fd\u5f97\u5230\u5982\u4e0b\u8f93\u51fa\uff1a"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"Model numel: 0.116 B\n[0] loss: 10.953\n[1] loss: 10.974\n[2] loss: 10.965\nTime: 7.739 s\nMem usage: 5966.445 MB\n")),(0,o.kt)("p",null,"\u7136\u540e\u4f7f\u7528\uff08\u5168\u91cf\uff09 NVME \u5378\u8f7d\uff1a"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"train_cpu(1.0)\n")),(0,o.kt)("p",null,"\u6211\u4eec\u53ef\u80fd\u5f97\u5230\uff1a"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"Model numel: 0.116 B\n[0] loss: 10.951\n[1] loss: 10.994\n[2] loss: 10.984\nTime: 8.527 s\nMem usage: 4968.016 MB\n")),(0,o.kt)("p",null,"\u5bf9\u4e8e\u67091.16\u4ebf\u53c2\u6570\u7684 GPT2-S \u6765\u8bf4\uff0c\u5b83\u7684\u4f18\u5316\u5668\u72b6\u6001\u5927\u7ea6\u9700\u8981\u5360\u7528 0.928 GB \u5185\u5b58\u3002NVME \u5378\u8f7d\u8282\u7701\u4e86\u5927\u7ea6 998 MB \u5185\u5b58\uff0c\u7b26\u5408\u6211\u4eec\u7684\u9884\u671f\u3002"),(0,o.kt)("p",null,"\u7136\u540e\u6211\u4eec\u53ef\u4ee5\u7528 Gemini \u6765\u8bad\u7ec3 GPT \u6a21\u578b\u3002\u653e\u7f6e\u7b56\u7565\u5e94\u8be5\u8bbe\u7f6e\u4e3a",(0,o.kt)("inlineCode",{parentName:"p"},'"auto"'),"\u3001 ",(0,o.kt)("inlineCode",{parentName:"p"},'"cpu"')," \u6216 ",(0,o.kt)("inlineCode",{parentName:"p"},'"const"'),"\u3002"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"def train_gemini_cpu(nvme_offload_fraction: float = 0.0):\n    colossalai.launch_from_torch({})\n    config = GPT2Config()\n    with ColoInitContext(device=torch.cuda.current_device()):\n        model = GPT2LMHeadModel(config)\n    criterion = GPTLMLoss()\n    optimizer = HybridAdam(model.parameters(), nvme_offload_fraction=nvme_offload_fraction)\n    print(f'Model numel: {get_model_numel(model) / 1024**3:.3f} B')\n\n    plugin = GeminiPlugin(\n                strict_ddp_mode=True,\n                device=torch.cuda.current_device(),\n                placement_policy='cpu',\n                pin_memory=True,\n                hidden_dim=config.n_embd,\n                initial_scale=2**5\n                )\n    booster = Booster(plugin)\n    model, optimizer, criterion, _* = booster.boost(model, optimizer, criterion)\n\n    start = time.time()\n    for step in range(3):\n        data = get_data(4, 128, config.vocab_size)\n        outputs = model(**data)\n        loss = criterion(outputs.logits, data['input_ids'])\n        booster.backward(loss, optimizer)\n        optimizer.step()\n        optimizer.zero_grad()\n        print(f'[{step}] loss: {loss.item():.3f}')\n    print(f'Time: {time.time() - start:.3f} s')\n    print(f'Mem usage: {get_mem_usage() / 1024**2:.3f} MB')\n")),(0,o.kt)("p",null,"\u4e0d\u4f7f\u7528 NVME \u5378\u8f7d\uff1a"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"train_gemini_cpu(0.0)\n")),(0,o.kt)("p",null,"\u6211\u4eec\u53ef\u80fd\u5f97\u5230\uff1a"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"Model numel: 0.116 B\nsearching chunk configuration is completed in 0.27 s.\nused number: 118.68 MB, wasted number: 0.75 MB\ntotal wasted percentage is 0.63%\n[0] loss: 10.953\n[1] loss: 10.938\n[2] loss: 10.969\nTime: 2.997 s\nMem usage: 5592.227 MB\n")),(0,o.kt)("p",null,"\u7136\u540e\u4f7f\u7528\uff08\u5168\u91cf\uff09 NVME \u5378\u8f7d\uff1a"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"train_gemini_cpu(1.0)\n")),(0,o.kt)("p",null,"\u6211\u4eec\u53ef\u80fd\u5f97\u5230\uff1a"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"Model numel: 0.116 B\nsearching chunk configuration is completed in 0.27 s.\nused number: 118.68 MB, wasted number: 0.75 MB\ntotal wasted percentage is 0.63%\n[0] loss: 10.953\n[1] loss: 10.938\n[2] loss: 10.969\nTime: 3.691 s\nMem usage: 5298.344 MB\n")),(0,o.kt)("p",null,"NVME \u5378\u8f7d\u8282\u7701\u4e86\u5927\u7ea6 294 MB \u5185\u5b58\u3002\u6ce8\u610f\u4f7f\u7528 Gemini \u7684 ",(0,o.kt)("inlineCode",{parentName:"p"},"pin_memory")," \u529f\u80fd\u53ef\u4ee5\u52a0\u901f\u8bad\u7ec3\uff0c\u4f46\u662f\u4f1a\u589e\u52a0\u5185\u5b58\u5360\u7528\u3002\u6240\u4ee5\u8fd9\u4e2a\u7ed3\u679c\u4e5f\u662f\u7b26\u5408\u6211\u4eec\u9884\u671f\u7684\u3002\u5982\u679c\u6211\u4eec\u5173\u95ed ",(0,o.kt)("inlineCode",{parentName:"p"},"pin_memory"),"\uff0c\u6211\u4eec\u4ecd\u7136\u53ef\u4ee5\u89c2\u5bdf\u5230\u5927\u7ea6 900 MB \u7684\u5185\u5b58\u5360\u7528\u4e0b\u964d\u3002"),(0,o.kt)("h2",{id:"api-\u53c2\u8003"},"API \u53c2\u8003"),(0,o.kt)(i.Cl,{mdxType:"DocStringContainer"},(0,o.kt)("div",null,(0,o.kt)(i.Dx,{type:"class",name:"colossalai.nn.HybridAdam",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/nn/optimizer/hybrid_adam.py#L11",mdxType:"Title"}),(0,o.kt)(i.Pc,{mdxType:"Signature"},"model_params, lr = 0.001, bias_correction = True, betas = (0.9, 0.999), eps = 1e-08, weight_decay = 0, adamw_mode = True, nvme_offload_fraction: float = 0.0, nvme_offload_dir: typing.Optional[str] = None, **defaults: typing.Any"),(0,o.kt)(i.aE,{mdxType:"Parameters"},"- **model_params** (iterable) -- iterable of parameters of dicts defining\n  parameter groups.\n- **lr** (float, optional) -- learning rate. (default: 1e-3)\n- **betas** (Tuple[float, float], optional) -- coefficients used for computing\n  running averages of gradient and its square. (default: (0.9, 0.999))\n- **eps** (float, optional) -- term added to the denominator to improve\n  numerical stability. (default: 1e-8)\n- **weight_decay** (float, optional) -- weight decay (L2 penalty) (default: 0)\n- **amsgrad** (boolean, optional) -- whether to use the AMSGrad variant of this\n  algorithm from the paper *On the Convergence of Adam and Beyond*_\n  (default: False) NOT SUPPORTED yet in CPUAdam!\n- **adamw_mode** (boolean, optional) -- Apply L2 regularization or weight decay\n  True for decoupled weight decay(also known as AdamW) (default: True)\n- **simd_log** (boolean, optional) -- whether to show if you are using SIMD to\n  accelerate. (default: False)\n- **nvme_offload_fraction** (float, optional) -- Fraction of optimizer states to be offloaded to NVMe. Defaults to 0.0.\n- **nvme_offload_dir** (Optional[str], optional) -- Directory to save NVMe offload files.\n  If it's `None`, a random temporary directory will be used. Defaults to None.")),(0,o.kt)("div",null,(0,o.kt)(i.iz,{name:"Description",mdxType:"Divider"}),"Implements Adam algorithm.",(0,o.kt)("p",null,"Supports parameters updating on both GPU and CPU, depending on the device of parameters.\nBut the parameters and gradients should on the same device:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Parameters on CPU and gradients on CPU is allowed."),(0,o.kt)("li",{parentName:"ul"},"Parameters on GPU and gradients on GPU is allowed."),(0,o.kt)("li",{parentName:"ul"},"Parameters on GPU and gradients on CPU is ",(0,o.kt)("strong",{parentName:"li"},"not")," allowed.")),(0,o.kt)("p",null,(0,o.kt)("em",{parentName:"p"},"HybridAdam")," requires CUDA extensions which can be built during installation or runtime."),(0,o.kt)("p",null,"This version of Hybrid Adam is an hybrid of CPUAdam and FusedAdam."),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"For parameters updating on CPU, it uses CPUAdam."),(0,o.kt)("li",{parentName:"ul"},"For parameters updating on GPU, it uses FusedAdam."),(0,o.kt)("li",{parentName:"ul"},"Hybrid precision calculation of fp16 and fp32 is supported, eg fp32 parameters and fp16 gradients.")),(0,o.kt)("p",null,"[",(0,o.kt)("inlineCode",{parentName:"p"},"colossalai.nn.optimizer.HybridAdam"),"]"," may be used as a drop-in replacement for ",(0,o.kt)("inlineCode",{parentName:"p"},"torch.optim.AdamW"),",\nor ",(0,o.kt)("inlineCode",{parentName:"p"},"torch.optim.Adam")," with ",(0,o.kt)("inlineCode",{parentName:"p"},"adamw_mode=False")),(0,o.kt)("p",null,"Adam was been proposed in ",(0,o.kt)("em",{parentName:"p"},"Adam: A Method for Stochastic Optimization"),"_."),(0,o.kt)("p",null,".. _Adam\\: A Method for Stochastic Optimization:\n",(0,o.kt)("a",{parentName:"p",href:"https://arxiv.org/abs/1412.6980"},"https://arxiv.org/abs/1412.6980"),"\n.. _On the Convergence of Adam and Beyond:\n",(0,o.kt)("a",{parentName:"p",href:"https://openreview.net/forum?id=ryQu7f-RZ"},"https://openreview.net/forum?id=ryQu7f-RZ")))),(0,o.kt)(i.Cl,{mdxType:"DocStringContainer"},(0,o.kt)("div",null,(0,o.kt)(i.Dx,{type:"class",name:"colossalai.nn.CPUAdam",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/nn/optimizer/cpu_adam.py#L11",mdxType:"Title"}),(0,o.kt)(i.Pc,{mdxType:"Signature"},"model_params, lr = 0.001, bias_correction = True, betas = (0.9, 0.999), eps = 1e-08, weight_decay = 0, adamw_mode = True, nvme_offload_fraction: float = 0.0, nvme_offload_dir: typing.Optional[str] = None"),(0,o.kt)(i.aE,{mdxType:"Parameters"},"- **model_params** (iterable) -- iterable of parameters of dicts defining\n  parameter groups.\n- **lr** (float, optional) -- learning rate. (default: 1e-3)\n- **betas** (Tuple[float, float], optional) -- coefficients used for computing\n  running averages of gradient and its square. (default: (0.9, 0.999))\n- **eps** (float, optional) -- term added to the denominator to improve\n  numerical stability. (default: 1e-8)\n- **weight_decay** (float, optional) -- weight decay (L2 penalty) (default: 0)\n- **amsgrad** (boolean, optional) -- whether to use the AMSGrad variant of this\n  algorithm from the paper *On the Convergence of Adam and Beyond*_\n  (default: False) NOT SUPPORTED yet in CPUAdam!\n- **adamw_mode** (boolean, optional) -- Apply L2 regularization or weight decay\n  True for decoupled weight decay(also known as AdamW) (default: True)\n- **simd_log** (boolean, optional) -- whether to show if you are using SIMD to\n  accelerate. (default: False)\n- **nvme_offload_fraction** (float, optional) -- Fraction of optimizer states to be offloaded to NVMe. Defaults to 0.0.\n- **nvme_offload_dir** (Optional[str], optional) -- Directory to save NVMe offload files.\n  If it's `None`, a random temporary directory will be used. Defaults to None.")),(0,o.kt)("div",null,(0,o.kt)(i.iz,{name:"Description",mdxType:"Divider"}),"Implements Adam algorithm.",(0,o.kt)("p",null,"Supports parameters updating on both GPU and CPU, depending on the device of parameters.\nBut the parameters and gradients should on the same device:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Parameters on CPU and gradients on CPU is allowed."),(0,o.kt)("li",{parentName:"ul"},"Parameters on GPU and gradients on GPU is allowed."),(0,o.kt)("li",{parentName:"ul"},"Parameters on GPU and gradients on CPU is ",(0,o.kt)("strong",{parentName:"li"},"not")," allowed.")),(0,o.kt)("p",null,(0,o.kt)("em",{parentName:"p"},"CPUAdam")," requires CUDA extensions which can be built during installation or runtime."),(0,o.kt)("p",null,"This version of CPU Adam accelerates parameters updating on CPU with SIMD.\nSupport of AVX2 or AVX512 is required."),(0,o.kt)("p",null,"The GPU part is implemented in an naive way."),(0,o.kt)("p",null,"CPU Adam also supports the hybrid precision calculation, eg. fp32 parameters and fp16 gradients."),(0,o.kt)("p",null,"[",(0,o.kt)("inlineCode",{parentName:"p"},"colossalai.nn.optimizer.CPUAdam"),"]"," may be used as a drop-in replacement for ",(0,o.kt)("inlineCode",{parentName:"p"},"torch.optim.AdamW"),",\nor ",(0,o.kt)("inlineCode",{parentName:"p"},"torch.optim.Adam")," with ",(0,o.kt)("inlineCode",{parentName:"p"},"adamw_mode=False")),(0,o.kt)("p",null,"Adam was been proposed in ",(0,o.kt)("em",{parentName:"p"},"Adam: A Method for Stochastic Optimization"),"_."),(0,o.kt)("p",null,".. _Adam\\: A Method for Stochastic Optimization:\n",(0,o.kt)("a",{parentName:"p",href:"https://arxiv.org/abs/1412.6980"},"https://arxiv.org/abs/1412.6980"),"\n.. _On the Convergence of Adam and Beyond:\n",(0,o.kt)("a",{parentName:"p",href:"https://openreview.net/forum?id=ryQu7f-RZ"},"https://openreview.net/forum?id=ryQu7f-RZ")))))}c.isMDXComponent=!0}}]);