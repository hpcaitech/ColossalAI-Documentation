"use strict";(self.webpackChunkdemo=self.webpackChunkdemo||[]).push([[8430],{3905:(e,n,t)=>{t.d(n,{Zo:()=>m,kt:()=>f});var o=t(7294);function a(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function r(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);n&&(o=o.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,o)}return t}function l(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?r(Object(t),!0).forEach((function(n){a(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):r(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function i(e,n){if(null==e)return{};var t,o,a=function(e,n){if(null==e)return{};var t,o,a={},r=Object.keys(e);for(o=0;o<r.length;o++)t=r[o],n.indexOf(t)>=0||(a[t]=e[t]);return a}(e,n);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(o=0;o<r.length;o++)t=r[o],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(a[t]=e[t])}return a}var s=o.createContext({}),p=function(e){var n=o.useContext(s),t=n;return e&&(t="function"==typeof e?e(n):l(l({},n),e)),t},m=function(e){var n=p(e.components);return o.createElement(s.Provider,{value:n},e.children)},c="mdxType",d={inlineCode:"code",wrapper:function(e){var n=e.children;return o.createElement(o.Fragment,{},n)}},u=o.forwardRef((function(e,n){var t=e.components,a=e.mdxType,r=e.originalType,s=e.parentName,m=i(e,["components","mdxType","originalType","parentName"]),c=p(t),u=a,f=c["".concat(s,".").concat(u)]||c[u]||d[u]||r;return t?o.createElement(f,l(l({ref:n},m),{},{components:t})):o.createElement(f,l({ref:n},m))}));function f(e,n){var t=arguments,a=n&&n.mdxType;if("string"==typeof e||a){var r=t.length,l=new Array(r);l[0]=u;var i={};for(var s in n)hasOwnProperty.call(n,s)&&(i[s]=n[s]);i.originalType=e,i[c]="string"==typeof e?e:a,l[1]=i;for(var p=2;p<r;p++)l[p]=t[p];return o.createElement.apply(null,l)}return o.createElement.apply(null,t)}u.displayName="MDXCreateElement"},7631:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>s,contentTitle:()=>l,default:()=>d,frontMatter:()=>r,metadata:()=>i,toc:()=>p});var o=t(7462),a=(t(7294),t(3905));const r={},l=void 0,i={unversionedId:"features/nvme_offload",id:"features/nvme_offload",title:"nvme_offload",description:"\u4f5c\u8005: Hongxin Liu",source:"@site/i18n/zh-Hans/docusaurus-plugin-content-docs/current/features/nvme_offload.md",sourceDirName:"features",slug:"/features/nvme_offload",permalink:"/zh-Hans/docs/features/nvme_offload",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/features/nvme_offload.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"\u6d41\u6c34\u5e76\u884c",permalink:"/zh-Hans/docs/features/pipeline_parallel"},next:{title:"\u4f7f\u7528\u6d41\u6c34\u5e76\u884c\u8bad\u7ec3 ViT",permalink:"/zh-Hans/docs/advanced_tutorials/train_vit_using_pipeline_parallelism"}},s={},p=[{value:"\u5f15\u8a00",id:"\u5f15\u8a00",level:2},{value:"\u4f7f\u7528",id:"\u4f7f\u7528",level:2},{value:"Exampls",id:"exampls",level:2},{value:"API \u53c2\u8003",id:"api-\u53c2\u8003",level:2}],m={toc:p},c="wrapper";function d(e){let{components:n,...t}=e;return(0,a.kt)(c,(0,o.Z)({},m,t,{components:n,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"nvme-offload"},"NVMe offload"),(0,a.kt)("p",null,"\u4f5c\u8005: Hongxin Liu"),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"\u524d\u7f6e\u6559\u7a0b:")),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"/zh-Hans/docs/features/zero_with_chunk"},"\u57fa\u4e8eChunk\u5185\u5b58\u7ba1\u7406\u7684\u96f6\u5197\u4f59\u4f18\u5316\u5668 (ZeRO)"))),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"\u76f8\u5173\u8bba\u6587")),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2101.06840"},"ZeRO-Offload: Democratizing Billion-Scale Model Training")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2104.07857"},"ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning"))),(0,a.kt)("h2",{id:"\u5f15\u8a00"},"\u5f15\u8a00"),(0,a.kt)("p",null,"\u5982\u679c\u6a21\u578b\u5177\u6709",(0,a.kt)("inlineCode",{parentName:"p"},"N"),"\u4e2a\u53c2\u6570\uff0c\u5728\u4f7f\u7528 Adam \u65f6\uff0c\u4f18\u5316\u5668\u72b6\u6001\u5177\u6709",(0,a.kt)("inlineCode",{parentName:"p"},"8N"),"\u4e2a\u53c2\u6570\u3002\u5bf9\u4e8e\u5341\u4ebf\u89c4\u6a21\u7684\u6a21\u578b\uff0c\u4f18\u5316\u5668\u72b6\u6001\u81f3\u5c11\u9700\u8981 32 GB \u5185\u5b58\u3002 GPU\u663e\u5b58\u9650\u5236\u4e86\u6211\u4eec\u53ef\u4ee5\u8bad\u7ec3\u7684\u6a21\u578b\u89c4\u6a21\uff0c\u8fd9\u79f0\u4e3aGPU\u663e\u5b58\u5899\u3002\u5982\u679c\u6211\u4eec\u5c06\u4f18\u5316\u5668\u72b6\u6001 offload \u5230\u78c1\u76d8\uff0c\u6211\u4eec\u53ef\u4ee5\u7a81\u7834 GPU \u5185\u5b58\u5899\u3002"),(0,a.kt)("p",null,"\u6211\u4eec\u5b9e\u73b0\u4e86\u4e00\u4e2a\u7528\u6237\u53cb\u597d\u4e14\u9ad8\u6548\u7684\u5f02\u6b65 Tensor I/O \u5e93\uff1a",(0,a.kt)("a",{parentName:"p",href:"https://github.com/hpcaitech/TensorNVMe"},"TensorNVMe"),"\u3002\u6709\u4e86\u8fd9\u4e2a\u5e93\uff0c\u6211\u4eec\u53ef\u4ee5\u7b80\u5355\u5730\u5b9e\u73b0 NVMe offload\u3002"),(0,a.kt)("blockquote",null,(0,a.kt)("p",{parentName:"blockquote"},"\u8be5\u5e93\u4e0e\u5404\u79cd\u78c1\u76d8\uff08HDD\u3001SATA SSD \u548c NVMe SSD\uff09\u517c\u5bb9\u3002\u7531\u4e8e HDD \u6216 SATA SSD \u7684 I/O \u5e26\u5bbd\u8f83\u4f4e\uff0c\u5efa\u8bae\u4ec5\u5728 NVMe \u78c1\u76d8\u4e0a\u4f7f\u7528\u6b64\u5e93\u3002")),(0,a.kt)("p",null,"\u5728\u4f18\u5316\u53c2\u6570\u65f6\uff0c\u6211\u4eec\u53ef\u4ee5\u5c06\u4f18\u5316\u8fc7\u7a0b\u5206\u4e3a\u4e09\u4e2a\u9636\u6bb5\uff1a\u8bfb\u53d6\u3001\u8ba1\u7b97\u548c offload\u3002\u6211\u4eec\u4ee5\u6d41\u6c34\u7ebf\u7684\u65b9\u5f0f\u6267\u884c\u4f18\u5316\u8fc7\u7a0b\uff0c\u8fd9\u53ef\u4ee5\u91cd\u53e0\u8ba1\u7b97\u548c I/O\u3002"),(0,a.kt)("figure",{style:{textAlign:"center"}},(0,a.kt)("img",{src:"https://s2.loli.net/2022/08/16/CvRnowrsNyB4hza.jpg"}),(0,a.kt)("figcaption",null,"\u4f18\u5316\u8fc7\u7a0b")),(0,a.kt)("h2",{id:"\u4f7f\u7528"},"\u4f7f\u7528"),(0,a.kt)("p",null,"\u9996\u5148\uff0c\u8bf7\u786e\u4fdd\u60a8\u5b89\u88c5\u4e86 ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/hpcaitech/TensorNVMe"},"TensorNVMe"),":"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell"},"pip install packaging\npip install tensornvme\n")),(0,a.kt)("p",null,"\u6211\u4eec\u4e3a Adam (",(0,a.kt)("a",{parentName:"p",href:"https://colossalai.readthedocs.io/en/latest/colossalai/colossalai.nn.optimizer.cpu_adam.html"},"CPUAdam")," \u548c ",(0,a.kt)("a",{parentName:"p",href:"https://colossalai.readthedocs.io/en/latest/colossalai/colossalai.nn.optimizer.hybrid_adam.html"},"HybridAdam"),") \u5b9e\u73b0\u4e86\u4f18\u5316\u5668\u72b6\u6001\u7684 NVMe offload\u3002"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"from colossalai.nn.optimizer import CPUAdam, HybridAdam\n\noptimizer = HybridAdam(model.parameters(), lr=1e-3, nvme_offload_fraction=1.0, nvme_offload_dir='./')\n")),(0,a.kt)("p",null,(0,a.kt)("inlineCode",{parentName:"p"},"nvme_offload_fraction")," \u662f\u8981 offload \u5230 NVMe \u7684\u4f18\u5316\u5668\u72b6\u6001\u7684\u6bd4\u4f8b\u3002 ",(0,a.kt)("inlineCode",{parentName:"p"},"nvme_offload_dir")," \u662f\u4fdd\u5b58 NVMe offload \u6587\u4ef6\u7684\u76ee\u5f55\u3002\u5982\u679c ",(0,a.kt)("inlineCode",{parentName:"p"},"nvme_offload_dir")," \u4e3a ",(0,a.kt)("inlineCode",{parentName:"p"},"None"),"\uff0c\u5c06\u4f7f\u7528\u968f\u673a\u4e34\u65f6\u76ee\u5f55\u3002"),(0,a.kt)("p",null,"\u5b83\u4e0e ColossalAI \u4e2d\u7684\u6240\u6709\u5e76\u884c\u65b9\u6cd5\u517c\u5bb9\u3002"),(0,a.kt)("blockquote",null,(0,a.kt)("p",{parentName:"blockquote"},"\u26a0 \u5b83\u53ea\u4f1a\u5378\u8f7d\u5728 CPU \u4e0a\u7684\u4f18\u5316\u5668\u72b6\u6001\u3002\u8fd9\u610f\u5473\u7740\u5b83\u53ea\u4f1a\u5f71\u54cd CPU \u8bad\u7ec3\u6216\u8005\u4f7f\u7528\u5378\u8f7d\u7684 Zero/Gemini\u3002")),(0,a.kt)("h2",{id:"exampls"},"Exampls"),(0,a.kt)("p",null,"Let's start from two simple examples -- training GPT with different methods. These examples relies on ",(0,a.kt)("inlineCode",{parentName:"p"},"transformers"),".\n\u9996\u5148\u8ba9\u6211\u4eec\u4ece\u4e24\u4e2a\u7b80\u5355\u7684\u4f8b\u5b50\u5f00\u59cb -- \u7528\u4e0d\u540c\u7684\u65b9\u6cd5\u8bad\u7ec3 GPT\u3002\u8fd9\u4e9b\u4f8b\u5b50\u4f9d\u8d56",(0,a.kt)("inlineCode",{parentName:"p"},"transformers"),"\u3002"),(0,a.kt)("p",null,"\u6211\u4eec\u9996\u5148\u5e94\u8be5\u5b89\u88c5\u4f9d\u8d56\uff1a"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell"},"pip install psutil transformers\n")),(0,a.kt)("p",null,"\u9996\u5148\uff0c\u6211\u4eec\u5bfc\u5165\u5fc5\u8981\u7684\u5305\u548c\u6a21\u5757\uff1a"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"import os\nimport time\nfrom typing import Dict, Optional\nimport psutil\nimport torch\nimport torch.nn as nn\nfrom transformers.models.gpt2.configuration_gpt2 import GPT2Config\nfrom transformers.models.gpt2.modeling_gpt2 import GPT2LMHeadModel\nimport colossalai\nfrom colossalai.nn.optimizer import HybridAdam\nfrom colossalai.nn.parallel import zero_model_wrapper, zero_optim_wrapper\nfrom colossalai.utils.model.colo_init_context import ColoInitContext\n")),(0,a.kt)("p",null,"\u7136\u540e\u6211\u4eec\u5b9a\u4e49\u4e00\u4e2a\u635f\u5931\u51fd\u6570\uff1a"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"class GPTLMLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.loss_fn = nn.CrossEntropyLoss()\n    def forward(self, logits, labels):\n        shift_logits = logits[..., :-1, :].contiguous()\n        shift_labels = labels[..., 1:].contiguous()\n        # Flatten the tokens\n        return self.loss_fn(shift_logits.view(-1, shift_logits.size(-1)),\n                            shift_labels.view(-1))\n")),(0,a.kt)("p",null,"\u6211\u4eec\u5b9a\u4e49\u4e00\u4e9b\u5de5\u5177\u51fd\u6570\uff0c\u7528\u6765\u751f\u6210\u968f\u673a\u6570\u636e\u3001\u8ba1\u7b97\u6a21\u578b\u53c2\u6570\u91cf\u548c\u83b7\u53d6\u5f53\u524d\u8fdb\u7a0b\u5185\u5b58\u5360\u7528\uff1a"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"def get_data(batch_size: int, seq_len: int,\n             vocab_size: int, device: Optional[str] = None) -> Dict[str, torch.Tensor]:\n    device = torch.cuda.current_device() if device is None else device\n    input_ids = torch.randint(vocab_size, (batch_size, seq_len),\n                              device=device)\n    attn_mask = torch.ones_like(input_ids)\n    return dict(input_ids=input_ids, attention_mask=attn_mask)\ndef get_model_numel(model: nn.Module) -> int:\n    return sum(p.numel() for p in model.parameters())\ndef get_mem_usage() -> int:\n    proc = psutil.Process(os.getpid())\n    return proc.memory_info().rss\n")),(0,a.kt)("p",null,"\u6211\u4eec\u9996\u5148\u5c1d\u8bd5\u5728 CPU \u4e0a\u8bad\u7ec3 GPT \u6a21\u578b\uff1a"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"def train_cpu(nvme_offload_fraction: float = 0.0):\n    config = GPT2Config()\n    model = GPT2LMHeadModel(config)\n    criterion = GPTLMLoss()\n    optimizer = HybridAdam(model.parameters(), nvme_offload_fraction=nvme_offload_fraction)\n    print(f'Model numel: {get_model_numel(model) / 1024**3:.3f} B')\n    start = time.time()\n    for step in range(3):\n        data = get_data(4, 128, config.vocab_size, device='cpu')\n        outputs = model(**data)\n        loss = criterion(outputs.logits, data['input_ids'])\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        print(f'[{step}] loss: {loss.item():.3f}')\n    print(f'Time: {time.time() - start:.3f} s')\n    print(f'Mem usage: {get_mem_usage() / 1024**2:.3f} MB')\n")),(0,a.kt)("p",null,"\u4e0d\u4f7f\u7528 NVME \u5378\u8f7d\uff1a"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"train_cpu(0.0)\n")),(0,a.kt)("p",null,"\u6211\u4eec\u53ef\u80fd\u5f97\u5230\u5982\u4e0b\u8f93\u51fa\uff1a"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"Model numel: 0.116 B\n[0] loss: 10.953\n[1] loss: 10.974\n[2] loss: 10.965\nTime: 7.739 s\nMem usage: 5966.445 MB\n")),(0,a.kt)("p",null,"\u7136\u540e\u4f7f\u7528\uff08\u5168\u91cf\uff09 NVME \u5378\u8f7d\uff1a"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"train_cpu(1.0)\n")),(0,a.kt)("p",null,"\u6211\u4eec\u53ef\u80fd\u5f97\u5230\uff1a"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"Model numel: 0.116 B\n[0] loss: 10.951\n[1] loss: 10.994\n[2] loss: 10.984\nTime: 8.527 s\nMem usage: 4968.016 MB\n")),(0,a.kt)("p",null,"\u5bf9\u4e8e\u67091.16\u4ebf\u53c2\u6570\u7684 GPT2-S \u6765\u8bf4\uff0c\u5b83\u7684\u4f18\u5316\u5668\u72b6\u6001\u5927\u7ea6\u9700\u8981\u5360\u7528 0.928 GB \u5185\u5b58\u3002NVME \u5378\u8f7d\u8282\u7701\u4e86\u5927\u7ea6 998 MB \u5185\u5b58\uff0c\u7b26\u5408\u6211\u4eec\u7684\u9884\u671f\u3002"),(0,a.kt)("p",null,"\u7136\u540e\u6211\u4eec\u53ef\u4ee5\u7528 Gemini \u6765\u8bad\u7ec3 GPT \u6a21\u578b\u3002\u653e\u7f6e\u7b56\u7565\u5e94\u8be5\u8bbe\u7f6e\u4e3a",(0,a.kt)("inlineCode",{parentName:"p"},'"auto"'),"\u3001 ",(0,a.kt)("inlineCode",{parentName:"p"},'"cpu"')," \u6216 ",(0,a.kt)("inlineCode",{parentName:"p"},'"const"'),"\u3002"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"def train_gemini_cpu(nvme_offload_fraction: float = 0.0):\n    colossalai.launch_from_torch({})\n    config = GPT2Config()\n    with ColoInitContext(device=torch.cuda.current_device()):\n        model = GPT2LMHeadModel(config)\n    criterion = GPTLMLoss()\n    optimizer = HybridAdam(model.parameters(), nvme_offload_fraction=nvme_offload_fraction)\n    print(f'Model numel: {get_model_numel(model) / 1024**3:.3f} B')\n    gemini_config = dict(strict_ddp_mode=True, device=torch.cuda.current_device(),\n                         placement_policy='cpu', pin_memory=True, hidden_dim=config.n_embd)\n    model = zero_model_wrapper(model, zero_stage=3, gemini_config=gemini_config)\n    optimizer = zero_optim_wrapper(model, optimizer, initial_scale=2**5)\n    start = time.time()\n    for step in range(3):\n        data = get_data(4, 128, config.vocab_size)\n        outputs = model(**data)\n        loss = criterion(outputs.logits, data['input_ids'])\n        optimizer.backward(loss)\n        optimizer.step()\n        optimizer.zero_grad()\n        print(f'[{step}] loss: {loss.item():.3f}')\n    print(f'Time: {time.time() - start:.3f} s')\n    print(f'Mem usage: {get_mem_usage() / 1024**2:.3f} MB')\n")),(0,a.kt)("p",null,"\u4e0d\u4f7f\u7528 NVME \u5378\u8f7d\uff1a"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"train_gemini_cpu(0.0)\n")),(0,a.kt)("p",null,"\u6211\u4eec\u53ef\u80fd\u5f97\u5230\uff1a"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"Model numel: 0.116 B\nsearching chunk configuration is completed in 0.27 s.\nused number: 118.68 MB, wasted number: 0.75 MB\ntotal wasted percentage is 0.63%\n[0] loss: 10.953\n[1] loss: 10.938\n[2] loss: 10.969\nTime: 2.997 s\nMem usage: 5592.227 MB\n")),(0,a.kt)("p",null,"\u7136\u540e\u4f7f\u7528\uff08\u5168\u91cf\uff09 NVME \u5378\u8f7d\uff1a"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"train_gemini_cpu(1.0)\n")),(0,a.kt)("p",null,"\u6211\u4eec\u53ef\u80fd\u5f97\u5230\uff1a"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"Model numel: 0.116 B\nsearching chunk configuration is completed in 0.27 s.\nused number: 118.68 MB, wasted number: 0.75 MB\ntotal wasted percentage is 0.63%\n[0] loss: 10.953\n[1] loss: 10.938\n[2] loss: 10.969\nTime: 3.691 s\nMem usage: 5298.344 MB\n")),(0,a.kt)("p",null,"NVME \u5378\u8f7d\u8282\u7701\u4e86\u5927\u7ea6 294 MB \u5185\u5b58\u3002\u6ce8\u610f\u4f7f\u7528 Gemini \u7684 ",(0,a.kt)("inlineCode",{parentName:"p"},"pin_memory")," \u529f\u80fd\u53ef\u4ee5\u52a0\u901f\u8bad\u7ec3\uff0c\u4f46\u662f\u4f1a\u589e\u52a0\u5185\u5b58\u5360\u7528\u3002\u6240\u4ee5\u8fd9\u4e2a\u7ed3\u679c\u4e5f\u662f\u7b26\u5408\u6211\u4eec\u9884\u671f\u7684\u3002\u5982\u679c\u6211\u4eec\u5173\u95ed ",(0,a.kt)("inlineCode",{parentName:"p"},"pin_memory"),"\uff0c\u6211\u4eec\u4ecd\u7136\u53ef\u4ee5\u89c2\u5bdf\u5230\u5927\u7ea6 900 MB \u7684\u5185\u5b58\u5360\u7528\u4e0b\u964d\u3002"),(0,a.kt)("h2",{id:"api-\u53c2\u8003"},"API \u53c2\u8003"),(0,a.kt)("p",null,"{{ autodoc:colossalai.nn.optimizer.HybridAdam }}"),(0,a.kt)("p",null,"{{ autodoc:colossalai.nn.optimizer.CPUAdam }}"))}d.isMDXComponent=!0}}]);