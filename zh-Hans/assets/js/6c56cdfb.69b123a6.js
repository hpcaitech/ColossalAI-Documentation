"use strict";(self.webpackChunkdemo=self.webpackChunkdemo||[]).push([[9173],{6999:(e,t,n)=>{n.d(t,{Cl:()=>l,Dx:()=>d,Pc:()=>a,aE:()=>p,e_:()=>u,iz:()=>r,nT:()=>s});var o=n(7294),i=n(398);n(814);function l(e){return o.createElement("div",{className:"docstring-container"},e.children)}function a(e){return o.createElement("div",{className:"signature"},"(",e.children,")")}function r(e){return o.createElement("div",{class:"divider"},o.createElement("span",{class:"divider-text"},e.name))}function p(e){return o.createElement("div",null,o.createElement(r,{name:"Parameters"}),o.createElement(i.D,null,e.children))}function s(e){return o.createElement("div",null,o.createElement(r,{name:"Returns"}),o.createElement(i.D,null,`${e.name}: ${e.desc}`))}function d(e){return o.createElement("div",{className:"title-container"},o.createElement("div",{className:"title-module"},o.createElement("h5",null,e.type),"\xa0 ",o.createElement("h3",null,e.name)),o.createElement("div",{className:"title-source"},"<",o.createElement("a",{href:e.source,className:"title-source"},"source"),">"))}function u(e){return o.createElement("div",null,o.createElement(r,{name:"Example"}),o.createElement(i.D,null,e.code))}},8552:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>r,default:()=>m,frontMatter:()=>a,metadata:()=>p,toc:()=>d});var o=n(7462),i=(n(7294),n(3905)),l=n(6999);const a={},r="\u96f6\u6c14\u6ce1\u6d41\u6c34\u7ebf\u5e76\u884c",p={unversionedId:"features/zerobubble_pipeline_parallelism",id:"features/zerobubble_pipeline_parallelism",title:"\u96f6\u6c14\u6ce1\u6d41\u6c34\u7ebf\u5e76\u884c",description:"\u4f5c\u8005: Junwen Duan, Hongxin Liu",source:"@site/i18n/zh-Hans/docusaurus-plugin-content-docs/current/features/zerobubble_pipeline_parallelism.md",sourceDirName:"features",slug:"/features/zerobubble_pipeline_parallelism",permalink:"/zh-Hans/docs/features/zerobubble_pipeline_parallelism",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/features/zerobubble_pipeline_parallelism.md",tags:[],version:"current",frontMatter:{}},s={},d=[{value:"\u4ecb\u7ecd",id:"\u4ecb\u7ecd",level:2},{value:"\u4f7f\u7528",id:"\u4f7f\u7528",level:2},{value:"step 1. \u5f15\u7528\u4ed3\u5e93",id:"step-1-\u5f15\u7528\u4ed3\u5e93",level:3},{value:"step 2. \u521d\u59cb\u5316\u5206\u5e03\u5f0f\u73af\u5883",id:"step-2-\u521d\u59cb\u5316\u5206\u5e03\u5f0f\u73af\u5883",level:3},{value:"step 3. \u521d\u59cb\u5316\u6a21\u578b\u4f18\u5316\u5668",id:"step-3-\u521d\u59cb\u5316\u6a21\u578b\u4f18\u5316\u5668",level:3},{value:"step 4.\u521d\u59cb\u5316\u6d41\u6c34\u7ebfSchedule",id:"step-4\u521d\u59cb\u5316\u6d41\u6c34\u7ebfschedule",level:3},{value:"step 5.\u521d\u59cb\u5316Booster",id:"step-5\u521d\u59cb\u5316booster",level:3},{value:"step 6.\u8bad\u7ec3\u6a21\u578b",id:"step-6\u8bad\u7ec3\u6a21\u578b",level:3},{value:"\u8fdb\u9636\u4f7f\u7528\u6280\u5de7",id:"\u8fdb\u9636\u4f7f\u7528\u6280\u5de7",level:2},{value:"1.\u5728ZeroBubble\u4e2d\u4f7f\u7528\u5143\u6570\u636e\u7f13\u5b58",id:"1\u5728zerobubble\u4e2d\u4f7f\u7528\u5143\u6570\u636e\u7f13\u5b58",level:3},{value:"2.\u540c\u65f6\u4f7f\u7528ZeroBubble\u548c\u6df7\u5408\u5e76\u884c",id:"2\u540c\u65f6\u4f7f\u7528zerobubble\u548c\u6df7\u5408\u5e76\u884c",level:3},{value:"\u6a21\u578b\u517c\u5bb9\u6027",id:"\u6a21\u578b\u517c\u5bb9\u6027",level:2},{value:"API \u53c2\u8003",id:"api-\u53c2\u8003",level:2}],u={toc:d},c="wrapper";function m(e){let{components:t,...n}=e;return(0,i.kt)(c,(0,o.Z)({},u,n,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"\u96f6\u6c14\u6ce1\u6d41\u6c34\u7ebf\u5e76\u884c"},"\u96f6\u6c14\u6ce1\u6d41\u6c34\u7ebf\u5e76\u884c"),(0,i.kt)("p",null,"\u4f5c\u8005: ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/duanjunwen"},"Junwen Duan"),", ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/ver217"},"Hongxin Liu")),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"\u76f8\u5173\u8bba\u6587")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2401.10241"},"Zero Bubble Pipeline Parallelism"))),(0,i.kt)("h2",{id:"\u4ecb\u7ecd"},"\u4ecb\u7ecd"),(0,i.kt)("p",null,"\u96f6\u6c14\u6ce1\uff08V Schedule\uff09\uff1a\n\u4e0e\u65e9\u671f\u5de5\u4f5c\u4e2d\u76841F1B\u65b9\u6848\u76f8\u6bd4\uff0c\u96f6\u6c14\u6ce1\u6d41\u6c34\u7ebf\u5e76\u884c\u5c06B\u5206\u6210\u4e24\u4e2a\u9636\u6bb5\uff08\u4e5f\u79f0\u4e3a\u6fc0\u6d3b\u68af\u5ea6\u548c\u6743\u91cd\u68af\u5ea6\uff09\uff0c\u5f62\u59821F1B1W\u8fd9\u6837\u7684\u65b9\u6848\u53ef\u4ee5\u8fdb\u4e00\u6b65\u51cf\u5c11\u6c14\u6ce1\u3002"),(0,i.kt)("h2",{id:"\u4f7f\u7528"},"\u4f7f\u7528"),(0,i.kt)("p",null,"\u6211\u4eec\u5c06\u6f14\u793a\u5982\u4f55\u5728 4 \u4e2a GPU \u4e0a\u4f7f\u7528\u5e26\u6709 booster API \u7684 ZeroBubble"),(0,i.kt)("h3",{id:"step-1-\u5f15\u7528\u4ed3\u5e93"},"step 1. \u5f15\u7528\u4ed3\u5e93"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"import torch\nimport torch.distributed as dist\nimport torch.nn as nn\nfrom torch.testing import assert_close\nfrom transformers.models.llama.configuration_llama import LlamaConfig\nfrom transformers.models.llama.modeling_llama import LlamaModel\n\nimport colossalai\nfrom colossalai.booster.booster import Booster\nfrom colossalai.booster.plugin.moe_hybrid_parallel_plugin import HybridParallelPlugin\nfrom colossalai.pipeline.schedule.zero_bubble_pp import ZeroBubbleVPipeScheduler\n")),(0,i.kt)("h3",{id:"step-2-\u521d\u59cb\u5316\u5206\u5e03\u5f0f\u73af\u5883"},"step 2. \u521d\u59cb\u5316\u5206\u5e03\u5f0f\u73af\u5883"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'colossalai.launch(rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")\n')),(0,i.kt)("h3",{id:"step-3-\u521d\u59cb\u5316\u6a21\u578b\u4f18\u5316\u5668"},"step 3. \u521d\u59cb\u5316\u6a21\u578b\u4f18\u5316\u5668"),(0,i.kt)("p",null,"\u5efa\u7acb\u6211\u4eec\u7684\u6a21\u578b\u548c\u4f18\u5316\u5668 \u6211\u4eec\u521b\u5efa\u4e86\u4e00\u4e2a\u5e26\u67098\u5c42Decoder-Layer\u7684 Llama\u3002\u7136\u540e\uff0c\u4f7f\u7528get_v_schedule()\u51fd\u6570\u521b\u5efaPipelineGraph\u548cPipeline schedule\u3002"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'# Global Param\nNUM_BATCH = 8\nNUM_TOK_PER_BATCH = 4\nNUM_LAYERS = 8\nHIDDEN_SIZE_PER_HEAD = 4\nNUM_HEADS = 4\n# Init Llama from huggingface\nconfiguration = LlamaConfig(\n    hidden_size=HIDDEN_SIZE_PER_HEAD * NUM_HEADS,\n    intermediate_size=HIDDEN_SIZE_PER_HEAD * NUM_HEADS * 2,\n    num_hidden_layers=NUM_LAYERS,\n    num_attention_heads=NUM_HEADS,\n    num_key_value_heads=NUM_HEADS,\n    attn_implementation="flash_attention_2",\n)\nmodel = LlamaModel(configuration).cuda()\noptimizer = torch.optim.Adam(torch_model.parameters(), lr=1)\n')),(0,i.kt)("h3",{id:"step-4\u521d\u59cb\u5316\u6d41\u6c34\u7ebfschedule"},"step 4.\u521d\u59cb\u5316\u6d41\u6c34\u7ebfSchedule"),(0,i.kt)("p",null,"\u7136\u540e\uff0c\u6211\u4eec\u9700\u8981\u4f7f\u7528 get_v_schedule() \u51fd\u6570\u521b\u5efa PipelineGraph \u548c PipelineSchedule\u3002\u6211\u4eec\u9700\u8981\u7528\u4ee5\u4e0b\u53c2\u6570\u521d\u59cb\u5316 PipelineGraph\u3002\nx_cost \u8868\u793a\u6bcf\u4e2a\u6a21\u578b\u5757\u7684\u64cd\u4f5c x \u6240\u6d88\u8017\u7684\u8fd0\u884c\u65f6\u95f4\u3002\nx_mem \u8868\u793a\u6bcf\u4e2a\u6a21\u578b\u5757\u7684\u64cd\u4f5c x \u6240\u6d88\u8017\u7684\u5185\u5b58\u91cf\u3002\n\u8fd9\u4e9b\u53c2\u6570\u90fd\u662f\u5728\u6d41\u6c34\u7ebf\u542f\u52a8\u524d\u4f30\u7b97\u5e76\u586b\u5165\u7684\u3002\u4e8b\u5b9e\u4e0a\uff0c\u5728\u6a21\u578b\u7684\u5b9e\u9645\u8ba1\u7b97\u8fc7\u7a0b\u4e2d\uff0c\u6839\u636e\u8fd0\u884c\u65f6\u95f4\u548c\u5185\u5b58\u6210\u672c\u53ef\u4ee5\u83b7\u5f97\u66f4\u597d\u7684\u7ed3\u679c\u3002\n\u5728\u4e0b\u9762\u7684\u4f8b\u5b50\u4e2d\uff0c\u6211\u4eec\u5047\u8bbe\u6a21\u578b\u7684\u6b63\u5411\u3001\u53cd\u5411 B \u548c\u53cd\u5411 W \u7684\u8ba1\u7b97\u65f6\u95f4\u5206\u522b\u4e3a 1\u30011\u30011\uff0cp2p \u901a\u4fe1\u65f6\u95f4\u4e3a 1\u3002"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"# Init schedule\nh, a, s = config.hidden_size, config.num_attention_heads, 1024\nmem_f = 34 * h + 5 * a * s\nmem_w = -32 * h\nmem_b = -mem_w - mem_f\ngraph = PipelineGraph(\n    n_stage=pp_size,\n    n_micro=num_microbatches,\n    f_cost=1,\n    b_cost=1,\n    w_cost=1,\n    c_cost=1,\n    f_mem=mem_f,\n    b_mem=mem_b,\n    w_mem=mem_w,\n)\nzbv_schedule = graph.get_v_schedule()\n")),(0,i.kt)("h3",{id:"step-5\u521d\u59cb\u5316booster"},"step 5.\u521d\u59cb\u5316Booster"),(0,i.kt)("p",null,'\u5728\u521d\u59cb\u5316Plugin\u65f6\u8f93\u5165pp_style="zbv"\uff0c\u4ee5\u4f7f\u7528ZeroBubble\u6d41\u6c34\u7ebf\u5e76\u884c\u3002'),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'plugin = HybridParallelPlugin(\n    pp_size=4,\n    num_microbatches=4,\n    tp_size=1,\n    sp_size=1,\n    zero_stage=1,\n    initial_scale=1,\n    find_unused_parameters=True,\n    pp_style="zbv",\n    scheduler_nodes=zbv_schedule,\n    num_model_chunks=2,\n)\n\ndp_size = plugin.dp_size\nbooster = Booster(plugin=plugin)\n')),(0,i.kt)("h3",{id:"step-6\u8bad\u7ec3\u6a21\u578b"},"step 6.\u8bad\u7ec3\u6a21\u578b"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'steps = 10\nfor step in range(steps):\n    input_embeddings = torch.rand(\n        NUM_BATCH, NUM_TOK_PER_BATCH, HIDDEN_SIZE_PER_HEAD * NUM_HEADS, requires_grad=True\n    ).cuda()\n    dist.all_reduce(\n        input_embeddings, group=plugin.pp_group\n    )\n    data_iter = iter([{"inputs_embeds": input_embeddings}])\n    output = booster.execute_pipeline(\n        data_iter,\n        model,\n        lambda x, y: x.last_hidden_state.mean(),\n        optimizer,\n        return_loss=True,\n        return_outputs=True,\n    )\n    optimizer.step()\n    optimizer.zero_grad()\n')),(0,i.kt)("h2",{id:"\u8fdb\u9636\u4f7f\u7528\u6280\u5de7"},"\u8fdb\u9636\u4f7f\u7528\u6280\u5de7"),(0,i.kt)("p",null,"\u5728 ColossalAI \u4e2d\uff0c\u901a\u8fc7\u4f7f\u7528MetaCache\u548c\u6df7\u5408\u5e76\u884c\u7684ZeroBubble\uff0c\u53ef\u4ee5\u83b7\u5f97\u66f4\u597d\u7684\u8bad\u7ec3\u6027\u80fd\u3002"),(0,i.kt)("h3",{id:"1\u5728zerobubble\u4e2d\u4f7f\u7528\u5143\u6570\u636e\u7f13\u5b58"},"1.\u5728ZeroBubble\u4e2d\u4f7f\u7528\u5143\u6570\u636e\u7f13\u5b58"),(0,i.kt)("p",null,'\u5728\u521d\u59cb\u5316Plugin\u65f6\u8f93\u5165 "enable_metadata_cache=True"\uff0c\u4ee5\u4fbf\u5728ZeroBubble\u7ba1\u9053\u4e2d\u4f7f\u7528\u5143\u6570\u636e\u7f13\u5b58\u3002'),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'plugin = HybridParallelPlugin(\n    pp_size=2,\n    num_microbatches=4,\n    tp_size=2,\n    sp_size=2,\n    zero_stage=1,\n    initial_scale=1,\n    enable_metadata_cache=True,\n    find_unused_parameters=True,\n    pp_style="zbv",\n    scheduler_nodes=zbv_schedule,\n    num_model_chunks=2,\n)\n')),(0,i.kt)("h3",{id:"2\u540c\u65f6\u4f7f\u7528zerobubble\u548c\u6df7\u5408\u5e76\u884c"},"2.\u540c\u65f6\u4f7f\u7528ZeroBubble\u548c\u6df7\u5408\u5e76\u884c"),(0,i.kt)("p",null,"\u5728\u521d\u59cb\u5316\u63d2\u4ef6\u65f6\u4f20\u9012 pp_size, tp_size, sp_size, \u4ee5\u4fbf\u4f7f\u7528\u96f6\u6c14\u6ce1\u6df7\u5408\u5e76\u884c\u7ba1\u9053\uff08HybridParallel with ZeroBubble Pipeline\uff09\u3002"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'plugin = HybridParallelPlugin(\n    pp_size=2,\n    num_microbatches=2,\n    tp_size=2,\n    sp_size=2,\n    zero_stage=1,\n    initial_scale=1,\n    find_unused_parameters=True,\n    pp_style="zbv",\n    scheduler_nodes=zbv_schedule,\n    num_model_chunks=2,\n)\n')),(0,i.kt)("p",null,"\u6027\u80fd\u6307\u6807"),(0,i.kt)("table",null,(0,i.kt)("tr",null,(0,i.kt)("th",{nowrap:"nowrap"},"HybridParallel Strategy"),(0,i.kt)("th",{nowrap:"nowrap",align:"center"},"Pipeline Parallel"),(0,i.kt)("th",{nowrap:"nowrap",align:"center"},"Sequence Parallel + Pipeline Parallel"),(0,i.kt)("th",{nowrap:"nowrap",align:"center"},"Data Parallel + Pipeline Parallel")),(0,i.kt)("tr",null,(0,i.kt)("td",{nowrap:"nowrap",align:"center",title:"1F1B"},"With 1F1B"),(0,i.kt)("td",{nowrap:"nowrap",align:"center"},"15.27 samples/sec"),(0,i.kt)("td",{nowrap:"nowrap",align:"center"},"17.22 samples/sec"),(0,i.kt)("td",{nowrap:"nowrap",align:"center"},"14.06 samples/sec")),(0,i.kt)("tr",null,(0,i.kt)("td",{nowrap:"nowrap",align:"center",title:"Zero Bubble"},"With Zero Bubble"),(0,i.kt)("td",{nowrap:"nowrap",align:"center"},"17.36 samples/sec"),(0,i.kt)("td",{nowrap:"nowrap",align:"center"},"18.38 samples/sec"),(0,i.kt)("td",{nowrap:"nowrap",align:"center"},"14.44 samples/sec")),(0,i.kt)("tr",null,(0,i.kt)("td",{colspan:"39"}))),(0,i.kt)("h2",{id:"\u6a21\u578b\u517c\u5bb9\u6027"},"\u6a21\u578b\u517c\u5bb9\u6027"),(0,i.kt)("table",null,(0,i.kt)("tr",null,(0,i.kt)("th",{nowrap:"nowrap"},"Shardformer/Model"),(0,i.kt)("th",{nowrap:"nowrap",align:"center"},"Bert"),(0,i.kt)("th",{nowrap:"nowrap",align:"center"},"Blip2"),(0,i.kt)("th",{nowrap:"nowrap",align:"center"},"Bloom"),(0,i.kt)("th",{nowrap:"nowrap",align:"center"},"Chatglm2"),(0,i.kt)("th",{nowrap:"nowrap",align:"center"},"Command"),(0,i.kt)("th",{nowrap:"nowrap",align:"center"},"Deepseek"),(0,i.kt)("th",{nowrap:"nowrap",align:"center"},"Falcon"),(0,i.kt)("th",{nowrap:"nowrap",align:"center"},"GPT2"),(0,i.kt)("th",{nowrap:"nowrap",align:"center"},"Gptj"),(0,i.kt)("th",{nowrap:"nowrap",align:"center"},"Llama"),(0,i.kt)("th",{nowrap:"nowrap",align:"center"},"Mistral"),(0,i.kt)("th",{nowrap:"nowrap",align:"center"},"Opt"),(0,i.kt)("th",{nowrap:"nowrap",align:"center"},"Qwen2"),(0,i.kt)("th",{nowrap:"nowrap",align:"center"},"Sam"),(0,i.kt)("th",{nowrap:"nowrap",align:"center"},"T5"),(0,i.kt)("th",{nowrap:"nowrap",align:"center"},"Vit"),(0,i.kt)("th",{nowrap:"nowrap",align:"center"},"Whisper")),(0,i.kt)("tr",null,(0,i.kt)("td",{nowrap:"nowrap",align:"center",title:"ZeroBubble"},"ZeroBubble"),(0,i.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,i.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,i.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,i.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,i.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,i.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,i.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,i.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,i.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,i.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,i.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,i.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,i.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,i.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,i.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,i.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,i.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f")),(0,i.kt)("tr",null,(0,i.kt)("td",{colspan:"39"}))),(0,i.kt)("h2",{id:"api-\u53c2\u8003"},"API \u53c2\u8003"),(0,i.kt)(l.Cl,{mdxType:"DocStringContainer"},(0,i.kt)("div",null,(0,i.kt)(l.Dx,{type:"class",name:"colossalai.pipeline.ZeroBubbleVPipeScheduler",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/pipeline/schedule/zero_bubble_pp.py#L40",mdxType:"Title"}),(0,i.kt)(l.Pc,{mdxType:"Signature"},"stage_manager: PipelineStageManager, schedule: typing.List[colossalai.pipeline.schedule.v_schedule.ScheduledNode], num_model_chunks: int, num_microbatch: typing.Optional[int] = None, microbatch_size: typing.Optional[int] = None, enable_metadata_cache: bool = True, overlap_p2p: bool = True"),(0,i.kt)(l.aE,{mdxType:"Parameters"},"- **stage_manager** (PipelineStageManager) -- If using pipeline parallelism, it's necessary to specify a pipeline stage manager for inter-process communication in pipeline parallelism. Defaults to None, which means not using pipeline parallelism.\n- **schedule** (List[ScheduledNode]) -- Schedule for ZeroBubbleVPipe.\n- **num_model_chunks** (int)  -- The number of model chunk in a device.\n- **num_microbatch** (Optional[int]) -- The number of microbatch.\n- **microbatch_size** (Optional[int]) -- The size per microbatch.\n- **enable_metadata_cache** (bool) -- whether to enable metadata cache to acclerate communication.\n- **overlap_p2p** (bool) -- whether to use overlap_p2p.")),(0,i.kt)("div",null,(0,i.kt)(l.iz,{name:"Description",mdxType:"Divider"}),(0,i.kt)("p",null,"ZeroBubbleVPipeScheduler")),(0,i.kt)(l.Cl,{mdxType:"DocStringContainer"},(0,i.kt)("div",null,(0,i.kt)(l.Dx,{type:"function",name:"backward_b_step",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/pipeline/schedule/zero_bubble_pp.py#L516",mdxType:"Title"}),(0,i.kt)(l.Pc,{mdxType:"Signature"},"model_chunk: typing.Union[torch.nn.modules.container.ModuleList, torch.nn.modules.module.Module], model_chunk_id: int, optimizer: OptimizerWrapper, input_obj: typing.Optional[dict], output_obj: typing.Union[dict, torch.Tensor], output_obj_grad: typing.Optional[dict]"),(0,i.kt)(l.aE,{mdxType:"Parameters"},"- **model_chunk** (ModuleList or Module) -- Model Chunk to be run;\n- **model_chunk_id** (int) -- The current model chunk idx;\n- **optimizer** (OptimizerWrapper) -- Optimizer to update the model\n- **input_obj** (Optional[Tuple(dict)]) -- x. (microbatch, input_obj)\n- **output_obj** (Union[dict, torch.Tensor]) -- y.\n- **output_obj_grad** (dict) -- dy."),(0,i.kt)(l.nT,{name:"Optional[dict]",desc:"dx.",mdxType:"Returns"})),(0,i.kt)("div",null,(0,i.kt)(l.iz,{name:"Description",mdxType:"Divider"}),'Backward dx step of the pipeline; we calculate "dx = w*dy" here;')),(0,i.kt)(l.Cl,{mdxType:"DocStringContainer"},(0,i.kt)("div",null,(0,i.kt)(l.Dx,{type:"function",name:"backward_w_step",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/pipeline/schedule/zero_bubble_pp.py#L591",mdxType:"Title"}),(0,i.kt)(l.Pc,{mdxType:"Signature"},"model_chunk: typing.Union[torch.nn.modules.container.ModuleList, torch.nn.modules.module.Module], model_chunk_id: int, optimizer: OptimizerWrapper, output_obj: typing.Union[dict, torch.Tensor], output_obj_grad: typing.Optional[dict]"),(0,i.kt)(l.aE,{mdxType:"Parameters"},"- **model_chunk** (ModuleList or Module) -- Model Chunk to be run;\n- **model_chunk_id** (int) -- The current model chunk idx;\n- **optimizer** (OptimizerWrapper) -- Optimizer to update the model\n- **output_obj** (Union[dict, torch.Tensor]) -- y.\n- **output_obj_grad** (dict) -- dy."),(0,i.kt)(l.nT,{name:"",desc:"Nothing need to return; we only calculate dw then update w;",mdxType:"Returns"})),(0,i.kt)("div",null,(0,i.kt)(l.iz,{name:"Description",mdxType:"Divider"}),'Backward dw step of the pipeline; we calculate "dw = x*dy" here;')),(0,i.kt)(l.Cl,{mdxType:"DocStringContainer"},(0,i.kt)("div",null,(0,i.kt)(l.Dx,{type:"function",name:"forward_backward_step",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/pipeline/schedule/zero_bubble_pp.py#L938",mdxType:"Title"}),(0,i.kt)(l.Pc,{mdxType:"Signature"},"model_chunk: typing.Union[torch.nn.modules.container.ModuleList, torch.nn.modules.module.Module], data_iter: typing.Iterable, criterion: typing.Callable[..., typing.Any], optimizer: typing.Optional[colossalai.interface.optimizer.OptimizerWrapper] = None, return_loss: bool = False, return_outputs: bool = False"),(0,i.kt)(l.aE,{mdxType:"Parameters"},"- **model_chunk** (ModuleList or Module) -- Model Chunk to be trained. Original interleaved uses a module list whereas shardformer uses entire model + layer specification\n- **data_iter** (Iterable) -- Data iterator.\n- **criterion** (Callable[[Any, Any], Tensor]) -- Criterion to be used. It should take two arguments: model outputs and inputs, and returns loss tensor.\n- **optimizer** (OptimizerWrapper, optional) -- Optimizer to be used. Can be None when only forward is executed. Defaults to None.\n- **return_loss** (bool, optional) -- Whether to return loss. Defaults to False. Whether to return loss.\n- **return_outputs** (bool, optional) -- Whether to return model outputs. Defaults to False. Whether to return model outputs."),(0,i.kt)(l.nT,{name:"dict",desc:"A dict with keys: 'loss' and 'outputs'.",mdxType:"Returns"})),(0,i.kt)("div",null,(0,i.kt)(l.iz,{name:"Description",mdxType:"Divider"}))),(0,i.kt)(l.Cl,{mdxType:"DocStringContainer"},(0,i.kt)("div",null,(0,i.kt)(l.Dx,{type:"function",name:"forward_step",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/pipeline/schedule/zero_bubble_pp.py#L474",mdxType:"Title"}),(0,i.kt)(l.Pc,{mdxType:"Signature"},"model_chunk: typing.Union[torch.nn.modules.container.ModuleList, torch.nn.modules.module.Module], model_chunk_id: int, micro_batch: typing.Optional[dict], input_obj: typing.Optional[dict], criterion: typing.Callable, accum_loss: typing.Optional[torch.Tensor] = None, outputs: typing.Optional[typing.List[typing.Any]] = None"),(0,i.kt)(l.aE,{mdxType:"Parameters"},"- **model_chunk** (ModuleList or Module) -- Model Chunk to be run;\n- **model_chunk_id** (int) -- The current model chunk idx;\n- **input_obj** (Optional[dict]) -- x;\n- **criterion** (Callable) -- loss function;\n- **accum_loss** (Optional[torch.Tensor], optional) -- Accumulated loss. Defaults to None.\n- **outputs** (Optional[List[Any]], optional) -- List to store the output of the last stage (final output). Defaults to None."),(0,i.kt)(l.nT,{name:"Union[torch.Tensor, dict]",desc:"The intermediate output (dict) of the current stage. If it is the last stage, the output is the loss (Tensor).",mdxType:"Returns"})),(0,i.kt)("div",null,(0,i.kt)(l.iz,{name:"Description",mdxType:"Divider"}),"Forward one step of the pipeline")),(0,i.kt)(l.Cl,{mdxType:"DocStringContainer"},(0,i.kt)("div",null,(0,i.kt)(l.Dx,{type:"function",name:"get_model_chunk_id",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/pipeline/schedule/zero_bubble_pp.py#L219",mdxType:"Title"}),(0,i.kt)(l.Pc,{mdxType:"Signature"},"microbatch_id: int, is_forward: bool"),(0,i.kt)(l.aE,{mdxType:"Parameters"},"- **microbatch_id** (int) -- the current microbatch idx\n- **forward** (bool) -- if is the forward process"),(0,i.kt)(l.nT,{name:"int",desc:"The model chunk idx of the input microbatch_id",mdxType:"Returns"})),(0,i.kt)("div",null,(0,i.kt)(l.iz,{name:"Description",mdxType:"Divider"}),"Helper method to get the model chunk ID given the iteration number.")),(0,i.kt)(l.Cl,{mdxType:"DocStringContainer"},(0,i.kt)("div",null,(0,i.kt)(l.Dx,{type:"function",name:"load_batch",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/pipeline/schedule/zero_bubble_pp.py#L170",mdxType:"Title"}),(0,i.kt)(l.Pc,{mdxType:"Signature"},"data_iter: typing.Iterable, device: typing.Optional[torch.device] = None"),(0,i.kt)(l.aE,{mdxType:"Parameters"},"- **data_iter** (Iterable) -- Data iterator.\n- **device** (Optional[torch.device], optional) -- Target device. Defaults to None.")),(0,i.kt)("div",null,(0,i.kt)(l.iz,{name:"Description",mdxType:"Divider"}),"Load a batch from data iterator.")),(0,i.kt)(l.Cl,{mdxType:"DocStringContainer"},(0,i.kt)("div",null,(0,i.kt)(l.Dx,{type:"function",name:"load_micro_batch",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/pipeline/schedule/zero_bubble_pp.py#L205",mdxType:"Title"}),(0,i.kt)(l.Pc,{mdxType:"Signature"},"model_chunk_id: int"),(0,i.kt)(l.aE,{mdxType:"Parameters"},"- **microbatch_id** (int) -- the current model chunk idx."),(0,i.kt)(l.nT,{name:"Any",desc:"Micro batch.",mdxType:"Returns"})),(0,i.kt)("div",null,(0,i.kt)(l.iz,{name:"Description",mdxType:"Divider"}),"Load a micro batch from the current batch.")),(0,i.kt)(l.Cl,{mdxType:"DocStringContainer"},(0,i.kt)("div",null,(0,i.kt)(l.Dx,{type:"function",name:"recv_backward",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/pipeline/schedule/zero_bubble_pp.py#L297",mdxType:"Title"}),(0,i.kt)(l.Pc,{mdxType:"Signature"},"model_chunk_id: int, next_rank: int = None"),(0,i.kt)(l.aE,{mdxType:"Parameters"},"- **model_chunk_id** (int) -- The current model chunk idx.\n- **next_rank** (int, optional) -- The rank of the source of the tensor."),(0,i.kt)(l.nT,{name:"Any",desc:"The input gradient tensor or gradient tensor list.\nAny: The wait handles for the communication.",mdxType:"Returns"})),(0,i.kt)("div",null,(0,i.kt)(l.iz,{name:"Description",mdxType:"Divider"}),"Copy the gradient tensor from the next stage in pipeline as the input gradient of this stage. For ZBV.")),(0,i.kt)(l.Cl,{mdxType:"DocStringContainer"},(0,i.kt)("div",null,(0,i.kt)(l.Dx,{type:"function",name:"recv_forward",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/pipeline/schedule/zero_bubble_pp.py#L239",mdxType:"Title"}),(0,i.kt)(l.Pc,{mdxType:"Signature"},"model_chunk_id: int, prev_rank: int = None"),(0,i.kt)(l.aE,{mdxType:"Parameters"},"- **model_chunk_id** (int) -- The current model chunk idx.\n- **prev_rank** (int, optional) -- The rank of the source of the tensor."),(0,i.kt)(l.nT,{name:"Any",desc:"The input tensor or input tensor list.\nAny: The wait handles for the communication.",mdxType:"Returns"})),(0,i.kt)("div",null,(0,i.kt)(l.iz,{name:"Description",mdxType:"Divider"}),"Copy the forward output from the previous stage in pipeline as the input tensor of this stage. For ZBV.")),(0,i.kt)(l.Cl,{mdxType:"DocStringContainer"},(0,i.kt)("div",null,(0,i.kt)(l.Dx,{type:"function",name:"run_forward_backward",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/pipeline/schedule/zero_bubble_pp.py#L871",mdxType:"Title"}),(0,i.kt)(l.Pc,{mdxType:"Signature"},"model_chunk: typing.Union[torch.nn.modules.container.ModuleList, torch.nn.modules.module.Module], data_iter: typing.Iterable, criterion: typing.Callable[..., typing.Any], optimizer: typing.Optional[colossalai.interface.optimizer.OptimizerWrapper] = None, return_loss: bool = False, return_outputs: bool = False")),(0,i.kt)("div",null,(0,i.kt)(l.iz,{name:"Description",mdxType:"Divider"}),(0,i.kt)("p",null,"Runs Zerobubble schedule, with communication between pipeline stages."))),(0,i.kt)(l.Cl,{mdxType:"DocStringContainer"},(0,i.kt)("div",null,(0,i.kt)(l.Dx,{type:"function",name:"schedule_b",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/pipeline/schedule/zero_bubble_pp.py#L741",mdxType:"Title"}),(0,i.kt)(l.Pc,{mdxType:"Signature"},"scheduled_node, model_chunk: typing.Union[torch.nn.modules.container.ModuleList, torch.nn.modules.module.Module], model_chunk_id: int, optimizer: OptimizerWrapper"),(0,i.kt)(l.aE,{mdxType:"Parameters"},"scheduled_node --\n- **model_chunk** (ModuleList or Module) -- Model Chunk to be run;\n- **model_chunk_id** (int) -- The current model chunk idx;"),(0,i.kt)(l.nT,{name:"",desc:"Nothing.",mdxType:"Returns"})),(0,i.kt)("div",null,(0,i.kt)(l.iz,{name:"Description",mdxType:"Divider"}),"A complete backward b schedule; Include recv bwd --\x3e cal bwd step --\x3e send bwd;")),(0,i.kt)(l.Cl,{mdxType:"DocStringContainer"},(0,i.kt)("div",null,(0,i.kt)(l.Dx,{type:"function",name:"schedule_f",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/pipeline/schedule/zero_bubble_pp.py#L636",mdxType:"Title"}),(0,i.kt)(l.Pc,{mdxType:"Signature"},"scheduled_node, model_chunk: ModuleList, model_chunk_id: int, criterion: typing.Callable, accum_loss: typing.Optional[torch.Tensor] = None, outputs: typing.Optional[typing.List[typing.Any]] = None"),(0,i.kt)(l.aE,{mdxType:"Parameters"},"scheduled_node --\n- **model_chunk** (ModuleList or Module) -- Model Chunk to be run;\n- **model_chunk_id** (int) -- The current model chunk idx;\n- **criterion** (Callable) -- loss function;\n- **accum_loss** (Optional[torch.Tensor], optional) -- Accumulated loss. Defaults to None.\n- **outputs** (Optional[List[Any]], optional) -- List to store the output of the last stage (final output). Defaults to None."),(0,i.kt)(l.nT,{name:"",desc:"Nothing.",mdxType:"Returns"})),(0,i.kt)("div",null,(0,i.kt)(l.iz,{name:"Description",mdxType:"Divider"}),"A complete forward schedule; Include recv fwd --\x3e cal fwd --\x3e send fwd;")),(0,i.kt)(l.Cl,{mdxType:"DocStringContainer"},(0,i.kt)("div",null,(0,i.kt)(l.Dx,{type:"function",name:"schedule_w",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/pipeline/schedule/zero_bubble_pp.py#L809",mdxType:"Title"}),(0,i.kt)(l.Pc,{mdxType:"Signature"},"scheduled_node, model_chunk: typing.Union[torch.nn.modules.container.ModuleList, torch.nn.modules.module.Module], model_chunk_id: int, optimizer: OptimizerWrapper"),(0,i.kt)(l.aE,{mdxType:"Parameters"},"scheduled_node --\n- **model_chunk** (ModuleList or Module) -- Model Chunk to be run;\n- **model_chunk_id** (int) -- The current model chunk idx;"),(0,i.kt)(l.nT,{name:"",desc:"Nothing.",mdxType:"Returns"})),(0,i.kt)("div",null,(0,i.kt)(l.iz,{name:"Description",mdxType:"Divider"}),"A complete backward w schedule; Include get y & dy from buffer --\x3e cal bwd w step(cal dw & update w);")),(0,i.kt)(l.Cl,{mdxType:"DocStringContainer"},(0,i.kt)("div",null,(0,i.kt)(l.Dx,{type:"function",name:"send_backward",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/pipeline/schedule/zero_bubble_pp.py#L415",mdxType:"Title"}),(0,i.kt)(l.Pc,{mdxType:"Signature"},"model_chunk_id: int, prev_rank: int = None"),(0,i.kt)(l.aE,{mdxType:"Parameters"},"- **model_chunk_id** (int) -- The current model chunk idx.\n- **prev_rank** (int, optional) -- The rank of the recipient of the tensor"),(0,i.kt)(l.nT,{name:"Any",desc:"The wait handles for the communication.",mdxType:"Returns"})),(0,i.kt)("div",null,(0,i.kt)(l.iz,{name:"Description",mdxType:"Divider"}),"Sends the gradient tensor to the previous stage in pipeline. For ZBV.")),(0,i.kt)(l.Cl,{mdxType:"DocStringContainer"},(0,i.kt)("div",null,(0,i.kt)(l.Dx,{type:"function",name:"send_forward",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/pipeline/schedule/zero_bubble_pp.py#L356",mdxType:"Title"}),(0,i.kt)(l.Pc,{mdxType:"Signature"},"model_chunk_id: int, next_rank: int = None"),(0,i.kt)(l.aE,{mdxType:"Parameters"},"- **model_chunk_id** (int) -- The current model chunk idx.\n- **next_rank** (int, optional) -- The rank of the recipient of the tensor."),(0,i.kt)(l.nT,{name:"Any",desc:"The wait handles for the communication.",mdxType:"Returns"})),(0,i.kt)("div",null,(0,i.kt)(l.iz,{name:"Description",mdxType:"Divider"}),"Sends the input tensor to the next stage in pipeline. For ZBV."))))}m.isMDXComponent=!0}}]);