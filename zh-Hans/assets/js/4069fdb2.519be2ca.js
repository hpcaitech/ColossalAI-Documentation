"use strict";(self.webpackChunkdemo=self.webpackChunkdemo||[]).push([[3404],{3905:(e,t,n)=>{n.d(t,{Zo:()=>u,kt:()=>_});var a=n(7294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function l(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function i(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},o=Object.keys(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var s=a.createContext({}),p=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):l(l({},t),e)),n},u=function(e){var t=p(e.components);return a.createElement(s.Provider,{value:t},e.children)},d="mdxType",c={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},m=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,o=e.originalType,s=e.parentName,u=i(e,["components","mdxType","originalType","parentName"]),d=p(n),m=r,_=d["".concat(s,".").concat(m)]||d[m]||c[m]||o;return n?a.createElement(_,l(l({ref:t},u),{},{components:n})):a.createElement(_,l({ref:t},u))}));function _(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=n.length,l=new Array(o);l[0]=m;var i={};for(var s in t)hasOwnProperty.call(t,s)&&(i[s]=t[s]);i.originalType=e,i[d]="string"==typeof e?e:r,l[1]=i;for(var p=2;p<o;p++)l[p]=n[p];return a.createElement.apply(null,l)}return a.createElement.apply(null,n)}m.displayName="MDXCreateElement"},9343:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>l,default:()=>c,frontMatter:()=>o,metadata:()=>i,toc:()=>p});var a=n(7462),r=(n(7294),n(3905));const o={},l="\u4f7f\u7528\u6df7\u5408\u5e76\u884c\u8bad\u7ec3 GPT-2",i={unversionedId:"advanced_tutorials/train_gpt_using_hybrid_parallelism",id:"advanced_tutorials/train_gpt_using_hybrid_parallelism",title:"\u4f7f\u7528\u6df7\u5408\u5e76\u884c\u8bad\u7ec3 GPT-2",description:"\u4f5c\u8005: Hongxin Liu, Yongbin Li, Mingyan Jiang",source:"@site/i18n/zh-Hans/docusaurus-plugin-content-docs/current/advanced_tutorials/train_gpt_using_hybrid_parallelism.md",sourceDirName:"advanced_tutorials",slug:"/advanced_tutorials/train_gpt_using_hybrid_parallelism",permalink:"/zh-Hans/docs/advanced_tutorials/train_gpt_using_hybrid_parallelism",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/advanced_tutorials/train_gpt_using_hybrid_parallelism.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"\u4f7f\u7528 Colossal-AI \uff08\u4ece\u6570\u636e\u5e76\u884c\u5230\u5f02\u6784\u5e76\u884c\uff09\u52a0\u901f ViT \u8bad\u7ec3\u8be6\u89e3",permalink:"/zh-Hans/docs/advanced_tutorials/train_vit_with_hybrid_parallelism"},next:{title:"\u8ba4\u8bc6Gemini\uff1aColossalAI\u7684\u5f02\u6784\u5185\u5b58\u7a7a\u95f4\u7ba1\u7406\u5668",permalink:"/zh-Hans/docs/advanced_tutorials/meet_gemini"}},s={},p=[{value:"\u5f15\u8a00",id:"\u5f15\u8a00",level:2},{value:"\u76ee\u5f55",id:"\u76ee\u5f55",level:2},{value:"\u5bfc\u5165\u4f9d\u8d56\u5e93",id:"\u5bfc\u5165\u4f9d\u8d56\u5e93",level:2},{value:"\u5b9a\u4e49plugin",id:"\u5b9a\u4e49plugin",level:3},{value:"\u521b\u5efa\u5206\u5e03\u5f0f\u73af\u5883.",id:"\u521b\u5efa\u5206\u5e03\u5f0f\u73af\u5883",level:2},{value:"\u5b9a\u4e49GPT-2\u6a21\u578b\u7684\u8bad\u7ec3\u7ec4\u4ef6",id:"\u5b9a\u4e49gpt-2\u6a21\u578b\u7684\u8bad\u7ec3\u7ec4\u4ef6",level:2},{value:"\u589e\u5f3aGPT-2\u6a21\u578b",id:"\u589e\u5f3agpt-2\u6a21\u578b",level:2},{value:"\u4f7f\u7528\u6df7\u5408\u5e76\u884c\u8bad\u7ec3 GPT-2",id:"\u4f7f\u7528\u6df7\u5408\u5e76\u884c\u8bad\u7ec3-gpt-2-1",level:2}],u={toc:p},d="wrapper";function c(e){let{components:t,...n}=e;return(0,r.kt)(d,(0,a.Z)({},u,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"\u4f7f\u7528\u6df7\u5408\u5e76\u884c\u8bad\u7ec3-gpt-2"},"\u4f7f\u7528\u6df7\u5408\u5e76\u884c\u8bad\u7ec3 GPT-2"),(0,r.kt)("p",null,"\u4f5c\u8005: Hongxin Liu, Yongbin Li, Mingyan Jiang"),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"\u524d\u7f6e\u6559\u7a0b")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"/zh-Hans/docs/basics/booster_plugins"},"\u5e76\u884c\u63d2\u4ef6")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"/zh-Hans/docs/basics/booster_api"},"booster API"))),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"\u793a\u4f8b\u4ee3\u7801")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://github.com/hpcaitech/ColossalAI/blob/main/examples/language/gpt/hybridparallelism/finetune.py"},"ColossalAI-Examples GPT2"))),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"\u76f8\u5173\u8bba\u6587")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2110.14883"},"Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2104.04473"},"Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM"))),(0,r.kt)("h2",{id:"\u5f15\u8a00"},"\u5f15\u8a00"),(0,r.kt)("p",null,"\u5728\u4e0a\u4e00\u7bc7\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u5982\u4f55\u7528\u6d41\u6c34\u5e76\u884c\u8bad\u7ec3 ViT\u3002\u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u4f60\u5c06\u5b66\u4e60\u4e00\u4e2a\u66f4\u590d\u6742\u7684\u573a\u666f--\u7528\u6df7\u5408\u5e76\u884c\u65b9\u5f0f\u8bad\u7ec3GPT-2\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u7531\u4e8eGPT-2\u8fc7\u5927\uff0c\u5373\u4f7fCPU\u5185\u5b58\u4e5f\u65e0\u6cd5\u5bb9\u7eb3\u5b83\u3002\u56e0\u6b64\uff0c\u8be5\u6a21\u578b\u5fc5\u987b\u88ab\u5206\u5272\u3002"),(0,r.kt)("h2",{id:"\u76ee\u5f55"},"\u76ee\u5f55"),(0,r.kt)("p",null,"\u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u4ecb\u7ecd:"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"\u521d\u59cb\u5316\u6df7\u5408\u5e76\u884c\u63d2\u4ef6"),(0,r.kt)("li",{parentName:"ol"},"\u5b9a\u4e49 GPT-2 \u6a21\u578b\u7684\u8bad\u7ec3\u7ec4\u4ef6"),(0,r.kt)("li",{parentName:"ol"},"\u4f7f\u7528 ",(0,r.kt)("a",{parentName:"li",href:"/zh-Hans/docs/basics/booster_plugins"},"HybridParallelPlugin")," \u589e\u5f3aGPT-2\u6a21\u578b"),(0,r.kt)("li",{parentName:"ol"},"\u4f7f\u7528\u6df7\u5408\u5e76\u884c\u8bad\u7ec3 GPT-2")),(0,r.kt)("h2",{id:"\u5bfc\u5165\u4f9d\u8d56\u5e93"},"\u5bfc\u5165\u4f9d\u8d56\u5e93"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"from typing import Callable, List, Union\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\nfrom torch.optim import Optimizer\nfrom torch.optim.lr_scheduler import _LRScheduler as LRScheduler\nfrom tqdm import tqdm\nfrom transformers import AutoConfig, GPT2ForSequenceClassification, get_linear_schedule_with_warmup\nfrom transformers import AutoTokenizer\n\nimport colossalai\nfrom colossalai.booster import Booster\nfrom colossalai.booster.plugin import GeminiPlugin, HybridParallelPlugin, LowLevelZeroPlugin, TorchDDPPlugin\nfrom colossalai.cluster import DistCoordinator\nfrom colossalai.nn.optimizer import HybridAdam\nfrom colossalai.utils import get_current_device\n")),(0,r.kt)("h3",{id:"\u5b9a\u4e49plugin"},"\u5b9a\u4e49plugin"),(0,r.kt)("p",null,"\u5b9a\u4e49\u4e00\u4e2a",(0,r.kt)("a",{parentName:"p",href:"/zh-Hans/docs/basics/booster_plugins"},(0,r.kt)("inlineCode",{parentName:"a"},"HybridParallelPlugin")),"\u5bf9\u8c61\uff0c\u6307\u5b9a\u6240\u9700\u8981\u4f7f\u7528\u7684\u5e76\u884c\u7b56\u7565\uff0c\u5728\u8be5\u4f8b\u5b50\u4e2d\uff0c\u540c\u65f6\u4f7f\u7528\u4e86\u6d41\u6c34\u7ebf\u5e76\u884c\u548czero1."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'plugin = HybridParallelPlugin(\n    tp_size=1,\n    pp_size=2,\n    num_microbatches=None,\n    microbatch_size=1,\n    enable_all_optimization=True,\n    zero_stage=1,\n    precision="fp16",\n    initial_scale=1,\n)\n')),(0,r.kt)("h2",{id:"\u521b\u5efa\u5206\u5e03\u5f0f\u73af\u5883"},"\u521b\u5efa\u5206\u5e03\u5f0f\u73af\u5883."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"# Launch ColossalAI\ncolossalai.launch_from_torch(config={}, seed=42)\ncoordinator = DistCoordinator()\n")),(0,r.kt)("h2",{id:"\u5b9a\u4e49gpt-2\u6a21\u578b\u7684\u8bad\u7ec3\u7ec4\u4ef6"},"\u5b9a\u4e49GPT-2\u6a21\u578b\u7684\u8bad\u7ec3\u7ec4\u4ef6"),(0,r.kt)("p",null,"\u5728\u4f7f\u7528\u6df7\u5408\u5e76\u884c\u4e4b\u524d\uff0c\u60a8\u9700\u8981\u5b9a\u4e49\u8bad\u7ec3\u6240\u4f7f\u7528\u7684\u7ec4\u4ef6\u3002\n\u5b9a\u4e49\u8d85\u53c2\u6570\u3002"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"NUM_EPOCHS = 3\nBATCH_SIZE = 32\nLEARNING_RATE = 2.4e-5\nWEIGHT_DECAY = 0.01\nWARMUP_FRACTION = 0.1\n")),(0,r.kt)("p",null,"\u83b7\u53d6\u6570\u636e\u96c6\u3002\u60a8\u53ef\u4ee5\u4f7f\u7528",(0,r.kt)("inlineCode",{parentName:"p"},"plugin.prepare_dataloader"),"\u751f\u6210dataloader,\u4e5f\u53ef\u4ee5\u81ea\u5b9a\u4e49\u60a8\u7684dataloader\u3002"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'def tokenize_batch(batch, tokenizer: Optional[AutoTokenizer] = None, max_length: int = 2048):\n    texts = [sample["sentence1"] + sample["sentence2"] for sample in batch]\n    data = tokenizer(texts, return_tensors="pt", padding="max_length", truncation=True, max_length=max_length)\n    data = {k: v.cuda() for k, v in data.items()}\n    data["labels"] = data["input_ids"].clone()\n    return data\n\ntokenizer = AutoTokenizer.from_pretrained("gpt2")\ndataset = datasets.load_dataset("glue", "mrpc")\ntrain_dataloader = plugin.prepare_dataloader(\n    dataset["train"],\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    drop_last=True,\n    collate_fn=partial(tokenize_batch, tokenizer=tokenizer, max_length=512),\n)\n')),(0,r.kt)("p",null,"\u5b9a\u4e49GPT-2\u6a21\u578b\u3002"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'cfg = AutoConfig.from_pretrained("gpt2", num_labels=2)\nmodel = GPT2ForSequenceClassification.from_pretrained("gpt2", config=cfg).cuda()\n')),(0,r.kt)("p",null,"\u51c6\u5907\u4f18\u5316\u5668"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'lr = LEARNING_RATE * coordinator.world_size\nno_decay = ["bias", "LayerNorm.weight"]\noptimizer_grouped_parameters = [\n    {\n        "params": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n        "weight_decay": WEIGHT_DECAY,\n    },\n    {\n        "params": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n        "weight_decay": 0.0,\n    },\n]\n\noptimizer = HybridAdam(optimizer_grouped_parameters, lr=lr, eps=1e-8)\n')),(0,r.kt)("p",null,"\u51c6\u5907 ",(0,r.kt)("inlineCode",{parentName:"p"},"lr_scheduler")," \u548c ",(0,r.kt)("inlineCode",{parentName:"p"},"criterion"),"\uff0c\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u5f53\u6df7\u5408\u5e76\u884c\u4f7f\u7528\u4e86\u7ba1\u9053\u5e76\u884c\u65f6\uff0c\u8fd8\u9700\u5b9a\u4e49",(0,r.kt)("inlineCode",{parentName:"p"},"criterion"),"\u51fd\u6570\u3002\u8fd9\u4e2a\u51fd\u6570\u5e94\u8be5\u4ee5\u6a21\u578b\u524d\u540e\u5411\u7684\u8f93\u5165\u548c\u8f93\u51fa\u4f5c\u4e3a\u53c2\u6570\uff0c\u5e76\u8fd4\u56deloss\u3002"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"# lr scheduler\ntotal_steps = len(train_dataloader) * NUM_EPOCHS\nnum_warmup_steps = int(WARMUP_FRACTION * total_steps)\nlr_scheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=num_warmup_steps,\n    num_training_steps=total_steps,\n)\n\ndef _criterion(outputs, inputs):\n    return outputs.loss\n")),(0,r.kt)("h2",{id:"\u589e\u5f3agpt-2\u6a21\u578b"},"\u589e\u5f3aGPT-2\u6a21\u578b"),(0,r.kt)("p",null,"\u4f7f\u7528 HybridParallelPlugin \u5b9a\u4e49\u4e00\u4e2a booster\uff08\u589e\u5f3a\u5668\uff09\u3002\u6839\u636e\u8bbe\u7f6e\u7684\u63d2\u4ef6\u53c2\u6570\uff0cbooster\u4f1a\u5c06\u4e00\u79cd\u6216\u8005\u591a\u79cd\u5e76\u884c\u7b56\u7565\u6ce8\u5165\u5230\u6a21\u578b\u4e2d\u3002\u8be5\u4f8b\u5b50\u4e2d\u4f7f\u7528\u4e86\u7ba1\u9053\u5e76\u884c\uff0czero1\uff0c\u53ca\u534a\u7cbe\u5ea6\u8bad\u7ec3\u7b49\u4f18\u5316\u3002"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"booster = Booster(plugin=plugin)\n")),(0,r.kt)("p",null,"\u4f7f\u7528\u5b9a\u4e49\u7684 booster \u6765\u589e\u5f3a\u8fd9\u4e9b\u7ec4\u4ef6\u3002"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"model, optimizer, _criterion, _, lr_scheduler = booster.boost(\n    model, optimizer, criterion=_criterion, lr_scheduler=lr_scheduler\n)\n")),(0,r.kt)("h2",{id:"\u4f7f\u7528\u6df7\u5408\u5e76\u884c\u8bad\u7ec3-gpt-2-1"},"\u4f7f\u7528\u6df7\u5408\u5e76\u884c\u8bad\u7ec3 GPT-2"),(0,r.kt)("p",null,"\u5728\u524d\u9762\u7684\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5df2\u7ecf\u89e3\u91ca\u4e86\u5982\u4f55\u4f7f\u7528 Booster \u548c HybridParallelPlugin \u5c06\u5404\u79cd\u5e76\u884c\u7279\u6027\u6ce8\u5165\u5230\u6a21\u578b\u53ca\u5176\u8bad\u7ec3\u7ec4\u4ef6\u4e2d\u3002\u73b0\u5728\u6211\u4eec\u53ef\u4ee5\u5f00\u59cb\u6a21\u578b\u8bad\u7ec3\u3002\n\u5b9a\u4e49\u4e00\u4e2a\u8bad\u7ec3\u51fd\u6570\u3002\u5f53\u4f7f\u7528\u4e86\u7ba1\u9053\u5e76\u884c\u65f6\uff0c\u9700\u8981\u8c03\u7528",(0,r.kt)("inlineCode",{parentName:"p"},"booster.execute_pipeline"),"\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\u7684\u9636\u6bb5\u8c03\u5ea6\u3002"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'def train_epoch(\n    epoch: int,\n    model: nn.Module,\n    optimizer: Optimizer,\n    _criterion: Callable,\n    lr_scheduler: LRScheduler,\n    train_dataloader: DataLoader,\n    booster: Booster,\n    coordinator: DistCoordinator,\n):\n    use_pipeline = isinstance(booster.plugin, HybridParallelPlugin) and booster.plugin.pp_size > 1\n    is_pp_last_stage = use_pipeline and booster.plugin.stage_manager.is_last_stage()\n    print_flag = (not use_pipeline and coordinator.is_master()) or (use_pipeline and is_pp_last_stage)\n    total_step = len(train_dataloader)\n\n    model.train()\n    optimizer.zero_grad()\n    train_dataloader_iter = iter(train_dataloader)\n    with tqdm(\n        range(total_step),\n        desc=f"Epoch [{epoch + 1}/{NUM_EPOCHS}]",\n        disable=not print_flag,\n    ) as pbar:\n        # Forward pass\n        for _ in pbar:\n            if use_pipeline:\n                outputs = booster.execute_pipeline(\n                    train_dataloader_iter, model, _criterion, optimizer, return_loss=True, return_outputs=True\n                )\n                # Backward and optimize\n                if is_pp_last_stage:\n                    loss = outputs["loss"]\n                    pbar.set_postfix({"loss": loss.item()})\n            else:\n                data = next(train_dataloader_iter)\n                data = move_to_cuda(data)\n                outputs = model(**data)\n                loss = _criterion(outputs, None)\n                # Backward\n                booster.backward(loss, optimizer)\n                pbar.set_postfix({"loss": loss.item()})\n\n            optimizer.step()\n            optimizer.zero_grad()\n            lr_scheduler.step()\n\n')),(0,r.kt)("p",null,"\u8bad\u7ec3 GPT-2 \u6a21\u578b\u3002"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"for epoch in range(NUM_EPOCHS):\n    train_epoch(epoch, model, optimizer, _criterion, lr_scheduler, train_dataloader, booster, coordinator)\n")))}c.isMDXComponent=!0}}]);