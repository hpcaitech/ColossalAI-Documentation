"use strict";(self.webpackChunkdemo=self.webpackChunkdemo||[]).push([[5140],{6999:(e,t,a)=>{a.d(t,{Cl:()=>i,Dx:()=>c,Pc:()=>l,aE:()=>s,e_:()=>m,iz:()=>r,nT:()=>p});var n=a(7294),o=a(398);a(814);function i(e){return n.createElement("div",{className:"docstring-container"},e.children)}function l(e){return n.createElement("div",{className:"signature"},"(",e.children,")")}function r(e){return n.createElement("div",{class:"divider"},n.createElement("span",{class:"divider-text"},e.name))}function s(e){return n.createElement("div",null,n.createElement(r,{name:"Parameters"}),n.createElement(o.D,null,e.children))}function p(e){return n.createElement("div",null,n.createElement(r,{name:"Returns"}),n.createElement(o.D,null,`${e.name}: ${e.desc}`))}function c(e){return n.createElement("div",{className:"title-container"},n.createElement("div",{className:"title-module"},n.createElement("h5",null,e.type),"\xa0 ",n.createElement("h3",null,e.name)),n.createElement("div",{className:"title-source"},"<",n.createElement("a",{href:e.source,className:"title-source"},"source"),">"))}function m(e){return n.createElement("div",null,n.createElement(r,{name:"Example"}),n.createElement(o.D,null,e.code))}},9466:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>p,contentTitle:()=>r,default:()=>u,frontMatter:()=>l,metadata:()=>s,toc:()=>c});var n=a(7462),o=(a(7294),a(3905)),i=a(6999);const l={},r="\u81ea\u52a8\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3",s={unversionedId:"features/mixed_precision_training_with_booster",id:"features/mixed_precision_training_with_booster",title:"\u81ea\u52a8\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3",description:"\u4f5c\u8005: Mingyan Jiang",source:"@site/i18n/zh-Hans/docusaurus-plugin-content-docs/current/features/mixed_precision_training_with_booster.md",sourceDirName:"features",slug:"/features/mixed_precision_training_with_booster",permalink:"/zh-Hans/docs/features/mixed_precision_training_with_booster",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/features/mixed_precision_training_with_booster.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Shardformer",permalink:"/zh-Hans/docs/features/shardformer"},next:{title:"\u68af\u5ea6\u7d2f\u79ef",permalink:"/zh-Hans/docs/features/gradient_accumulation_with_booster"}},p={},c=[{value:"\u5f15\u8a00",id:"\u5f15\u8a00",level:2},{value:"\u76ee\u5f55",id:"\u76ee\u5f55",level:2},{value:"AMP \u4ecb\u7ecd",id:"amp-\u4ecb\u7ecd",level:2},{value:"Colossal-AI \u4e2d\u7684 AMP",id:"colossal-ai-\u4e2d\u7684-amp",level:2},{value:"booster \u542f\u52a8\u65b9\u5f0f",id:"booster-\u542f\u52a8\u65b9\u5f0f",level:4},{value:"Torch AMP \u914d\u7f6e",id:"torch-amp-\u914d\u7f6e",level:3},{value:"Apex AMP \u914d\u7f6e",id:"apex-amp-\u914d\u7f6e",level:3},{value:"Naive AMP \u914d\u7f6e",id:"naive-amp-\u914d\u7f6e",level:3},{value:"\u5b9e\u4f8b",id:"\u5b9e\u4f8b",level:2},{value:"\u6b65\u9aa4 1. \u5728 train.py \u5bfc\u5165\u76f8\u5173\u5e93",id:"\u6b65\u9aa4-1-\u5728-trainpy-\u5bfc\u5165\u76f8\u5173\u5e93",level:3},{value:"\u6b65\u9aa4 2. \u521d\u59cb\u5316\u5206\u5e03\u5f0f\u73af\u5883",id:"\u6b65\u9aa4-2-\u521d\u59cb\u5316\u5206\u5e03\u5f0f\u73af\u5883",level:3},{value:"\u6b65\u9aa4 3. \u521b\u5efa\u8bad\u7ec3\u7ec4\u4ef6",id:"\u6b65\u9aa4-3-\u521b\u5efa\u8bad\u7ec3\u7ec4\u4ef6",level:3},{value:"\u6b65\u9aa4 4. \u63d2\u5165 AMP",id:"\u6b65\u9aa4-4-\u63d2\u5165-amp",level:3},{value:"\u6b65\u9aa4 5. \u4f7f\u7528 booster \u8bad\u7ec3",id:"\u6b65\u9aa4-5-\u4f7f\u7528-booster-\u8bad\u7ec3",level:3},{value:"\u6b65\u9aa4 6. \u542f\u52a8\u8bad\u7ec3\u811a\u672c",id:"\u6b65\u9aa4-6-\u542f\u52a8\u8bad\u7ec3\u811a\u672c",level:3}],m={toc:c},d="wrapper";function u(e){let{components:t,...a}=e;return(0,o.kt)(d,(0,n.Z)({},m,a,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"\u81ea\u52a8\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3"},"\u81ea\u52a8\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3"),(0,o.kt)("p",null,"\u4f5c\u8005: ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/jiangmingyan"},"Mingyan Jiang")),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"\u524d\u7f6e\u6559\u7a0b")),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"/zh-Hans/docs/basics/booster_api"},"booster \u4f7f\u7528"))),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"\u76f8\u5173\u8bba\u6587")),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/0808.2794"},"Accelerating Scientific Computations with Mixed Precision Algorithms"))),(0,o.kt)("h2",{id:"\u5f15\u8a00"},"\u5f15\u8a00"),(0,o.kt)("p",null,"AMP \u4ee3\u8868\u81ea\u52a8\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u3002\n\u5728 Colossal-AI \u4e2d, \u6211\u4eec\u7ed3\u5408\u4e86\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u7684\u4e0d\u540c\u5b9e\u73b0:"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},"torch.cuda.amp"),(0,o.kt)("li",{parentName:"ol"},"apex.amp"),(0,o.kt)("li",{parentName:"ol"},"naive amp")),(0,o.kt)("table",null,(0,o.kt)("thead",{parentName:"table"},(0,o.kt)("tr",{parentName:"thead"},(0,o.kt)("th",{parentName:"tr",align:null},"Colossal-AI"),(0,o.kt)("th",{parentName:"tr",align:null},"\u652f\u6301\u5f20\u91cf\u5e76\u884c"),(0,o.kt)("th",{parentName:"tr",align:null},"\u652f\u6301\u6d41\u6c34\u5e76\u884c"),(0,o.kt)("th",{parentName:"tr",align:null},"fp16 \u8303\u56f4"))),(0,o.kt)("tbody",{parentName:"table"},(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"AMP_TYPE.TORCH"),(0,o.kt)("td",{parentName:"tr",align:null},"\u2705"),(0,o.kt)("td",{parentName:"tr",align:null},"\u274c"),(0,o.kt)("td",{parentName:"tr",align:null},"\u5728\u524d\u5411\u548c\u53cd\u5411\u4f20\u64ad\u671f\u95f4\uff0c\u6a21\u578b\u53c2\u6570\u3001\u6fc0\u6d3b\u548c\u68af\u5ea6\u5411\u4e0b\u8f6c\u6362\u81f3 fp16")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"AMP_TYPE.APEX"),(0,o.kt)("td",{parentName:"tr",align:null},"\u274c"),(0,o.kt)("td",{parentName:"tr",align:null},"\u274c"),(0,o.kt)("td",{parentName:"tr",align:null},"\u66f4\u7ec6\u7c92\u5ea6\uff0c\u6211\u4eec\u53ef\u4ee5\u9009\u62e9 opt_level O0, O1, O2, O3")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"AMP_TYPE.NAIVE"),(0,o.kt)("td",{parentName:"tr",align:null},"\u2705"),(0,o.kt)("td",{parentName:"tr",align:null},"\u2705"),(0,o.kt)("td",{parentName:"tr",align:null},"\u6a21\u578b\u53c2\u6570\u3001\u524d\u5411\u548c\u53cd\u5411\u64cd\u4f5c\uff0c\u5168\u90fd\u5411\u4e0b\u8f6c\u6362\u81f3 fp16")))),(0,o.kt)("p",null,"\u524d\u4e24\u4e2a\u4f9d\u8d56\u4e8e PyTorch (1.6 \u53ca\u4ee5\u4e0a) \u548c NVIDIA Apex \u7684\u539f\u59cb\u5b9e\u73b0\u3002\u6700\u540e\u4e00\u79cd\u65b9\u6cd5\u7c7b\u4f3c Apex O2\u3002\u5728\u8fd9\u4e9b\u65b9\u6cd5\u4e2d\uff0cApex-AMP \u4e0e\u5f20\u91cf\u5e76\u884c\u4e0d\u517c\u5bb9\u3002\u8fd9\u662f\u56e0\u4e3a\u5f20\u91cf\u662f\u4ee5\u5f20\u91cf\u5e76\u884c\u7684\u65b9\u5f0f\u5728\u8bbe\u5907\u4e4b\u95f4\u62c6\u5206\u7684\uff0c\u56e0\u6b64\uff0c\u9700\u8981\u5728\u4e0d\u540c\u7684\u8fdb\u7a0b\u4e4b\u95f4\u8fdb\u884c\u901a\u4fe1\uff0c\u4ee5\u68c0\u67e5\u6574\u4e2a\u6a21\u578b\u6743\u91cd\u4e2d\u662f\u5426\u51fa\u73b0 inf \u6216 nan\u3002\u6211\u4eec\u4fee\u6539\u4e86 torch amp \u5b9e\u73b0\uff0c\u4f7f\u5176\u73b0\u5728\u4e0e\u5f20\u91cf\u5e76\u884c\u517c\u5bb9\u3002"),(0,o.kt)("blockquote",null,(0,o.kt)("p",{parentName:"blockquote"},"\u274c\ufe0f fp16 \u4e0e ZeRO \u4e0d\u517c\u5bb9"),(0,o.kt)("p",{parentName:"blockquote"},"\u26a0\ufe0f \u6d41\u6c34\u5e76\u884c\u76ee\u524d\u4ec5\u652f\u6301 naive amp")),(0,o.kt)("p",null,"\u6211\u4eec\u5efa\u8bae\u4f7f\u7528 torch AMP\uff0c\u56e0\u4e3a\u5728\u4e0d\u4f7f\u7528\u6d41\u6c34\u5e76\u884c\u65f6\uff0c\u5b83\u901a\u5e38\u6bd4 NVIDIA AMP \u63d0\u4f9b\u66f4\u597d\u7684\u51c6\u786e\u6027\u3002"),(0,o.kt)("h2",{id:"\u76ee\u5f55"},"\u76ee\u5f55"),(0,o.kt)("p",null,"\u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u4ecb\u7ecd:"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("a",{parentName:"li",href:"#amp-%E4%BB%8B%E7%BB%8D"},"AMP \u4ecb\u7ecd")),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("a",{parentName:"li",href:"#colossal-ai-%E4%B8%AD%E7%9A%84-amp"},"Colossal-AI \u4e2d\u7684 AMP")),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("a",{parentName:"li",href:"#%E5%AE%9E%E4%BE%8B"},"\u7ec3\u4e60\u5b9e\u4f8b"))),(0,o.kt)("h2",{id:"amp-\u4ecb\u7ecd"},"AMP \u4ecb\u7ecd"),(0,o.kt)("p",null,"\u81ea\u52a8\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u662f\u6df7\u5408 FP16 \u548c FP32 \u8bad\u7ec3\u3002"),(0,o.kt)("p",null,"\u534a\u7cbe\u5ea6\u6d6e\u70b9\u683c\u5f0f\uff08FP16\uff09\u5177\u6709\u8f83\u4f4e\u7684\u7b97\u6cd5\u590d\u6742\u5ea6\u548c\u8f83\u9ad8\u7684\u8ba1\u7b97\u6548\u7387\u3002\u6b64\u5916\uff0cFP16 \u4ec5\u9700\u8981 FP32 \u6240\u9700\u7684\u4e00\u534a\u5b58\u50a8\u7a7a\u95f4\uff0c\u5e76\u8282\u7701\u4e86\u5185\u5b58\u548c\u7f51\u7edc\u5e26\u5bbd\uff0c\u4ece\u800c\u4e3a\u5927 batch size \u548c\u5927\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u591a\u5185\u5b58\u3002"),(0,o.kt)("p",null,"\u7136\u800c\uff0c\u8fd8\u6709\u5176\u4ed6\u64cd\u4f5c\uff0c\u5982\u7f29\u51cf\uff0c\u9700\u8981 FP32 \u7684\u52a8\u6001\u8303\u56f4\uff0c\u4ee5\u907f\u514d\u6570\u503c\u6ea2\u51fa/\u4e0b\u6ea2\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u5f15\u5165\u81ea\u52a8\u6df7\u5408\u7cbe\u5ea6\uff0c\u5c1d\u8bd5\u5c06\u6bcf\u4e2a\u64cd\u4f5c\u4e0e\u5176\u76f8\u5e94\u7684\u6570\u636e\u7c7b\u578b\u76f8\u5339\u914d\uff0c\u8fd9\u53ef\u4ee5\u51cf\u5c11\u5185\u5b58\u5360\u7528\u5e76\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u3002"),(0,o.kt)("figure",{style:{textAlign:"center"}},(0,o.kt)("img",{src:"https://s2.loli.net/2022/01/28/URzLJ3MPeDQbtck.png"}),(0,o.kt)("figcaption",null,"AMP \u793a\u610f\u56fe (\u56fe\u7247\u6765\u81ea ",(0,o.kt)("a",{href:"https://arxiv.org/abs/2108.05818"},"PatrickStar \u8bba\u6587"),")")),(0,o.kt)("h2",{id:"colossal-ai-\u4e2d\u7684-amp"},"Colossal-AI \u4e2d\u7684 AMP"),(0,o.kt)("p",null,"\u6211\u4eec\u652f\u6301\u4e09\u79cd AMP \u8bad\u7ec3\u65b9\u6cd5\uff0c\u5e76\u5141\u8bb8\u7528\u6237\u5728\u6ca1\u6709\u6539\u53d8\u4ee3\u7801\u7684\u60c5\u51b5\u4e0b\u4f7f\u7528 AMP \u8fdb\u884c\u8bad\u7ec3\u3002booster \u652f\u6301 amp \u7279\u6027\u6ce8\u5165\uff0c\u5982\u679c\u60a8\u8981\u4f7f\u7528\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\uff0c\u5219\u5728\u521b\u5efa booster \u5b9e\u4f8b\u65f6\u6307\u5b9a",(0,o.kt)("inlineCode",{parentName:"p"},"mixed_precision"),"\u53c2\u6570;\u540e\u7eed\u5c06\u4f1a\u62d3\u5c55",(0,o.kt)("inlineCode",{parentName:"p"},"bf16"),",",(0,o.kt)("inlineCode",{parentName:"p"},"pf8"),"\u7684\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3."),(0,o.kt)("h4",{id:"booster-\u542f\u52a8\u65b9\u5f0f"},"booster \u542f\u52a8\u65b9\u5f0f"),(0,o.kt)("p",null,"\u60a8\u53ef\u4ee5\u5728\u521b\u5efa booster \u5b9e\u4f8b\u65f6\uff0c\u6307\u5b9a",(0,o.kt)("inlineCode",{parentName:"p"},'mixed_precision="fp16"'),"\u5373\u4f7f\u7528 torch amp\u3002"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"\"\"\"\n    \u521d\u59cb\u5316\u6620\u5c04\u5173\u7cfb\u5982\u4e0b\uff1a\n    'fp16': torch amp\n    'fp16_apex': apex amp,\n    'bf16': bf16,\n    'fp8': fp8,\n    'fp16_naive': naive amp\n\"\"\"\nfrom colossalai import Booster\nbooster = Booster(mixed_precision='fp16',...)\n")),(0,o.kt)("p",null,"\u6216\u8005\u60a8\u53ef\u4ee5\u81ea\u5b9a\u4e49\u4e00\u4e2a",(0,o.kt)("inlineCode",{parentName:"p"},"FP16TorchMixedPrecision"),"\u5bf9\u8c61\uff0c\u5982"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from colossalai.mixed_precision import FP16TorchMixedPrecision\nmixed_precision = FP16TorchMixedPrecision(\n    init_scale=2.**16,\n    growth_factor=2.0,\n    backoff_factor=0.5,\n    growth_interval=2000)\nbooster = Booster(mixed_precision=mixed_precision,...)\n")),(0,o.kt)("p",null,"\u5176\u4ed6\u7c7b\u578b\u7684 amp \u4f7f\u7528\u65b9\u5f0f\u4e5f\u662f\u4e00\u6837\u7684\u3002"),(0,o.kt)("h3",{id:"torch-amp-\u914d\u7f6e"},"Torch AMP \u914d\u7f6e"),(0,o.kt)(i.Cl,{mdxType:"DocStringContainer"},(0,o.kt)("div",null,(0,o.kt)(i.Dx,{type:"class",name:"colossalai.booster.mixed_precision.FP16TorchMixedPrecision",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/booster/mixed_precision/fp16_torch.py#L96",mdxType:"Title"}),(0,o.kt)(i.Pc,{mdxType:"Signature"},"init_scale: float = 65536.0, growth_factor: float = 2.0, backoff_factor: float = 0.5, growth_interval: int = 2000"),(0,o.kt)(i.aE,{mdxType:"Parameters"},"- **init_scale** (float) -- Initial scale factor. Default: 2**16.\n- **growth_factor** (float) -- Factor by which the scale is multiplied during\n  [`torch.cuda.amp.GradScaler.step`] if gradients were found to be finite\n  this iteration. Default: 2.0.\n- **backoff_factor** (float) -- Factor by which the scale is multiplied during\n  [`torch.cuda.amp.GradScaler.step`] if gradients were found to be infinite\n  this iteration. Default: 0.5.\n- **growth_interval** (int) -- Number of iterations between [`torch.cuda.amp.GradScaler.step`]\n  calls that may cause the scale to increase. Default: 2000.")),(0,o.kt)("div",null,(0,o.kt)(i.iz,{name:"Description",mdxType:"Divider"}),(0,o.kt)("p",null,"Precision for mixed precision training in FP16 using PyTorch AMP."))),(0,o.kt)("h3",{id:"apex-amp-\u914d\u7f6e"},"Apex AMP \u914d\u7f6e"),(0,o.kt)("p",null,"\u5bf9\u4e8e\u8fd9\u79cd\u6a21\u5f0f\uff0c\u6211\u4eec\u4f9d\u9760 Apex \u5b9e\u73b0\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u3002\u6211\u4eec\u652f\u6301\u8fd9\u4e2a\u63d2\u4ef6\uff0c\u56e0\u4e3a\u5b83\u5141\u8bb8\u5bf9\u6df7\u5408\u7cbe\u5ea6\u7684\u7c92\u5ea6\u8fdb\u884c\u66f4\u7cbe\u7ec6\u7684\u63a7\u5236\u3002\n\u4f8b\u5982, O2 \u6c34\u5e73 (\u4f18\u5316\u5668\u6c34\u5e73 2) \u5c06\u4fdd\u6301 batch normalization \u4e3a FP32\u3002"),(0,o.kt)("p",null,"\u5982\u679c\u4f60\u60f3\u4e86\u89e3\u66f4\u591a\u7ec6\u8282\uff0c\u8bf7\u53c2\u8003 ",(0,o.kt)("a",{parentName:"p",href:"https://nvidia.github.io/apex/"},"Apex Documentation"),"\u3002"),(0,o.kt)(i.Cl,{mdxType:"DocStringContainer"},(0,o.kt)("div",null,(0,o.kt)(i.Dx,{type:"class",name:"colossalai.booster.mixed_precision.FP16ApexMixedPrecision",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/booster/mixed_precision/fp16_apex.py#L8",mdxType:"Title"}),(0,o.kt)(i.Pc,{mdxType:"Signature"},"opt_level: typing.Optional[str] = 'O1', cast_model_type: dtype = None, patch_torch_functions: bool = None, keep_batchnorm_fp32: typing.Union[bool, str] = None, master_weights: bool = None, loss_scale: typing.Union[float, str] = None, cast_model_outputs: typing.Any = None, num_losses: typing.Optional[int] = 1, verbosity: int = 1, min_loss_scale: float = None, max_loss_scale: float = 16777216.0"),(0,o.kt)(i.aE,{mdxType:"Parameters"},'- **opt_level(str,** optional, default="O1" ) -- Pure or mixed precision optimization level. Accepted values are \u201cO0\u201d, \u201cO1\u201d, \u201cO2\u201d, and \u201cO3\u201d, explained in detail above Apex AMP Documentation.\n- **cast_model_type** (torch.dtype, optional, default=None) -- Casts your model\u2019s parameters and buffers to the desired type.\n- **patch_torch_functions** (bool, optional, default=None) -- Patch all Torch functions and Tensor methods to perform Tensor Core-friendly ops like GEMMs and convolutions in FP16, and any ops that benefit from FP32 precision in FP32.\n- **keep_batchnorm_fp32** (bool or str, optional, default=None) -- To enhance precision and enable cudnn batchnorm (which improves performance), it\u2019s often beneficial to keep batchnorm weights in FP32 even if the rest of the model is FP16.\n- **master_weights** (bool, optional, default=None) -- Maintain FP32 master weights to accompany any FP16 model weights. FP32 master weights are stepped by the optimizer to enhance precision and capture small gradients.\n- **loss_scale** (float or str, optional, default=None) -- If loss_scale is a float value, use this value as the static (fixed) loss scale. If loss_scale is the string "dynamic", adaptively adjust the loss scale over time. Dynamic loss scale adjustments are performed by Amp automatically.\n- **cast_model_outputs** (torch.dpython --type, optional, default=None): Option to ensure that the outputs of your model(s) are always cast to a particular type regardless of opt_level.\n- **num_losses(int,** optional, default=1) -- Option to tell AMP in advance how many losses/backward passes you plan to use. When used in conjunction with the loss_id argument to `amp.scale_loss`, enables Amp to use a different loss scale per loss/backward pass, which can improve stability. If num_losses is left to 1, Amp will still support multiple losses/backward passes, but use a single global loss scale for all of them.\n- **verbosity(int,** default=1) -- Set to 0 to suppress Amp-related output.\n- **min_loss_scale(float,** default=None) -- Sets a floor for the loss scale values that can be chosen by dynamic loss scaling. The default value of None means that no floor is imposed. If dynamic loss scaling is not used, min_loss_scale is ignored.\n- **max_loss_scale(float,** default=2.**24 ) -- Sets a ceiling for the loss scale values that can be chosen by dynamic loss scaling. If dynamic loss scaling is not used, max_loss_scale is ignored.')),(0,o.kt)("div",null,(0,o.kt)(i.iz,{name:"Description",mdxType:"Divider"}),(0,o.kt)("p",null,"Precision for mixed precision training in FP16 using apex AMP."))),(0,o.kt)("h3",{id:"naive-amp-\u914d\u7f6e"},"Naive AMP \u914d\u7f6e"),(0,o.kt)("p",null,"\u5728 Naive AMP \u6a21\u5f0f\u4e2d, \u6211\u4eec\u5b9e\u73b0\u4e86\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0e\u590d\u6742\u5f20\u91cf\u548c\u6d41\u6c34\u5e76\u884c\u7684\u517c\u5bb9\u6027\u3002\u8be5 AMP \u6a21\u5f0f\u5c06\u6240\u6709\u64cd\u4f5c\u8f6c\u4e3a FP16 \u3002\u4e0b\u5217\u4ee3\u7801\u5757\u5c55\u793a\u4e86\u8be5\u6a21\u5f0f\u7684 booster \u542f\u52a8\u65b9\u5f0f\u3002"),(0,o.kt)(i.Cl,{mdxType:"DocStringContainer"},(0,o.kt)("div",null,(0,o.kt)(i.Dx,{type:"class",name:"colossalai.booster.mixed_precision.FP16NaiveMixedPrecision",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/booster/mixed_precision/fp16_naive.py#L4",mdxType:"Title"}),(0,o.kt)(i.Pc,{mdxType:"Signature"},"log_num_zeros_in_grad: bool, initial_scale: int, growth_factor: int, backoff_factor: float, hysteresis: int, max_scale: int, verbose: bool = None"),(0,o.kt)(i.aE,{mdxType:"Parameters"},"log_num_zeros_in_grad(bool) -- return number of zeros in the gradients.\ninitial_scale(int) -- initial scale of gradient scaler.\ngrowth_factor(int) -- the growth rate of loss scale.\nbackoff_factor(float) -- the decrease rate of loss scale.\nhysteresis(int) -- delay shift in dynamic loss scaling.\nmax_scale(int) -- maximum loss scale allowed.\nverbose(bool) -- if set to `True`, will print debug info.")),(0,o.kt)("div",null,(0,o.kt)(i.iz,{name:"Description",mdxType:"Divider"}),(0,o.kt)("p",null,"Precision for mixed precision training in FP16 using naive AMP."))),(0,o.kt)("p",null,"\u5f53\u4f7f\u7528",(0,o.kt)("inlineCode",{parentName:"p"},"colossalai.booster"),"\u65f6, \u9996\u5148\u9700\u8981\u5b9e\u4f8b\u5316\u4e00\u4e2a\u6a21\u578b\u3001\u4e00\u4e2a\u4f18\u5316\u5668\u548c\u4e00\u4e2a\u6807\u51c6\u3002\u5c06\u8f93\u51fa\u6a21\u578b\u8f6c\u6362\u4e3a\u5185\u5b58\u6d88\u8017\u8f83\u5c0f\u7684 AMP \u6a21\u578b\u3002\u5982\u679c\u60a8\u7684\u8f93\u5165\u6a21\u578b\u5df2\u7ecf\u592a\u5927\uff0c\u65e0\u6cd5\u653e\u7f6e\u5728 GPU \u4e2d\uff0c\u8bf7\u4f7f\u7528",(0,o.kt)("inlineCode",{parentName:"p"},"dtype=torch.float16"),"\u5b9e\u4f8b\u5316\u4f60\u7684\u6a21\u578b\u3002\u6216\u8005\u8bf7\u5c1d\u8bd5\u66f4\u5c0f\u7684\u6a21\u578b\uff0c\u6216\u5c1d\u8bd5\u66f4\u591a\u7684\u5e76\u884c\u5316\u8bad\u7ec3\u6280\u672f\uff01"),(0,o.kt)("h2",{id:"\u5b9e\u4f8b"},"\u5b9e\u4f8b"),(0,o.kt)("p",null,"\u4e0b\u9762\u6211\u4eec\u5c06\u5c55\u73b0\u5982\u4f55\u5728 Colossal-AI \u4f7f\u7528 AMP\u3002\u5728\u8be5\u4f8b\u7a0b\u4e2d\uff0c\u6211\u4eec\u4f7f\u7528 Torch AMP."),(0,o.kt)("h3",{id:"\u6b65\u9aa4-1-\u5728-trainpy-\u5bfc\u5165\u76f8\u5173\u5e93"},"\u6b65\u9aa4 1. \u5728 train.py \u5bfc\u5165\u76f8\u5173\u5e93"),(0,o.kt)("p",null,"\u521b\u5efa",(0,o.kt)("inlineCode",{parentName:"p"},"train.py"),"\u5e76\u5bfc\u5165\u5fc5\u8981\u4f9d\u8d56. \u8bf7\u8bb0\u5f97\u901a\u8fc7\u547d\u4ee4",(0,o.kt)("inlineCode",{parentName:"p"},"pip install timm scipy"),"\u5b89\u88c5",(0,o.kt)("inlineCode",{parentName:"p"},"scipy"),"\u548c",(0,o.kt)("inlineCode",{parentName:"p"},"timm"),"\u3002"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"import os\nfrom pathlib import Path\n\nimport torch\nfrom timm.models import vit_base_patch16_224\nfrom titans.utils import barrier_context\nfrom torchvision import datasets, transforms\n\nimport colossalai\nfrom colossalai.booster import Booster\nfrom colossalai.booster.plugin import TorchDDPPlugin\nfrom colossalai.logging import get_dist_logger\nfrom colossalai.nn.lr_scheduler import LinearWarmupLR\n")),(0,o.kt)("h3",{id:"\u6b65\u9aa4-2-\u521d\u59cb\u5316\u5206\u5e03\u5f0f\u73af\u5883"},"\u6b65\u9aa4 2. \u521d\u59cb\u5316\u5206\u5e03\u5f0f\u73af\u5883"),(0,o.kt)("p",null,"\u6211\u4eec\u9700\u8981\u521d\u59cb\u5316\u5206\u5e03\u5f0f\u73af\u5883\u3002\u4e3a\u4e86\u5feb\u901f\u6f14\u793a\uff0c\u6211\u4eec\u4f7f\u7528",(0,o.kt)("inlineCode",{parentName:"p"},"launch_from_torch"),"\u3002\u4f60\u53ef\u4ee5\u53c2\u8003 ",(0,o.kt)("a",{parentName:"p",href:"/zh-Hans/docs/basics/launch_colossalai"},"Launch Colossal-AI"),"\n\u4f7f\u7528\u5176\u4ed6\u521d\u59cb\u5316\u65b9\u6cd5\u3002"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"# \u521d\u59cb\u5316\u5206\u5e03\u5f0f\u8bbe\u7f6e\nparser = colossalai.get_default_parser()\nargs = parser.parse_args()\n\n# launch from torch\ncolossalai.launch_from_torch()\n\n")),(0,o.kt)("h3",{id:"\u6b65\u9aa4-3-\u521b\u5efa\u8bad\u7ec3\u7ec4\u4ef6"},"\u6b65\u9aa4 3. \u521b\u5efa\u8bad\u7ec3\u7ec4\u4ef6"),(0,o.kt)("p",null,"\u6784\u5efa\u4f60\u7684\u6a21\u578b\u3001\u4f18\u5316\u5668\u3001\u635f\u5931\u51fd\u6570\u3001\u5b66\u4e60\u7387\u8c03\u6574\u5668\u548c\u6570\u636e\u52a0\u8f7d\u5668\u3002\u6ce8\u610f\u6570\u636e\u96c6\u7684\u8def\u5f84\u4ece\u73af\u5883\u53d8\u91cf",(0,o.kt)("inlineCode",{parentName:"p"},"DATA"),"\u83b7\u5f97\u3002\u4f60\u53ef\u4ee5\u901a\u8fc7 ",(0,o.kt)("inlineCode",{parentName:"p"},"export DATA=/path/to/data")," \u6216 ",(0,o.kt)("inlineCode",{parentName:"p"},"Path(os.environ['DATA'])"),"\n\u5728\u4f60\u7684\u673a\u5668\u4e0a\u8bbe\u7f6e\u8def\u5f84\u3002\u6570\u636e\u5c06\u4f1a\u88ab\u81ea\u52a8\u4e0b\u8f7d\u5230\u8be5\u8def\u5f84\u3002"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"# define the constants\nNUM_EPOCHS = 2\nBATCH_SIZE = 128\n# build model\nmodel = vit_base_patch16_224(drop_rate=0.1)\n\n# build dataloader\ntrain_dataset = datasets.Caltech101(\n    root=Path(os.environ['DATA']),\n    download=True,\n    transform=transforms.Compose([\n        transforms.Resize(256),\n        transforms.RandomResizedCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        Gray2RGB(),\n        transforms.Normalize([0.5, 0.5, 0.5],\n                                [0.5, 0.5, 0.5])\n    ]))\n\n# build optimizer\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-2, weight_decay=0.1)\n\n# build loss\ncriterion = torch.nn.CrossEntropyLoss()\n\n# lr_scheduler\nlr_scheduler = LinearWarmupLR(optimizer, warmup_steps=50, total_steps=NUM_EPOCHS)\n")),(0,o.kt)("h3",{id:"\u6b65\u9aa4-4-\u63d2\u5165-amp"},"\u6b65\u9aa4 4. \u63d2\u5165 AMP"),(0,o.kt)("p",null,"\u521b\u5efa\u4e00\u4e2a MixedPrecision \u5bf9\u8c61\uff08\u5982\u679c\u9700\u8981\uff09\u53ca torchDDPPlugin \u5bf9\u8c61\uff0c\u8c03\u7528 ",(0,o.kt)("inlineCode",{parentName:"p"},"colossalai.boost")," \u5c06\u6240\u6709\u8bad\u7ec3\u7ec4\u4ef6\u8f6c\u4e3a\u4e3a FP16 \u6a21\u5f0f."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"plugin = TorchDDPPlugin()\ntrain_dataloader = plugin.prepare_dataloader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\nbooster = Booster(mixed_precision='fp16', plugin=plugin)\n\n# if you need to customize the config, do like this\n# >>> from colossalai.mixed_precision import FP16TorchMixedPrecision\n# >>> mixed_precision = FP16TorchMixedPrecision(\n# >>>     init_scale=2.**16,\n# >>>     growth_factor=2.0,\n# >>>     backoff_factor=0.5,\n# >>>     growth_interval=2000)\n# >>> plugin = TorchDDPPlugin()\n# >>> booster = Booster(mixed_precision=mixed_precision, plugin=plugin)\n\n# boost model, optimizer, criterion, dataloader, lr_scheduler\nmodel, optimizer, criterion, dataloader, lr_scheduler = booster.boost(model, optimizer, criterion, dataloader, lr_scheduler)\n")),(0,o.kt)("h3",{id:"\u6b65\u9aa4-5-\u4f7f\u7528-booster-\u8bad\u7ec3"},"\u6b65\u9aa4 5. \u4f7f\u7528 booster \u8bad\u7ec3"),(0,o.kt)("p",null,"\u4f7f\u7528 booster \u6784\u5efa\u4e00\u4e2a\u666e\u901a\u7684\u8bad\u7ec3\u5faa\u73af\u3002"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"model.train()\nfor epoch in range(NUM_EPOCHS):\n    for img, label in enumerate(train_dataloader):\n        img = img.cuda()\n        label = label.cuda()\n        optimizer.zero_grad()\n        output = model(img)\n        loss = criterion(output, label)\n        booster.backward(loss, optimizer)\n        optimizer.step()\n    lr_scheduler.step()\n")),(0,o.kt)("h3",{id:"\u6b65\u9aa4-6-\u542f\u52a8\u8bad\u7ec3\u811a\u672c"},"\u6b65\u9aa4 6. \u542f\u52a8\u8bad\u7ec3\u811a\u672c"),(0,o.kt)("p",null,"\u4f7f\u7528\u4e0b\u5217\u547d\u4ee4\u542f\u52a8\u8bad\u7ec3\u811a\u672c\uff0c\u4f60\u53ef\u4ee5\u6539\u53d8 ",(0,o.kt)("inlineCode",{parentName:"p"},"--nproc_per_node")," \u4ee5\u4f7f\u7528\u4e0d\u540c\u6570\u91cf\u7684 GPU\u3002"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"colossalai run --nproc_per_node 1 train.py\n")))}u.isMDXComponent=!0}}]);