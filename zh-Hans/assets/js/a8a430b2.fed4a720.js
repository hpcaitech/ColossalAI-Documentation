"use strict";(self.webpackChunkdemo=self.webpackChunkdemo||[]).push([[5088],{6999:(e,t,a)=>{a.d(t,{Cl:()=>n,Dx:()=>d,Pc:()=>o,aE:()=>s,e_:()=>u,iz:()=>l,nT:()=>p});var i=a(7294),r=a(398);a(814);function n(e){return i.createElement("div",{className:"docstring-container"},e.children)}function o(e){return i.createElement("div",{className:"signature"},"(",e.children,")")}function l(e){return i.createElement("div",{class:"divider"},i.createElement("span",{class:"divider-text"},e.name))}function s(e){return i.createElement("div",null,i.createElement(l,{name:"Parameters"}),i.createElement(r.D,null,e.children))}function p(e){return i.createElement("div",null,i.createElement(l,{name:"Returns"}),i.createElement(r.D,null,`${e.name}: ${e.desc}`))}function d(e){return i.createElement("div",{className:"title-container"},i.createElement("div",{className:"title-module"},i.createElement("h5",null,e.type),"\xa0 ",i.createElement("h3",null,e.name)),i.createElement("div",{className:"title-source"},"<",i.createElement("a",{href:e.source,className:"title-source"},"source"),">"))}function u(e){return i.createElement("div",null,i.createElement(l,{name:"Example"}),i.createElement(r.D,null,e.code))}},8953:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>p,contentTitle:()=>l,default:()=>m,frontMatter:()=>o,metadata:()=>s,toc:()=>d});var i=a(7462),r=(a(7294),a(3905)),n=a(6999);const o={},l="\u5206\u5e03\u5f0f\u4f18\u5316\u5668",s={unversionedId:"features/distributed_optimizers",id:"features/distributed_optimizers",title:"\u5206\u5e03\u5f0f\u4f18\u5316\u5668",description:"Author: Wenxuan Tan, Junwen Duan, Renjie Mao",source:"@site/i18n/zh-Hans/docusaurus-plugin-content-docs/current/features/distributed_optimizers.md",sourceDirName:"features",slug:"/features/distributed_optimizers",permalink:"/zh-Hans/docs/features/distributed_optimizers",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/features/distributed_optimizers.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"\u61d2\u60f0\u521d\u59cb\u5316",permalink:"/zh-Hans/docs/features/lazy_init"},next:{title:"\u96c6\u7fa4\u5b9e\u7528\u7a0b\u5e8f",permalink:"/zh-Hans/docs/features/cluster_utils"}},p={},d=[{value:"\u4ecb\u7ecd",id:"\u4ecb\u7ecd",level:2},{value:"\u4f18\u5316\u5668",id:"\u4f18\u5316\u5668",level:2},{value:"\u4f7f\u7528",id:"\u4f7f\u7528",level:2},{value:"step 1. \u5bfc\u5305",id:"step-1-\u5bfc\u5305",level:3},{value:"step 2. \u521d\u59cb\u5316\u5206\u5e03\u5f0f",id:"step-2-\u521d\u59cb\u5316\u5206\u5e03\u5f0f",level:3},{value:"step 3. \u521d\u59cb\u5316\u6a21\u578b\u548c\u4f18\u5316\u5668",id:"step-3-\u521d\u59cb\u5316\u6a21\u578b\u548c\u4f18\u5316\u5668",level:3},{value:"step 4.\u521d\u59cb\u5316booster\u548cplugin",id:"step-4\u521d\u59cb\u5316booster\u548cplugin",level:3},{value:"step 5.\u8bad\u7ec3",id:"step-5\u8bad\u7ec3",level:3},{value:"GaLore\u7684\u7279\u6b8a\u521d\u671f",id:"galore\u7684\u7279\u6b8a\u521d\u671f",level:3},{value:"\u517c\u5bb9\u6027",id:"\u517c\u5bb9\u6027",level:2},{value:"API \u53c2\u8003",id:"api-\u53c2\u8003",level:2}],u={toc:d},c="wrapper";function m(e){let{components:t,...a}=e;return(0,r.kt)(c,(0,i.Z)({},u,a,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"\u5206\u5e03\u5f0f\u4f18\u5316\u5668"},"\u5206\u5e03\u5f0f\u4f18\u5316\u5668"),(0,r.kt)("p",null,"Author: Wenxuan Tan, Junwen Duan, Renjie Mao"),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"\u76f8\u5173\u8bba\u6587")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/1804.04235"},"Adafactor: Adaptive Learning Rates with Sublinear Memory Cost")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2307.02047"},"CAME: Confidence-guided Adaptive Memory Efficient Optimization")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2403.03507"},"GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://arxiv.org/pdf/1904.00962"},"Large Batch Optimization for Deep Learning: Training BERT in 76 minutes"))),(0,r.kt)("h2",{id:"\u4ecb\u7ecd"},"\u4ecb\u7ecd"),(0,r.kt)("p",null,"\u9664\u4e86\u5e7f\u6cdb\u91c7\u7528\u7684Adam\u548cSGD\u5916\uff0c\u8bb8\u591a\u73b0\u4ee3\u4f18\u5316\u5668\u9700\u8981\u9010\u5c42\u7edf\u8ba1\u4fe1\u606f\u4ee5\u6709\u6548\u66f4\u65b0\u53c2\u6570\uff0c\u56e0\u6b64\u65e0\u6cd5\u76f4\u63a5\u5e94\u7528\u4e8e\u6a21\u578b\u5c42\u5728\u591a\u4e2a\u8bbe\u5907\u4e0a\u5206\u7247\u7684\u5e76\u884c\u8bbe\u7f6e\u3002\u6211\u4eec\u4ee5\u63d0\u4f9b\u4e86\u4f18\u5316\u7684\u5206\u5e03\u5f0f\u5b9e\u73b0\uff0c\uff0c\u5e76\u4e14\u901a\u8fc7plugin\u4e0eTensor Parallel\u3001DDP\u548cZeRO\u65e0\u7f1d\u96c6\u6210\u3002"),(0,r.kt)("h2",{id:"\u4f18\u5316\u5668"},"\u4f18\u5316\u5668"),(0,r.kt)("p",null,"Adafactor \u662f\u4e00\u79cd\u9996\u6b21\u91c7\u7528\u975e\u8d1f\u77e9\u9635\u5206\u89e3\uff08NMF\uff09\u7684 Adam \u53d8\u4f53\uff0c\u7528\u4e8e\u51cf\u5c11\u5185\u5b58\u5360\u7528\u3002CAME \u901a\u8fc7\u5f15\u5165\u4e00\u4e2a\u7f6e\u4fe1\u5ea6\u77e9\u9635\u6765\u6539\u8fdb NMF \u7684\u6548\u679c\u3002GaLore \u901a\u8fc7\u5c06\u68af\u5ea6\u6295\u5f71\u5230\u4f4e\u79e9\u7a7a\u95f4\uff0c\u5e76\u4f7f\u7528 8 \u4f4d\u5757\u72b6\u91cf\u5316\u8fdb\u4e00\u6b65\u51cf\u5c11\u5185\u5b58\u5360\u7528\u3002Lamb \u5141\u8bb8\u4f7f\u7528\u5de8\u5927\u7684\u6279\u91cf\u5927\u5c0f\u800c\u4e0d\u5931\u51c6\u786e\u6027\uff0c\u901a\u8fc7\u6309\u5176 Lipschitz \u5e38\u6570\u7684\u5012\u6570\u754c\u5b9a\u7684\u9010\u5c42\u81ea\u9002\u5e94\u66f4\u65b0\u5b9e\u73b0"),(0,r.kt)("h2",{id:"\u4f7f\u7528"},"\u4f7f\u7528"),(0,r.kt)("p",null,"\u73b0\u5728\u6211\u4eec\u5c55\u793a\u5982\u4f55\u4f7f\u7528\u5206\u5e03\u5f0f Adafactor \u4e0e booster API \u7ed3\u5408 Tensor Parallel \u548c ZeRO 2\u3002\u5373\u4f7f\u60a8\u4e0d\u4f7f\u7528distributed optimizer\uff0cplugin \u4e5f\u4f1a\u81ea\u52a8\u5c06optimizer\u8f6c\u6362\u4e3a\u5206\u5e03\u5f0f\u7248\u672c\u4ee5\u65b9\u4fbf\u4f7f\u7528\u3002"),(0,r.kt)("h3",{id:"step-1-\u5bfc\u5305"},"step 1. \u5bfc\u5305"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"from transformers import LlamaModel, LlamaConfig\nfrom colossalai.nn.optimizer.distributed_adafactor import DistributedAdaFactor\nfrom colossalai.booster import Booster\nfrom colossalai.booster.plugin import HybridParallelPlugin\nimport colossalai\nimport torch\n")),(0,r.kt)("h3",{id:"step-2-\u521d\u59cb\u5316\u5206\u5e03\u5f0f"},"step 2. \u521d\u59cb\u5316\u5206\u5e03\u5f0f"),(0,r.kt)("p",null,"\u6211\u4eec\u9700\u8981\u5148\u521d\u59cb\u5316\u5206\u5e03\u5f0f\u73af\u5883. \u4e3a\u4e86\u5c55\u793a, \u6211\u4eec\u4f7f\u7528 ",(0,r.kt)("inlineCode",{parentName:"p"},"colossal run --nproc_per_node 4"),". \u66f4\u591a\u521d\u59cb\u5316\u65b9\u5f0f\u8bf7\u53c2\u8003 ",(0,r.kt)("a",{parentName:"p",href:"/zh-Hans/docs/basics/launch_colossalai"},"Launch Colossal-AI")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"colossalai.launch_from_torch()\n")),(0,r.kt)("h3",{id:"step-3-\u521d\u59cb\u5316\u6a21\u578b\u548c\u4f18\u5316\u5668"},"step 3. \u521d\u59cb\u5316\u6a21\u578b\u548c\u4f18\u5316\u5668"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"configuration = LlamaConfig()\nmodel = LlamaModel(configuration).cuda()\ncriterion = lambda x: x.mean()\ndist_optim = DistributedAdaFactor(model.parameters())\n\n")),(0,r.kt)("h3",{id:"step-4\u521d\u59cb\u5316booster\u548cplugin"},"step 4.\u521d\u59cb\u5316booster\u548cplugin"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"plugin = HybridParallelPlugin(tp_size=2, zero_stage=2, pp_size=1, enable_all_optimization=True)\nbooster = Booster(plugin=plugin)\n# You should also pass in your own dataset.\nmodel, dist_optim, criterion, dataloader, _ = booster.boost(model, dist_optim, criterion)\n\n")),(0,r.kt)("h3",{id:"step-5\u8bad\u7ec3"},"step 5.\u8bad\u7ec3"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'steps = 10\nfor step in range(steps):\n    input_ids = torch.ones(1, 100, device="cuda", dtype=torch.int)\n    attention_mask = input_ids.clone()\n    outputs = model(input_ids.cuda(), attention_mask.cuda())\n    loss = criterion(outputs.last_hidden_state)\n    booster.backward(loss, dist_optim)\n    dist_optim.step()\n    dist_optim.zero_grad()\n')),(0,r.kt)("h3",{id:"galore\u7684\u7279\u6b8a\u521d\u671f"},"GaLore\u7684\u7279\u6b8a\u521d\u671f"),(0,r.kt)("p",null,"\u5bf9\u4e8e GaLore\uff0c\u6211\u4eec\u9700\u8981\u4e3a\u6bcf\u4e2a\u53c2\u6570\u7ec4\u6307\u5b9a\u6295\u5f71rank\uff0c\u4ee5\u53ca\u91cf\u5316\u548c\u5206\u9875\u4f18\u5316\u5668\u53c2\u6570\u3002\u6709\u5173\u91cf\u5316\u7684\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u8003 bitandbytes."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"from colossalai.nn.optimizer.galore import get_galore_param_groups\nfrom colossalai.nn.optimizer import DistGaloreAwamW\noptim = DistGaloreAwamW(\n    get_galore_param_groups(model, decay=1e-2, rank=8),\n    lr=lr,\n    betas=(beta1, beta2),\n    eps=eps,\n    nbits=8,\n    percentile_clipping=100,\n    block_wise=True,\n    min_8bit_size=4096,\n)\n")),(0,r.kt)("h2",{id:"\u517c\u5bb9\u6027"},"\u517c\u5bb9\u6027"),(0,r.kt)("table",null,(0,r.kt)("tr",null,(0,r.kt)("th",{nowrap:"nowrap"},"Optimizer/Plugin"),(0,r.kt)("th",{nowrap:"nowrap",align:"center"},"Hybrid Parallel Plugin"),(0,r.kt)("th",{nowrap:"nowrap",align:"center"},"Low Level Zero Plugin"),(0,r.kt)("th",{nowrap:"nowrap",align:"center"},"Torch DDP Plugin"),(0,r.kt)("th",{nowrap:"nowrap",align:"center"},"Gemini Plugin"),(0,r.kt)("th",{nowrap:"nowrap",align:"center"},"Moe Hybrid Plugin")),(0,r.kt)("tr",null,(0,r.kt)("td",{nowrap:"nowrap",align:"center",title:"Lamb"},"Lamb"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u274c"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u274c")),(0,r.kt)("tr",null,(0,r.kt)("td",{nowrap:"nowrap",align:"center",title:"GaLore"},"GaLore"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u274c"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u274c")),(0,r.kt)("tr",null,(0,r.kt)("td",{nowrap:"nowrap",align:"center",title:"Adafactor"},"Adafactor"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u274c"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u274c")),(0,r.kt)("tr",null,(0,r.kt)("td",{nowrap:"nowrap",align:"center",title:"CAME"},"CAME"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u2714\ufe0f"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u274c"),(0,r.kt)("td",{nowrap:"nowrap",align:"center"},"\u274c")),(0,r.kt)("tr",null,(0,r.kt)("td",{colspan:"39"}))),(0,r.kt)("h2",{id:"api-\u53c2\u8003"},"API \u53c2\u8003"),(0,r.kt)(n.Cl,{mdxType:"DocStringContainer"},(0,r.kt)("div",null,(0,r.kt)(n.Dx,{type:"class",name:"colossalai.nn.DistributedAdaFactor",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/nn/optimizer/distributed_adafactor.py#L15",mdxType:"Title"}),(0,r.kt)(n.Pc,{mdxType:"Signature"},"params, lr = None, eps = (1e-30, 0.001), clip_threshold = 1.0, decay_rate = -0.8, beta1 = None, weight_decay = 0.0, scale_parameter = True, relative_step = True, warmup_init = False")),(0,r.kt)("div",null,(0,r.kt)(n.iz,{name:"Description",mdxType:"Divider"})),(0,r.kt)(n.Cl,{mdxType:"DocStringContainer"},(0,r.kt)("div",null,(0,r.kt)(n.Dx,{type:"function",name:"setup_distributed",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/nn/optimizer/distributed_adafactor.py#L60",mdxType:"Title"}),(0,r.kt)(n.Pc,{mdxType:"Signature"},"tp_group: ProcessGroup = None, dp_group: ProcessGroup = None, shard_to_working_param: typing.Dict = {}, padding_map = None, use_zero: bool = True"),(0,r.kt)(n.aE,{mdxType:"Parameters"},"tp_group -- The devices group for tensor parallel;\ndp_group -- The devices group for data parallel;\n- **shard_to_working_param** (Dict) -- ZeRO 2 feeds the optimizer a sharded param view as grads are sharded.\n  This maps from id(view) to working params used in forward & backward.\n  padding_map -- An empty interface placeholder;\n  use_zero -- Whether or not to use zero;")),(0,r.kt)("div",null,(0,r.kt)(n.iz,{name:"Description",mdxType:"Divider"}),"Setup process groups for TP and ZeRO 2. Inject features to the Optimizer")),(0,r.kt)(n.Cl,{mdxType:"DocStringContainer"},(0,r.kt)("div",null,(0,r.kt)(n.Dx,{type:"function",name:"step",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/nn/optimizer/distributed_adafactor.py#L285",mdxType:"Title"}),(0,r.kt)(n.Pc,{mdxType:"Signature"},"closure = None"),(0,r.kt)(n.aE,{mdxType:"Parameters"},"- **closure** (callable, optional) -- A closure that reevaluates the model\n  and returns the loss.")),(0,r.kt)("div",null,(0,r.kt)(n.iz,{name:"Description",mdxType:"Divider"}),(0,r.kt)("p",null,"Performs a single optimization steps")))),(0,r.kt)(n.Cl,{mdxType:"DocStringContainer"},(0,r.kt)("div",null,(0,r.kt)(n.Dx,{type:"class",name:"colossalai.nn.DistributedLamb",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/nn/optimizer/distributed_lamb.py#L15",mdxType:"Title"}),(0,r.kt)(n.Pc,{mdxType:"Signature"},"params, lr = 0.001, betas = (0.9, 0.999), eps = 1e-06, weight_decay = 0, bias_correction = True"),(0,r.kt)(n.aE,{mdxType:"Parameters"},"- **params** (iterable) -- iterable of parameters to optimize or dicts defining\n  parameter groups\n- **lr** (float, optional) -- learning rate (default: 1e-3)\n- **betas** (Tuple[float, float], optional) -- coefficients used for computing\n  running averages of gradient and its square (default: (0.9, 0.999))\n- **eps** (float, optional) -- term added to the denominator to improve\n  numerical stability (default: 1e-8)\n- **weight_decay** (float, optional) -- weight decay (L2 penalty) (default: 0)")),(0,r.kt)("div",null,(0,r.kt)(n.iz,{name:"Description",mdxType:"Divider"}),"Implements the Lamb algorithm, with extra support for ZeRO 2 and Tensor Parallel. Proposed in `Large Batch Optimization for Deep Learning: Training BERT in 76 minutes`_. It's recommended to use this with HybridParallelPlugin/ZeRO plugin and booster, which will take care of setup_distributed. Example with 4 devices: >>> optim = DistributedLamb(model.parameters(), lr=1e-3) >>> proc_mesh = ProcessGroupMesh(tp_size, zero_size) >>> tp_group = proc_mesh.get_group_along_axis(0) >>> dp_group = proc_mesh.get_group_along_axis(1) >>> optim.setup_distributed(tp_group, dp_group)",(0,r.kt)("p",null,".. _Large Batch Optimization for Deep Learning: Training BERT in 76 minutes:\n",(0,r.kt)("a",{parentName:"p",href:"https://arxiv.org/abs/1904.00962"},"https://arxiv.org/abs/1904.00962"))),(0,r.kt)(n.Cl,{mdxType:"DocStringContainer"},(0,r.kt)("div",null,(0,r.kt)(n.Dx,{type:"function",name:"setup_distributed",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/nn/optimizer/distributed_lamb.py#L65",mdxType:"Title"}),(0,r.kt)(n.Pc,{mdxType:"Signature"},"tp_group: typing.Optional[torch.distributed.distributed_c10d.ProcessGroup] = None, dp_group: typing.Optional[torch.distributed.distributed_c10d.ProcessGroup] = None, shard_to_working_param: typing.Optional[typing.Dict] = {}, padding_map = None, is_zero: typing.Optional[bool] = False"),(0,r.kt)(n.aE,{mdxType:"Parameters"},"- **tp_group** (dist.ProcessGroup) -- Tensor Parallel process group\n- **dp_group** (dist.ProcessGroup) -- ZeRO 2 process group\n- **shard_to_working_param** (Dict) -- ZeRO 2 feeds the optimizer a sharded param view as grads are sharded.\n  This maps from id(view) to working params used in forward & backward.\n  padding_map -- An empty interface placeholder\n- **is_zero** (bool) -- Whether to use ZeRO 2.")),(0,r.kt)("div",null,(0,r.kt)(n.iz,{name:"Description",mdxType:"Divider"}),"Assign process groups for TP and ZeRO 2.")),(0,r.kt)(n.Cl,{mdxType:"DocStringContainer"},(0,r.kt)("div",null,(0,r.kt)(n.Dx,{type:"function",name:"step",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/nn/optimizer/distributed_lamb.py#L103",mdxType:"Title"}),(0,r.kt)(n.Pc,{mdxType:"Signature"},"closure = None"),(0,r.kt)(n.aE,{mdxType:"Parameters"},"- **closure** (callable, optional) -- A closure that reevaluates the model\n  and returns the loss.")),(0,r.kt)("div",null,(0,r.kt)(n.iz,{name:"Description",mdxType:"Divider"}),"Performs a single optimization step."))),(0,r.kt)(n.Cl,{mdxType:"DocStringContainer"},(0,r.kt)("div",null,(0,r.kt)(n.Dx,{type:"class",name:"colossalai.nn.DistGaloreAwamW",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/nn/optimizer/distributed_galore.py#L21",mdxType:"Title"}),(0,r.kt)(n.Pc,{mdxType:"Signature"},"params, lr = 0.01, betas = (0.9, 0.999), eps = 1e-08, weight_decay = 0.01, nbits = 8, min_8bit_size = 4096, percentile_clipping = 100, block_wise = True, is_paged = False, args = None"),(0,r.kt)(n.aE,{mdxType:"Parameters"},"- **params** (iterable) -- iterable of parameters to optimize or dicts defining\n  parameter groups.\n- **lr** (float, optional) -- learning rate. (default: 1e-3)\n- **betas** (Tuple[float, float], optional) -- coefficients used for computing\n  running averages of gradient and its norm. (default: (0.9, 0.999))\n- **eps** (float, optional) -- term added to the denominator to improve\n  numerical stability. (default: 1e-6)\n- **weight_decay** (float, optional) -- weight decay (L2 penalty) (default: 0.01)\n  nbits -- Number of bits for quantization optim states. Only 32 and 8 are supported.\n- **min_8bit_size** (`int`, defaults to 4096) --\n  The minimum number of elements of the parameter tensors for 8-bit optimization.\n- **percentile_clipping** (`int`, defaults to 100) --\n  Adapts clipping threshold automatically by tracking the last 100 gradient norms and clipping the gradient at a certain percentile to improve stability.\n- **block_wise** (`bool`, defaults to `True`) --\n  Whether to independently quantize each block of tensors to reduce outlier effects and improve stability.\n- **is_paged** (`bool`, defaults to `False`) --\n  Whether the optimizer is a paged optimizer (handle memory spike via CPU-GPU transfer) or not.\n- **args** (dict, optional) -- quantization-related arguments. If passed, will override all quantization args above.")),(0,r.kt)("div",null,(0,r.kt)(n.iz,{name:"Description",mdxType:"Divider"}),"Implements Galore, a optimizer-agonistic gradient compression technique on 8-bit AdamW. It largely compresses gradient via low-rank projection and is claimed to be insensitive to hyperparams like lr. Supports Tensor Parallel and ZeRO stage 1 and 2 via booster and plugin. Proposed in `GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection` https://arxiv.org/abs/2403.03507"),(0,r.kt)(n.Cl,{mdxType:"DocStringContainer"},(0,r.kt)("div",null,(0,r.kt)(n.Dx,{type:"function",name:"setup_distributed",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/nn/optimizer/distributed_galore.py#L94",mdxType:"Title"}),(0,r.kt)(n.Pc,{mdxType:"Signature"},"tp_group: typing.Optional[torch.distributed.distributed_c10d.ProcessGroup] = None, dp_group: typing.Optional[torch.distributed.distributed_c10d.ProcessGroup] = None, shard_to_working_param: typing.Optional[typing.Dict] = {}, padding_map: typing.Optional[typing.Dict] = defaultdict(<class 'int'>, {}), is_zero: typing.Optional[bool] = False"),(0,r.kt)(n.aE,{mdxType:"Parameters"},"- **tp_group** (dist.ProcessGroup) -- Tensor Parallel process group\n- **dp_group** (dist.ProcessGroup) -- ZeRO 2 process group\n- **shard_to_working_param** (Dict) -- ZeRO 2 feeds the optimizer a sharded param view as grads are sharded.\n  This maps from id(view) to working params used in forward & backward.\n- **padding_map** (Dict) -- Padding size of each param from ZeRO's param store. Required if ZeRO is used.\n- **is_zero** (bool) -- Whether to use ZeRO 2.")),(0,r.kt)("div",null,(0,r.kt)(n.iz,{name:"Description",mdxType:"Divider"}),"Setup process groups for TP and ZeRO 2.")),(0,r.kt)(n.Cl,{mdxType:"DocStringContainer"},(0,r.kt)("div",null,(0,r.kt)(n.Dx,{type:"function",name:"step",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/nn/optimizer/distributed_galore.py#L142",mdxType:"Title"}),(0,r.kt)(n.Pc,{mdxType:"Signature"},"closure = None"),(0,r.kt)(n.aE,{mdxType:"Parameters"},"- **closure** (callable, optional) -- A closure that reevaluates the model\n  and returns the loss.")),(0,r.kt)("div",null,(0,r.kt)(n.iz,{name:"Description",mdxType:"Divider"}),"Performs a single optimization step.")),(0,r.kt)(n.Cl,{mdxType:"DocStringContainer"},(0,r.kt)("div",null,(0,r.kt)(n.Dx,{type:"function",name:"to_master_shape",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/nn/optimizer/distributed_galore.py#L268",mdxType:"Title"}),(0,r.kt)(n.Pc,{mdxType:"Signature"},"data, padding")),(0,r.kt)("div",null,(0,r.kt)(n.iz,{name:"Description",mdxType:"Divider"}),"Pad to master (optimizer) param shape"))),(0,r.kt)(n.Cl,{mdxType:"DocStringContainer"},(0,r.kt)("div",null,(0,r.kt)(n.Dx,{type:"class",name:"colossalai.nn.DistributedCAME",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/nn/optimizer/distributed_came.py#L11",mdxType:"Title"}),(0,r.kt)(n.Pc,{mdxType:"Signature"},"params, lr = None, eps = (1e-30, 1e-16), clip_threshold = 1.0, betas = (0.9, 0.999, 0.9999), weight_decay = 0.0"),(0,r.kt)(n.aE,{mdxType:"Parameters"},"- **params** (iterable) -- iterable of parameters to optimize or dicts defining\n  parameter groups\n- **lr** (float, optional) -- external learning rate (default: None)\n- **eps** (tuple[float, float]) -- regularization constants for square gradient\n  and instability respectively (default: (1e-30, 1e-16))\n- **clip_threshold** (float) -- threshold of root-mean-square of\n  final gradient update (default: 1.0)\n- **betas** (tuple[float, float, float]) -- coefficient used for computing running averages of\n- **update,** square gradient and instability (default -- (0.9, 0.999, 0.9999)))\n- **weight_decay** (float, optional) -- weight decay (L2 penalty) (default: 0)")),(0,r.kt)("div",null,(0,r.kt)(n.iz,{name:"Description",mdxType:"Divider"}),"Implements CAME algorithm. This implementation is based on: `CAME: Confidence-guided Adaptive Memory Efficient Optimization`"),(0,r.kt)(n.Cl,{mdxType:"DocStringContainer"},(0,r.kt)("div",null,(0,r.kt)(n.Dx,{type:"function",name:"setup_distributed",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/nn/optimizer/distributed_came.py#L68",mdxType:"Title"}),(0,r.kt)(n.Pc,{mdxType:"Signature"},"tp_group: ProcessGroup = None, dp_group: ProcessGroup = None, shard_to_working_param: typing.Dict = {}, padding_map = None, use_zero: bool = True"),(0,r.kt)(n.aE,{mdxType:"Parameters"},"tp_group -- The devices group for tensor parallel;\ndp_group -- The devices group for data parallel;\n- **shard_to_working_param** (Dict) -- ZeRO 2 feeds the optimizer a sharded param view as grads are sharded.\n  This maps from id(view) to working params used in forward & backward.\n  padding_map -- Interface placeholder\n  use_zero -- Whether or not to use zero;")),(0,r.kt)("div",null,(0,r.kt)(n.iz,{name:"Description",mdxType:"Divider"}),(0,r.kt)("p",null,"Inject features to the Optimizer"))),(0,r.kt)(n.Cl,{mdxType:"DocStringContainer"},(0,r.kt)("div",null,(0,r.kt)(n.Dx,{type:"function",name:"step",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/nn/optimizer/distributed_came.py#L332",mdxType:"Title"}),(0,r.kt)(n.Pc,{mdxType:"Signature"},"closure = None"),(0,r.kt)(n.aE,{mdxType:"Parameters"},"- **closure** (callable, optional) -- A closure that reevaluates the model\n  and returns the loss.")),(0,r.kt)("div",null,(0,r.kt)(n.iz,{name:"Description",mdxType:"Divider"}),"Performs a single optimization step."))))}m.isMDXComponent=!0}}]);