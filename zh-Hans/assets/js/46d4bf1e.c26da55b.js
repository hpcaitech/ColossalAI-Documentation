"use strict";(self.webpackChunkdemo=self.webpackChunkdemo||[]).push([[9532],{6999:(e,t,o)=>{o.d(t,{Cl:()=>r,Dx:()=>d,Pc:()=>a,aE:()=>l,e_:()=>c,iz:()=>s,nT:()=>p});var i=o(7294),n=o(398);o(814);function r(e){return i.createElement("div",{className:"docstring-container"},e.children)}function a(e){return i.createElement("div",{className:"signature"},"(",e.children,")")}function s(e){return i.createElement("div",{class:"divider"},i.createElement("span",{class:"divider-text"},e.name))}function l(e){return i.createElement("div",null,i.createElement(s,{name:"Parameters"}),i.createElement(n.D,null,e.children))}function p(e){return i.createElement("div",null,i.createElement(s,{name:"Returns"}),i.createElement(n.D,null,`${e.name}: ${e.desc}`))}function d(e){return i.createElement("div",{className:"title-container"},i.createElement("div",{className:"title-module"},i.createElement("h5",null,e.type),"\xa0 ",i.createElement("h3",null,e.name)),i.createElement("div",{className:"title-source"},"<",i.createElement("a",{href:e.source,className:"title-source"},"source"),">"))}function c(e){return i.createElement("div",null,i.createElement(s,{name:"Example"}),i.createElement(n.D,null,e.code))}},9591:(e,t,o)=>{o.r(t),o.d(t,{assets:()=>p,contentTitle:()=>s,default:()=>m,frontMatter:()=>a,metadata:()=>l,toc:()=>d});var i=o(7462),n=(o(7294),o(3905)),r=o(6999);const a={},s="Booster API",l={unversionedId:"basics/booster_api",id:"basics/booster_api",title:"Booster API",description:"\u4f5c\u8005: Mingyan Jiang, Jianghai Chen, Baizhou Zhang",source:"@site/i18n/zh-Hans/docusaurus-plugin-content-docs/current/basics/booster_api.md",sourceDirName:"basics",slug:"/basics/booster_api",permalink:"/zh-Hans/docs/basics/booster_api",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/basics/booster_api.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"\u542f\u52a8 Colossal-AI",permalink:"/zh-Hans/docs/basics/launch_colossalai"},next:{title:"Booster \u63d2\u4ef6",permalink:"/zh-Hans/docs/basics/booster_plugins"}},p={},d=[{value:"\u7b80\u4ecb",id:"\u7b80\u4ecb",level:2},{value:"Booster \u63d2\u4ef6",id:"booster-\u63d2\u4ef6",level:3},{value:"Booster \u63a5\u53e3",id:"booster-\u63a5\u53e3",level:3},{value:"\u4f7f\u7528\u65b9\u6cd5\u53ca\u793a\u4f8b",id:"\u4f7f\u7528\u65b9\u6cd5\u53ca\u793a\u4f8b",level:2}],c={toc:d},u="wrapper";function m(e){let{components:t,...o}=e;return(0,n.kt)(u,(0,i.Z)({},c,o,{components:t,mdxType:"MDXLayout"}),(0,n.kt)("h1",{id:"booster-api"},"Booster API"),(0,n.kt)("p",null,"\u4f5c\u8005: ",(0,n.kt)("a",{parentName:"p",href:"https://github.com/jiangmingyan"},"Mingyan Jiang"),", ",(0,n.kt)("a",{parentName:"p",href:"https://github.com/CjhHa1"},"Jianghai Chen"),", ",(0,n.kt)("a",{parentName:"p",href:"https://github.com/Fridge003"},"Baizhou Zhang")),(0,n.kt)("p",null,(0,n.kt)("strong",{parentName:"p"},"\u9884\u5907\u77e5\u8bc6:")),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"/zh-Hans/docs/concepts/distributed_training"},"\u5206\u5e03\u5f0f\u8bad\u7ec3")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"/zh-Hans/docs/concepts/colossalai_overview"},"Colossal-AI \u603b\u89c8"))),(0,n.kt)("p",null,(0,n.kt)("strong",{parentName:"p"},"\u793a\u4f8b\u4ee3\u7801")),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/hpcaitech/ColossalAI/blob/main/examples/tutorial/new_api/cifar_resnet"},"\u4f7f\u7528Booster\u5728CIFAR-10\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3ResNet")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/llama2"},"\u4f7f\u7528Booster\u5728RedPajama\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3Llama-1/2"))),(0,n.kt)("h2",{id:"\u7b80\u4ecb"},"\u7b80\u4ecb"),(0,n.kt)("p",null,"\u5728\u6211\u4eec\u7684\u65b0\u8bbe\u8ba1\u4e2d\uff0c ",(0,n.kt)("inlineCode",{parentName:"p"},"colossalai.booster")," \u4ee3\u66ff ",(0,n.kt)("inlineCode",{parentName:"p"},"colossalai.initialize")," \u5c06\u7279\u5f81(\u4f8b\u5982\uff0c\u6a21\u578b\u3001\u4f18\u5316\u5668\u3001\u6570\u636e\u52a0\u8f7d\u5668)\u65e0\u7f1d\u6ce8\u5165\u5230\u60a8\u7684\u8bad\u7ec3\u7ec4\u4ef6\u4e2d\u3002 \u4f7f\u7528 booster API, \u60a8\u53ef\u4ee5\u66f4\u53cb\u597d\u5730\u5c06\u6211\u4eec\u7684\u5e76\u884c\u7b56\u7565\u6574\u5408\u5230\u5f85\u8bad\u7ec3\u6a21\u578b\u4e2d. \u8c03\u7528 ",(0,n.kt)("inlineCode",{parentName:"p"},"colossalai.booster")," \u662f\u60a8\u8fdb\u5165\u8bad\u7ec3\u6d41\u7a0b\u524d\u7684\u6b63\u5e38\u64cd\u4f5c\u3002\n\u5728\u4e0b\u9762\u7684\u7ae0\u8282\u4e2d\uff0c\u6211\u4eec\u5c06\u4ecb\u7ecd ",(0,n.kt)("inlineCode",{parentName:"p"},"colossalai.booster")," \u662f\u5982\u4f55\u5de5\u4f5c\u7684\u4ee5\u53ca\u4f7f\u7528\u65f6\u6211\u4eec\u8981\u6ce8\u610f\u7684\u7ec6\u8282\u3002"),(0,n.kt)("h3",{id:"booster-\u63d2\u4ef6"},"Booster \u63d2\u4ef6"),(0,n.kt)("p",null,"Booster \u63d2\u4ef6\u662f\u7ba1\u7406\u5e76\u884c\u914d\u7f6e\u7684\u91cd\u8981\u7ec4\u4ef6\uff08eg\uff1agemini \u63d2\u4ef6\u5c01\u88c5\u4e86 gemini \u52a0\u901f\u65b9\u6848\uff09\u3002\u76ee\u524d\u652f\u6301\u7684\u63d2\u4ef6\u5982\u4e0b\uff1a"),(0,n.kt)("p",null,(0,n.kt)("strong",{parentName:"p"},(0,n.kt)("em",{parentName:"strong"},"HybridParallelPlugin:"))," HybirdParallelPlugin \u63d2\u4ef6\u5c01\u88c5\u4e86\u6df7\u5408\u5e76\u884c\u7684\u52a0\u901f\u89e3\u51b3\u65b9\u6848\u3002\u5b83\u63d0\u4f9b\u7684\u63a5\u53e3\u53ef\u4ee5\u5728\u5f20\u91cf\u5e76\u884c\uff0c\u6d41\u6c34\u7ebf\u5e76\u884c\u4ee5\u53ca\u4e24\u79cd\u6570\u636e\u5e76\u884c\u65b9\u6cd5\uff08DDP, Zero\uff09\u95f4\u8fdb\u884c\u4efb\u610f\u7684\u7ec4\u5408\u3002"),(0,n.kt)("p",null,(0,n.kt)("strong",{parentName:"p"},(0,n.kt)("em",{parentName:"strong"},"GeminiPlugin:"))," GeminiPlugin \u63d2\u4ef6\u5c01\u88c5\u4e86 gemini \u52a0\u901f\u89e3\u51b3\u65b9\u6848\uff0c\u5373\u57fa\u4e8e\u5757\u5185\u5b58\u7ba1\u7406\u7684 ZeRO \u4f18\u5316\u65b9\u6848\u3002"),(0,n.kt)("p",null,(0,n.kt)("strong",{parentName:"p"},(0,n.kt)("em",{parentName:"strong"},"TorchDDPPlugin:"))," TorchDDPPlugin \u63d2\u4ef6\u5c01\u88c5\u4e86Pytorch\u7684DDP\u52a0\u901f\u65b9\u6848\uff0c\u5b9e\u73b0\u4e86\u6a21\u578b\u7ea7\u522b\u7684\u6570\u636e\u5e76\u884c\uff0c\u53ef\u4ee5\u8de8\u591a\u673a\u8fd0\u884c\u3002"),(0,n.kt)("p",null,(0,n.kt)("strong",{parentName:"p"},(0,n.kt)("em",{parentName:"strong"},"LowLevelZeroPlugin:"))," LowLevelZeroPlugin \u63d2\u4ef6\u5c01\u88c5\u4e86\u96f6\u5197\u4f59\u4f18\u5316\u5668\u7684 1/2 \u9636\u6bb5\u3002\u9636\u6bb5 1\uff1a\u5207\u5206\u4f18\u5316\u5668\u53c2\u6570\uff0c\u5206\u53d1\u5230\u5404\u5e76\u53d1\u8fdb\u7a0b\u6216\u5e76\u53d1 GPU \u4e0a\u3002\u9636\u6bb5 2\uff1a\u5207\u5206\u4f18\u5316\u5668\u53c2\u6570\u53ca\u68af\u5ea6\uff0c\u5206\u53d1\u5230\u5404\u5e76\u53d1\u8fdb\u7a0b\u6216\u5e76\u53d1 GPU \u4e0a\u3002"),(0,n.kt)("p",null,(0,n.kt)("strong",{parentName:"p"},(0,n.kt)("em",{parentName:"strong"},"TorchFSDPPlugin:"))," TorchFSDPPlugin\u5c01\u88c5\u4e86 Pytorch\u7684FSDP\u52a0\u901f\u65b9\u6848\uff0c\u53ef\u4ee5\u7528\u4e8e\u96f6\u5197\u4f59\u4f18\u5316\u5668\u6570\u636e\u5e76\u884c\uff08ZeroDP\uff09\u7684\u8bad\u7ec3\u3002"),(0,n.kt)("p",null,"\u82e5\u60f3\u4e86\u89e3\u66f4\u591a\u5173\u4e8e\u63d2\u4ef6\u7684\u7528\u6cd5\u7ec6\u8282\uff0c\u8bf7\u53c2\u8003",(0,n.kt)("a",{parentName:"p",href:"/zh-Hans/docs/basics/booster_plugins"},"Booster \u63d2\u4ef6"),"\u7ae0\u8282\u3002"),(0,n.kt)("h3",{id:"booster-\u63a5\u53e3"},"Booster \u63a5\u53e3"),(0,n.kt)(r.Cl,{mdxType:"DocStringContainer"},(0,n.kt)("div",null,(0,n.kt)(r.Dx,{type:"class",name:"colossalai.booster.Booster",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/booster/booster.py#L22",mdxType:"Title"}),(0,n.kt)(r.Pc,{mdxType:"Signature"},"device: typing.Optional[str] = None, mixed_precision: typing.Union[colossalai.booster.mixed_precision.mixed_precision_base.MixedPrecision, str, NoneType] = None, plugin: typing.Optional[colossalai.booster.plugin.plugin_base.Plugin] = None"),(0,n.kt)(r.aE,{mdxType:"Parameters"},"- **device** (str or torch.device) -- The device to run the training. Default: None.\n  If plugin is not used or plugin doesn't control the device,\n  this argument will be set as training device ('cuda' will be used if argument is None).\n- **mixed_precision** (str or MixedPrecision) -- The mixed precision to run the training. Default: None.\n  If the argument is a string, it can be 'fp16', 'fp16_apex', 'bf16', or 'fp8'.\n  'fp16' would use PyTorch AMP while `fp16_apex` would use Nvidia Apex.\n- **plugin** (Plugin) -- The plugin to run the training. Default: None.")),(0,n.kt)("div",null,(0,n.kt)(r.iz,{name:"Description",mdxType:"Divider"}),(0,n.kt)("p",null,"Booster is a high-level API for training neural networks. It provides a unified interface for\ntraining with different precision, accelerator, and plugin."),(0,n.kt)(r.e_,{code:"```python\n# Following is pseudocode\n\ncolossalai.launch(...)\nplugin = GeminiPlugin(...)\nbooster = Booster(precision='fp16', plugin=plugin)\n\nmodel = GPT2()\noptimizer = HybridAdam(model.parameters())\ndataloader = plugin.prepare_dataloader(train_dataset, batch_size=8)\nlr_scheduler = LinearWarmupScheduler()\ncriterion = GPTLMLoss()\n\nmodel, optimizer, criterion, dataloader, lr_scheduler = booster.boost(model, optimizer, criterion, dataloader, lr_scheduler)\n\nfor epoch in range(max_epochs):\n    for input_ids, attention_mask in dataloader:\n        outputs = model(input_ids.cuda(), attention_mask.cuda())\n        loss = criterion(outputs.logits, input_ids)\n        booster.backward(loss, optimizer)\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n```",mdxType:"ExampleCode"})),(0,n.kt)(r.Cl,{mdxType:"DocStringContainer"},(0,n.kt)("div",null,(0,n.kt)(r.Dx,{type:"function",name:"backward",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/booster/booster.py#L151",mdxType:"Title"}),(0,n.kt)(r.Pc,{mdxType:"Signature"},"loss: Tensor, optimizer: Optimizer"),(0,n.kt)(r.aE,{mdxType:"Parameters"},"- **loss** (torch.Tensor) -- The loss for backpropagation.\n- **optimizer** (Optimizer) -- The optimizer to be updated.")),(0,n.kt)("div",null,(0,n.kt)(r.iz,{name:"Description",mdxType:"Divider"}),"Execution of backward during training step.")),(0,n.kt)(r.Cl,{mdxType:"DocStringContainer"},(0,n.kt)("div",null,(0,n.kt)(r.Dx,{type:"function",name:"boost",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/booster/booster.py#L109",mdxType:"Title"}),(0,n.kt)(r.Pc,{mdxType:"Signature"},"model: Module, optimizer: typing.Optional[torch.optim.optimizer.Optimizer] = None, criterion: typing.Optional[typing.Callable] = None, dataloader: typing.Optional[torch.utils.data.dataloader.DataLoader] = None, lr_scheduler: typing.Optional[torch.optim.lr_scheduler._LRScheduler] = None"),(0,n.kt)(r.aE,{mdxType:"Parameters"},"- **model** (nn.Module) -- Convert model into a wrapped model for distributive training.\n  The model might be decorated or partitioned by plugin's strategy after execution of this method.\n- **optimizer** (Optimizer, optional) -- Convert optimizer into a wrapped optimizer for distributive training.\n  The optimizer's param groups or states might be decorated or partitioned by plugin's strategy after execution of this method. Defaults to None.\n- **criterion** (Callable, optional) -- The function that calculates loss. Defaults to None.\n- **dataloader** (DataLoader, optional) -- The prepared dataloader for training. Defaults to None.\n- **lr_scheduler** (LRScheduler, optional) -- The learning scheduler for training. Defaults to None."),(0,n.kt)(r.nT,{name:"List[Union[nn.Module, Optimizer, LRScheduler, DataLoader]]",desc:"The list of boosted input arguments.",mdxType:"Returns"})),(0,n.kt)("div",null,(0,n.kt)(r.iz,{name:"Description",mdxType:"Divider"}),(0,n.kt)("p",null,"Wrap and inject features to the passed in model, optimizer, criterion, lr_scheduler, and dataloader."))),(0,n.kt)(r.Cl,{mdxType:"DocStringContainer"},(0,n.kt)("div",null,(0,n.kt)(r.Dx,{type:"function",name:"execute_pipeline",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/booster/booster.py#L161",mdxType:"Title"}),(0,n.kt)(r.Pc,{mdxType:"Signature"},"data_iter: typing.Iterator, model: Module, criterion: typing.Callable[[typing.Any, typing.Any], torch.Tensor], optimizer: typing.Optional[torch.optim.optimizer.Optimizer] = None, return_loss: bool = True, return_outputs: bool = False"),(0,n.kt)(r.aE,{mdxType:"Parameters"},"data_iter(Iterator) -- The iterator for getting the next batch of data. Usually there are two ways to obtain this argument:\n1. wrap the dataloader to iterator through: iter(dataloader)\n2. get the next batch from dataloader, and wrap this batch to iterator: iter([batch])\n- **model** (nn.Module) -- The model to execute forward/backward, it should be a model wrapped by a plugin that supports pipeline.\ncriterion -- (Callable[[Any, Any], torch.Tensor]): Criterion to be used. It should take two arguments: model outputs and inputs, and returns loss tensor.\n'lambda y, x: loss_fn(y)' can turn a normal loss function into a valid two-argument criterion here.\n  - **optimizer** (Optimizer, optional) -- The optimizer for execution of backward. Can be None when only doing forward (i.e. evaluation). Defaults to None.\n- **return_loss** (bool, optional) -- Whether to return loss in the dict returned by this method. Defaults to True.\n- **return_output** (bool, optional) -- Whether to return Huggingface style model outputs in the dict returned by this method. Defaults to False."),(0,n.kt)(r.nT,{name:"Dict[str, Any]",desc:"Output dict in the form of &lcub;'loss': ..., 'outputs': ...}.\nret_dict['loss'] is the loss of forward if return_loss is set to True, else None.\nret_dict['outputs'] is the Huggingface style model outputs during forward if return_output is set to True, else None.",mdxType:"Returns"})),(0,n.kt)("div",null,(0,n.kt)(r.iz,{name:"Description",mdxType:"Divider"}),(0,n.kt)("p",null,"Execute forward & backward when utilizing pipeline parallel.\nReturn loss or Huggingface style model outputs if needed."),(0,n.kt)("p",null,"Warning: This function is tailored for the scenario of pipeline parallel.\nAs a result, please don't do the forward/backward pass in the conventional way (model(input)/loss.backward())\nwhen doing pipeline parallel training with booster, which will cause unexpected errors."))),(0,n.kt)(r.Cl,{mdxType:"DocStringContainer"},(0,n.kt)("div",null,(0,n.kt)(r.Dx,{type:"function",name:"load_lr_scheduler",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/booster/booster.py#L310",mdxType:"Title"}),(0,n.kt)(r.Pc,{mdxType:"Signature"},"lr_scheduler: _LRScheduler, checkpoint: str"),(0,n.kt)(r.aE,{mdxType:"Parameters"},"- **lr_scheduler** (LRScheduler) -- A lr scheduler boosted by Booster.\n- **checkpoint** (str) -- Path to the checkpoint. It must be a local file path.")),(0,n.kt)("div",null,(0,n.kt)(r.iz,{name:"Description",mdxType:"Divider"}),"Load lr scheduler from checkpoint.")),(0,n.kt)(r.Cl,{mdxType:"DocStringContainer"},(0,n.kt)("div",null,(0,n.kt)(r.Dx,{type:"function",name:"load_model",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/booster/booster.py#L216",mdxType:"Title"}),(0,n.kt)(r.Pc,{mdxType:"Signature"},"model: typing.Union[torch.nn.modules.module.Module, colossalai.interface.model.ModelWrapper], checkpoint: str, strict: bool = True"),(0,n.kt)(r.aE,{mdxType:"Parameters"},"- **model** (nn.Module or ModelWrapper) -- A model boosted by Booster.\n- **checkpoint** (str) -- Path to the checkpoint. It must be a local path.\n  It should be a directory path if the checkpoint is sharded. Otherwise, it should be a file path.\n- **strict** (bool, optional) -- whether to strictly enforce that the keys\n  in :attr:*state_dict* match the keys returned by this module's\n  [`~torch.nn.Module.state_dict`] function. Defaults to True.")),(0,n.kt)("div",null,(0,n.kt)(r.iz,{name:"Description",mdxType:"Divider"}),"Load model from checkpoint.")),(0,n.kt)(r.Cl,{mdxType:"DocStringContainer"},(0,n.kt)("div",null,(0,n.kt)(r.Dx,{type:"function",name:"load_optimizer",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/booster/booster.py#L263",mdxType:"Title"}),(0,n.kt)(r.Pc,{mdxType:"Signature"},"optimizer: Optimizer, checkpoint: str"),(0,n.kt)(r.aE,{mdxType:"Parameters"},"- **optimizer** (Optimizer) -- An optimizer boosted by Booster.\n- **checkpoint** (str) -- Path to the checkpoint. It must be a local path.\n  It should be a directory path if the checkpoint is sharded. Otherwise, it should be a file path.\n- **prefix** (str, optional) -- A prefix added to parameter and buffer\n  names to compose the keys in state_dict. Defaults to None.\n- **size_per_shard** (int, optional) -- Maximum size of checkpoint shard file in MB. This is useful only when `shard=True`. Defaults to 1024.")),(0,n.kt)("div",null,(0,n.kt)(r.iz,{name:"Description",mdxType:"Divider"}),"Load optimizer from checkpoint.")),(0,n.kt)(r.Cl,{mdxType:"DocStringContainer"},(0,n.kt)("div",null,(0,n.kt)(r.Dx,{type:"function",name:"no_sync",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/booster/booster.py#L199",mdxType:"Title"}),(0,n.kt)(r.Pc,{mdxType:"Signature"},"model: Module = None, optimizer: OptimizerWrapper = None"),(0,n.kt)(r.aE,{mdxType:"Parameters"},"- **model** (nn.Module) -- The model to be disabled gradient synchronization, for DDP\n- **optimizer** (OptimizerWrapper) -- The optimizer to be disabled gradient synchronization, for ZeRO1-1"),(0,n.kt)(r.nT,{name:"contextmanager",desc:"Context to disable gradient synchronization.",mdxType:"Returns"})),(0,n.kt)("div",null,(0,n.kt)(r.iz,{name:"Description",mdxType:"Divider"}),"Context manager to disable gradient synchronization across DP process groups. Support torch DDP and Low Level ZeRO-1 for now.")),(0,n.kt)(r.Cl,{mdxType:"DocStringContainer"},(0,n.kt)("div",null,(0,n.kt)(r.Dx,{type:"function",name:"save_lr_scheduler",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/booster/booster.py#L301",mdxType:"Title"}),(0,n.kt)(r.Pc,{mdxType:"Signature"},"lr_scheduler: _LRScheduler, checkpoint: str"),(0,n.kt)(r.aE,{mdxType:"Parameters"},"- **lr_scheduler** (LRScheduler) -- A lr scheduler boosted by Booster.\n- **checkpoint** (str) -- Path to the checkpoint. It must be a local file path.")),(0,n.kt)("div",null,(0,n.kt)(r.iz,{name:"Description",mdxType:"Divider"}),"Save lr scheduler to checkpoint.")),(0,n.kt)(r.Cl,{mdxType:"DocStringContainer"},(0,n.kt)("div",null,(0,n.kt)(r.Dx,{type:"function",name:"save_model",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/booster/booster.py#L229",mdxType:"Title"}),(0,n.kt)(r.Pc,{mdxType:"Signature"},"model: typing.Union[torch.nn.modules.module.Module, colossalai.interface.model.ModelWrapper], checkpoint: str, shard: bool = False, gather_dtensor: bool = True, prefix: typing.Optional[str] = None, size_per_shard: int = 1024, use_safetensors: bool = False"),(0,n.kt)(r.aE,{mdxType:"Parameters"},"- **model** (nn.Module or ModelWrapper) -- A model boosted by Booster.\n- **checkpoint** (str) -- Path to the checkpoint. It must be a local path.\n  It is a file path if `shard=False`. Otherwise, it is a directory path.\n- **shard** (bool, optional) -- Whether to save checkpoint a sharded way.\n  If true, the checkpoint will be a folder with the same format as Huggingface transformers checkpoint. Otherwise, it will be a single file. Defaults to False.\n- **gather_dtensor** (bool, optional) -- whether to gather the distributed tensor to the first device. Default: True.\n- **prefix** (str, optional) -- A prefix added to parameter and buffer\n  names to compose the keys in state_dict. Defaults to None.\n- **size_per_shard** (int, optional) -- Maximum size of checkpoint shard file in MB. This is useful only when `shard=True`. Defaults to 1024.\n- **use_safetensors** (bool, optional) -- whether to use safe tensors. Default: False. If set to True, the checkpoint will be saved.")),(0,n.kt)("div",null,(0,n.kt)(r.iz,{name:"Description",mdxType:"Divider"}),"Save model to checkpoint.")),(0,n.kt)(r.Cl,{mdxType:"DocStringContainer"},(0,n.kt)("div",null,(0,n.kt)(r.Dx,{type:"function",name:"save_optimizer",source:"https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/booster/booster.py#L276",mdxType:"Title"}),(0,n.kt)(r.Pc,{mdxType:"Signature"},"optimizer: Optimizer, checkpoint: str, shard: bool = False, gather_dtensor: bool = True, prefix: typing.Optional[str] = None, size_per_shard: int = 1024"),(0,n.kt)(r.aE,{mdxType:"Parameters"},"- **optimizer** (Optimizer) -- An optimizer boosted by Booster.\n- **checkpoint** (str) -- Path to the checkpoint. It must be a local path.\n  It is a file path if `shard=False`. Otherwise, it is a directory path.\n- **shard** (bool, optional) -- Whether to save checkpoint a sharded way.\n  If true, the checkpoint will be a folder. Otherwise, it will be a single file. Defaults to False.\n- **gather_dtensor** (bool) -- whether to gather the distributed tensor to the first device. Default: True.\n- **prefix** (str, optional) -- A prefix added to parameter and buffer\n  names to compose the keys in state_dict. Defaults to None.\n- **size_per_shard** (int, optional) -- Maximum size of checkpoint shard file in MB. This is useful only when `shard=True`. Defaults to 1024.")),(0,n.kt)("div",null,(0,n.kt)(r.iz,{name:"Description",mdxType:"Divider"}),(0,n.kt)("p",null,"Save optimizer to checkpoint.")))),(0,n.kt)("h2",{id:"\u4f7f\u7528\u65b9\u6cd5\u53ca\u793a\u4f8b"},"\u4f7f\u7528\u65b9\u6cd5\u53ca\u793a\u4f8b"),(0,n.kt)("p",null,"\u5728\u4f7f\u7528 colossalai \u8bad\u7ec3\u65f6\uff0c\u9996\u5148\u9700\u8981\u5728\u8bad\u7ec3\u811a\u672c\u7684\u5f00\u5934\u542f\u52a8\u5206\u5e03\u5f0f\u73af\u5883\uff0c\u5e76\u521b\u5efa\u9700\u8981\u4f7f\u7528\u7684\u6a21\u578b\u3001\u4f18\u5316\u5668\u3001\u635f\u5931\u51fd\u6570\u3001\u6570\u636e\u52a0\u8f7d\u5668\u7b49\u5bf9\u8c61\u3002\u4e4b\u540e\uff0c\u8c03\u7528",(0,n.kt)("inlineCode",{parentName:"p"},"booster.boost")," \u5c06\u7279\u5f81\u6ce8\u5165\u5230\u8fd9\u4e9b\u5bf9\u8c61\u4e2d\uff0c\u60a8\u5c31\u53ef\u4ee5\u4f7f\u7528\u6211\u4eec\u7684 booster API \u53bb\u8fdb\u884c\u60a8\u63a5\u4e0b\u6765\u7684\u8bad\u7ec3\u6d41\u7a0b\u3002"),(0,n.kt)("p",null,"\u4ee5\u4e0b\u662f\u4e00\u4e2a\u4f2a\u4ee3\u7801\u793a\u4f8b\uff0c\u5c06\u5c55\u793a\u5982\u4f55\u4f7f\u7528\u6211\u4eec\u7684 booster API \u8fdb\u884c\u6a21\u578b\u8bad\u7ec3:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},"import torch\nfrom torch.optim import SGD\nfrom torchvision.models import resnet18\n\nimport colossalai\nfrom colossalai.booster import Booster\nfrom colossalai.booster.plugin import TorchDDPPlugin\n\ndef train():\n    # launch colossalai\n    colossalai.launch(config=dict(), rank=rank, world_size=world_size, port=port, host='localhost')\n\n    # create plugin and objects for training\n    plugin = TorchDDPPlugin()\n    booster = Booster(plugin=plugin)\n    model = resnet18()\n    criterion = lambda x: x.mean()\n    optimizer = SGD((model.parameters()), lr=0.001)\n    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n\n    # use booster.boost to wrap the training objects\n    model, optimizer, criterion, _, scheduler = booster.boost(model, optimizer, criterion, lr_scheduler=scheduler)\n\n    # do training as normal, except that the backward should be called by booster\n    x = torch.randn(4, 3, 224, 224)\n    x = x.to('cuda')\n    output = model(x)\n    loss = criterion(output)\n    booster.backward(loss, optimizer)\n    optimizer.clip_grad_by_norm(1.0)\n    optimizer.step()\n    scheduler.step()\n    optimizer.zero_grad()\n\n    # checkpointing using booster api\n    save_path = \"./model\"\n    booster.save_model(model, save_path, shard=True, size_per_shard=10, use_safetensors=True)\n\n    new_model = resnet18()\n    booster.load_model(new_model, save_path)\n")),(0,n.kt)("p",null,"\u66f4\u591a\u7684Booster\u8bbe\u8ba1\u7ec6\u8282\u8bf7\u53c2\u8003\u8fd9\u4e00",(0,n.kt)("a",{parentName:"p",href:"https://github.com/hpcaitech/ColossalAI/discussions/3046"},"\u9875\u9762")))}m.isMDXComponent=!0}}]);