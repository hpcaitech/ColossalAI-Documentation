"use strict";(self.webpackChunkagile_docs=self.webpackChunkagile_docs||[]).push([[7969],{3905:function(e,n,t){t.d(n,{Zo:function(){return d},kt:function(){return m}});var a=t(7294);function i(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function r(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,a)}return t}function o(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?r(Object(t),!0).forEach((function(n){i(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):r(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function l(e,n){if(null==e)return{};var t,a,i=function(e,n){if(null==e)return{};var t,a,i={},r=Object.keys(e);for(a=0;a<r.length;a++)t=r[a],n.indexOf(t)>=0||(i[t]=e[t]);return i}(e,n);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)t=r[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(i[t]=e[t])}return i}var p=a.createContext({}),s=function(e){var n=a.useContext(p),t=n;return e&&(t="function"==typeof e?e(n):o(o({},n),e)),t},d=function(e){var n=s(e.components);return a.createElement(p.Provider,{value:n},e.children)},_={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},c=a.forwardRef((function(e,n){var t=e.components,i=e.mdxType,r=e.originalType,p=e.parentName,d=l(e,["components","mdxType","originalType","parentName"]),c=s(t),m=i,u=c["".concat(p,".").concat(m)]||c[m]||_[m]||r;return t?a.createElement(u,o(o({ref:n},d),{},{components:t})):a.createElement(u,o({ref:n},d))}));function m(e,n){var t=arguments,i=n&&n.mdxType;if("string"==typeof e||i){var r=t.length,o=new Array(r);o[0]=c;var l={};for(var p in n)hasOwnProperty.call(n,p)&&(l[p]=n[p]);l.originalType=e,l.mdxType="string"==typeof e?e:i,o[1]=l;for(var s=2;s<r;s++)o[s]=t[s];return a.createElement.apply(null,o)}return a.createElement.apply(null,t)}c.displayName="MDXCreateElement"},5266:function(e,n,t){t.r(n),t.d(n,{frontMatter:function(){return l},contentTitle:function(){return p},metadata:function(){return s},toc:function(){return d},default:function(){return c}});var a=t(3117),i=t(102),r=(t(7294),t(3905)),o=["components"],l={},p="\u4f7f\u7528\u6df7\u5408\u5e76\u884c\u8bad\u7ec3 GPT",s={unversionedId:"advanced_tutorials/train_gpt_using_hybrid_parallelism",id:"version-v0.2.2/advanced_tutorials/train_gpt_using_hybrid_parallelism",title:"\u4f7f\u7528\u6df7\u5408\u5e76\u884c\u8bad\u7ec3 GPT",description:"\u4f5c\u8005: Hongxin Liu, Yongbin Li",source:"@site/i18n/zh-Hans/docusaurus-plugin-content-docs/version-v0.2.2/advanced_tutorials/train_gpt_using_hybrid_parallelism.md",sourceDirName:"advanced_tutorials",slug:"/advanced_tutorials/train_gpt_using_hybrid_parallelism",permalink:"/zh-Hans/docs/advanced_tutorials/train_gpt_using_hybrid_parallelism",tags:[],version:"v0.2.2",frontMatter:{}},d=[{value:"\u5f15\u8a00",id:"\u5f15\u8a00",children:[],level:2},{value:"\u76ee\u5f55",id:"\u76ee\u5f55",children:[],level:2},{value:"\u5bfc\u5165\u4f9d\u8d56\u5e93",id:"\u5bfc\u5165\u4f9d\u8d56\u5e93",children:[],level:2},{value:"\u5b9a\u4e49 GPT \u6a21\u578b",id:"\u5b9a\u4e49-gpt-\u6a21\u578b",children:[],level:2},{value:"\u5904\u7406\u6570\u636e\u96c6",id:"\u5904\u7406\u6570\u636e\u96c6",children:[],level:2},{value:"\u4f7f\u7528\u6df7\u5408\u5e76\u884c\u8bad\u7ec3 GPT",id:"\u4f7f\u7528\u6df7\u5408\u5e76\u884c\u8bad\u7ec3-gpt-1",children:[],level:2}],_={toc:d};function c(e){var n=e.components,t=(0,i.Z)(e,o);return(0,r.kt)("wrapper",(0,a.Z)({},_,t,{components:n,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"\u4f7f\u7528\u6df7\u5408\u5e76\u884c\u8bad\u7ec3-gpt"},"\u4f7f\u7528\u6df7\u5408\u5e76\u884c\u8bad\u7ec3 GPT"),(0,r.kt)("p",null,"\u4f5c\u8005: Hongxin Liu, Yongbin Li"),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"\u793a\u4f8b\u4ee3\u7801")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://github.com/hpcaitech/ColossalAI-Examples/tree/main/language/gpt_2"},"ColossalAI-Examples GPT2")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://github.com/hpcaitech/ColossalAI-Examples/tree/main/language/gpt_3"},"ColossalAI-Examples GPT3"))),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"\u76f8\u5173\u8bba\u6587")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2110.14883"},"Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2104.04473"},"Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM"))),(0,r.kt)("h2",{id:"\u5f15\u8a00"},"\u5f15\u8a00"),(0,r.kt)("p",null,"\u5728\u4e0a\u4e00\u7bc7\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u5982\u4f55\u7528\u6d41\u6c34\u5e76\u884c\u8bad\u7ec3 ViT\u3002\u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u4f60\u5c06\u5b66\u4e60\u4e00\u4e2a\u66f4\u590d\u6742\u7684\u573a\u666f--\u7528\u6df7\u5408\u5e76\u884c\u65b9\u5f0f\u8bad\u7ec3GPT\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u7531\u4e8eGPT-3\u8fc7\u5927\uff0c\u5373\u4f7fCPU\u5185\u5b58\u4e5f\u65e0\u6cd5\u5bb9\u7eb3\u5b83\u3002\u56e0\u6b64\uff0c\u4f60\u5fc5\u987b\u81ea\u5df1\u5206\u5272\u6a21\u578b\u3002"),(0,r.kt)("h2",{id:"\u76ee\u5f55"},"\u76ee\u5f55"),(0,r.kt)("p",null,"\u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u4ecb\u7ecd:"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"\u57fa\u4e8e colossalai/model_zoo \u5b9a\u4e49 GPT \u6a21\u578b"),(0,r.kt)("li",{parentName:"ol"},"\u5904\u7406\u6570\u636e\u96c6"),(0,r.kt)("li",{parentName:"ol"},"\u4f7f\u7528\u6df7\u5408\u5e76\u884c\u8bad\u7ec3 GPT")),(0,r.kt)("h2",{id:"\u5bfc\u5165\u4f9d\u8d56\u5e93"},"\u5bfc\u5165\u4f9d\u8d56\u5e93"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"import json\nimport os\nfrom typing import Callable\n\nimport colossalai\nimport colossalai.utils as utils\nimport model_zoo.gpt.gpt as col_gpt\nimport torch\nimport torch.nn as nn\nfrom colossalai import nn as col_nn\nfrom colossalai.amp import AMP_TYPE\nfrom colossalai.builder.pipeline import partition_uniform\nfrom colossalai.context.parallel_mode import ParallelMode\nfrom colossalai.core import global_context as gpc\nfrom colossalai.engine.schedule import (InterleavedPipelineSchedule,\n                                        PipelineSchedule)\nfrom colossalai.logging import disable_existing_loggers, get_dist_logger\nfrom colossalai.nn.layer.wrapper import PipelineSharedModuleWrapper\nfrom colossalai.trainer import Trainer, hooks\nfrom colossalai.utils.timer import MultiTimer\nfrom model_zoo.gpt import GPTLMLoss\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset\nfrom transformers import GPT2Tokenizer\n")),(0,r.kt)("h2",{id:"\u5b9a\u4e49-gpt-\u6a21\u578b"},"\u5b9a\u4e49 GPT \u6a21\u578b"),(0,r.kt)("p",null,"\u5728\u524d\u9762\u7684\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e863\u79cd\u5efa\u7acb\u6d41\u6c34\u5e76\u884c\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u4f46\u5bf9\u4e8e\u50cf GPT-3 \u8fd9\u6837\u7684\u5de8\u5927\u6a21\u578b\uff0c\u4f60\u751a\u81f3\u4e0d\u80fd\u5728 CPU \u4e2d\u5efa\u7acb\u6a21\u578b\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u4f60\u5fc5\u987b\u81ea\u5df1\u5206\u5272\u6a21\u578b\u3002"),(0,r.kt)("p",null,"GPT \u6570\u636e\u52a0\u8f7d\u5668\u8fd4\u56de ",(0,r.kt)("inlineCode",{parentName:"p"},"input_ids")," \u548c ",(0,r.kt)("inlineCode",{parentName:"p"},"attention_mask"),", \u56e0\u6b64\u6211\u4eec\u5728 ",(0,r.kt)("inlineCode",{parentName:"p"},"forward()")," \u4e2d\u4f7f\u7528\u4e24\u4e2a\u5173\u952e\u5b57\u53c2\u6570\u6765\u83b7\u5f97\u5b83\u4eec\u3002\u8bf7\u6ce8\u610f\uff0c\u5bf9\u4e8e\u9664\u7b2c\u4e00\u9636\u6bb5\u4ee5\u5916\u7684\u5176\u4ed6\u9636\u6bb5\uff0c ",(0,r.kt)("inlineCode",{parentName:"p"},"forward()")," \u7684\u7b2c\u4e00\u4e2a\u4f4d\u7f6e\u53c2\u6570\u662f\u4e0a\u4e00\u9636\u6bb5\u7684\u8f93\u51fa\u5f20\u91cf\u3002\u6240\u4ee5 ",(0,r.kt)("inlineCode",{parentName:"p"},"hidden_states")," \u6765\u81ea\u524d\u4e00\u9636\u6bb5\uff0c\u5e76\u4e14\u5bf9\u4e8e\u7b2c\u4e00\u9636\u6bb5\u6765\u8bf4\uff0c\u5b83\u662f ",(0,r.kt)("inlineCode",{parentName:"p"},"None"),"\u3002"),(0,r.kt)("p",null,"\u5bf9\u4e8e GPT, ",(0,r.kt)("em",{parentName:"p"},"word embedding layer")," \u4e0e ",(0,r.kt)("em",{parentName:"p"},"output head")," \u5171\u4eab\u6743\u91cd\u3002\u6211\u4eec\u63d0\u4f9b ",(0,r.kt)("inlineCode",{parentName:"p"},"PipelineSharedModuleWrapper")," \u5728\u6d41\u6c34\u9636\u6bb5\u95f4\u5171\u4eab\u53c2\u6570\u3002\u5b83\u9700\u8981\u4e00\u4e2a ",(0,r.kt)("inlineCode",{parentName:"p"},"int")," \u578b\u7684 ",(0,r.kt)("inlineCode",{parentName:"p"},"list")," \u4f5c\u4e3a\u53c2\u6570, \u8fd9\u610f\u5473\u7740 rank \u4eec\u5171\u4eab\u8fd9\u4e9b\u53c2\u6570\u3002\u4f60\u53ef\u4ee5\u4f7f\u7528 ",(0,r.kt)("inlineCode",{parentName:"p"},"register_module()"),"\n\u6216 ",(0,r.kt)("inlineCode",{parentName:"p"},"register_parameter()")," \u6765\u6ce8\u518c\u4e00\u4e2a\u6a21\u5757\u6216\u4e00\u4e2a\u53c2\u6570\u4f5c\u4e3a\u5171\u4eab\u6a21\u5757\u6216\u53c2\u6570\u3002\u5982\u679c\u4f60\u6709\u591a\u7ec4\u5171\u4eab\u6a21\u5757/\u53c2\u6570\uff0c\u4f60\u5e94\u8be5\u6709\u591a\u4e2a ",(0,r.kt)("inlineCode",{parentName:"p"},"PipelineSharedModuleWrapper")," \u5b9e\u4f8b\u3002 \u5982\u679c\u53c2\u6570\u5728",(0,r.kt)("strong",{parentName:"p"},"\u4e00\u4e2a"),"\u9636\u6bb5\u5185\u5171\u4eab, \u4f60\u4e0d\u5e94\u8be5\u4f7f\u7528\n",(0,r.kt)("inlineCode",{parentName:"p"},"PipelineSharedModuleWrapper"),", \u800c\u53ea\u662f\u4f7f\u7528\u540c\u4e00\u4e2a\u6a21\u5757/\u53c2\u6570\u5b9e\u4f8b\u3002\u5728\u8fd9\u4e2a\u4f8b\u5b50\u4e2d\uff0c",(0,r.kt)("em",{parentName:"p"},"word embedding layer")," \u5728\u7b2c\u4e00\u9636\u6bb5, \u800c ",(0,r.kt)("em",{parentName:"p"},"output head")," \u5728\u6700\u540e\u4e00\u4e2a\u9636\u6bb5\u3002\u56e0\u6b64\uff0c\u4ed6\u4eec\u5728 rank ",(0,r.kt)("inlineCode",{parentName:"p"},"[0, pipeline_size - 1]")," \u4e4b\u95f4\u5171\u4eab\u53c2\u6570\u3002"),(0,r.kt)("p",null,"\u5bf9\u4e8e\u7b2c\u4e00\u9636\u6bb5\uff0c\u5b83\u7ef4\u62a4 embedding layer \u548c\u4e00\u4e9b transformer blocks\u3002\u5bf9\u4e8e\u6700\u540e\u4e00\u4e2a\u9636\u6bb5\uff0c\u5b83\u7ef4\u62a4\u4e00\u4e9b transformer blocks \u548c output head layer\u3002\u5bf9\u4e8e\u5176\u4ed6\u9636\u6bb5\uff0c\u4ed6\u4eec\u53ea\u7ef4\u62a4\u4e00\u4e9b transformer blocks\u3002\n",(0,r.kt)("inlineCode",{parentName:"p"},"partition_uniform(num_layers, pipeline_size, num_chunks)")," \u8fd4\u56de\u6240\u6709 rank \u7684 parts, part \u662f\u4e00\u4e2a ",(0,r.kt)("inlineCode",{parentName:"p"},"(start, end)")," (\u4e0d\u5305\u62ecend) \u7684 ",(0,r.kt)("inlineCode",{parentName:"p"},"tuple"),"\u3002",(0,r.kt)("inlineCode",{parentName:"p"},"start == 0")," \u8868\u793a\u8fd9\u662f\u7b2c\u4e00\u9636\u6bb5, \u800c ",(0,r.kt)("inlineCode",{parentName:"p"},"end == num_layers")," \u8868\u793a\u8fd9\u662f\u6700\u540e\u4e00\u4e2a\u9636\u6bb5\u3002"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"class PipelineGPTHybrid(nn.Module):\n    def __init__(self,\n                 num_layers: int = 12,\n                 hidden_size: int = 768,\n                 num_attention_heads: int = 12,\n                 vocab_size: int = 50304,\n                 embed_drop_rate: float = 0.,\n                 act_func: Callable = F.gelu,\n                 mlp_ratio: int = 4,\n                 attn_drop_rate: float = 0.,\n                 drop_rate: float = 0.,\n                 dtype: torch.dtype = torch.float,\n                 checkpoint: bool = False,\n                 max_position_embeddings: int = 1024,\n                 layer_norm_epsilon: float = 1e-5,\n                 first: bool = False,\n                 last: bool = False):\n        super().__init__()\n        self.embedding = None\n        self.norm = None\n        self.head = None\n        if first:\n            self.embedding = col_gpt.GPTEmbedding(\n                hidden_size, vocab_size, max_position_embeddings, dropout=embed_drop_rate, dtype=dtype)\n        self.blocks = nn.ModuleList([\n            col_gpt.GPTBlock(hidden_size, num_attention_heads, mlp_ratio=mlp_ratio, attention_dropout=attn_drop_rate,\n                             dropout=drop_rate, dtype=dtype, checkpoint=checkpoint, activation=act_func)\n            for _ in range(num_layers)\n        ])\n        if last:\n            self.norm = col_nn.LayerNorm(hidden_size, eps=layer_norm_epsilon)\n            self.head = col_gpt.GPTLMHead(vocab_size=vocab_size,\n                                          dim=hidden_size,\n                                          dtype=dtype,\n                                          bias=False)\n\n    def forward(self, hidden_states=None, input_ids=None, attention_mask=None):\n        if self.embedding is not None:\n            hidden_states = self.embedding(input_ids=input_ids)\n        batch_size = hidden_states.shape[0]\n        attention_mask = attention_mask.view(batch_size, -1)\n        attention_mask = attention_mask[:, None, None, :]\n        attention_mask = attention_mask.to(dtype=hidden_states.dtype)  # fp16 compatibility\n        attention_mask = (1.0 - attention_mask) * -10000.0\n        for block in self.blocks:\n            hidden_states, attention_mask = block(hidden_states, attention_mask)\n        if self.norm is not None:\n            hidden_states = self.head(self.norm(hidden_states))\n        return hidden_states\n\n\ndef build_gpt_pipeline(num_layers, num_chunks, device=torch.device('cuda'), **kwargs):\n    logger = get_dist_logger()\n    pipeline_size = gpc.get_world_size(ParallelMode.PIPELINE)\n    pipeline_rank = gpc.get_local_rank(ParallelMode.PIPELINE)\n    rank = gpc.get_global_rank()\n    wrapper = PipelineSharedModuleWrapper([0, pipeline_size - 1])\n    parts = partition_uniform(num_layers, pipeline_size, num_chunks)[pipeline_rank]\n    models = []\n    for start, end in parts:\n        kwargs['num_layers'] = end - start\n        kwargs['first'] = start == 0\n        kwargs['last'] = end == num_layers\n        logger.info(f'Rank{rank} build layer {start}-{end}, {end-start}/{num_layers} layers')\n        chunk = PipelineGPTHybrid(**kwargs).to(device)\n        if start == 0:\n            wrapper.register_module(chunk.embedding.word_embeddings)\n        elif end == num_layers:\n            wrapper.register_module(chunk.head)\n        models.append(chunk)\n    if len(models) == 1:\n        model = models[0]\n    else:\n        model = nn.ModuleList(models)\n    return model\n\n\ndef GPT2_exlarge_pipeline_hybrid(num_chunks=1, checkpoint=False, dtype=torch.float):\n    cfg = dict(hidden_size=1600, num_attention_heads=32, checkpoint=checkpoint, dtype=dtype)\n    return build_gpt_pipeline(48, num_chunks, **cfg)\n\n\ndef GPT3_pipeline_hybrid(num_chunks=1, checkpoint=False, dtype=torch.float):\n    cfg = dict(hidden_size=12288, num_attention_heads=96,\n               checkpoint=checkpoint, max_position_embeddings=2048, dtype=dtype)\n    return build_gpt_pipeline(96, num_chunks, **cfg)\n")),(0,r.kt)("h2",{id:"\u5904\u7406\u6570\u636e\u96c6"},"\u5904\u7406\u6570\u636e\u96c6"),(0,r.kt)("p",null,"\u6211\u4eec\u5728\u8fd9\u91cc\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5c0f\u578b GPT web-text \u6570\u636e\u96c6\u3002 \u539f\u59cb\u683c\u5f0f\u662f loose JSON, \u6211\u4eec\u5c06\u4fdd\u5b58\u5904\u7406\u540e\u7684\u6570\u636e\u96c6\u3002"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"class WebtextDataset(Dataset):\n    def __init__(self, path, seq_len=1024) -> None:\n        super().__init__()\n        root = os.path.dirname(path)\n        encoded_data_cache_path = os.path.join(root, f'gpt_webtext_{seq_len}.pt')\n        if os.path.isfile(encoded_data_cache_path):\n            seq_len_, data, attention_mask = torch.load(\n                encoded_data_cache_path)\n            if seq_len_ == seq_len:\n                self.data = data\n                self.attention_mask = attention_mask\n                return\n        raw_data = []\n        with open(path) as f:\n            for line in f.readlines():\n                raw_data.append(json.loads(line)['text'])\n        tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n        tokenizer.pad_token = tokenizer.unk_token\n        encoded_data = tokenizer(\n            raw_data, padding=True, truncation=True, max_length=seq_len, return_tensors='pt')\n        self.data = encoded_data['input_ids']\n        self.attention_mask = encoded_data['attention_mask']\n        torch.save((seq_len, self.data, self.attention_mask),\n                   encoded_data_cache_path)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        return {\n            'input_ids': self.data[index],\n            'attention_mask': self.attention_mask[index]\n        }, self.data[index]\n")),(0,r.kt)("h2",{id:"\u4f7f\u7528\u6df7\u5408\u5e76\u884c\u8bad\u7ec3-gpt-1"},"\u4f7f\u7528\u6df7\u5408\u5e76\u884c\u8bad\u7ec3 GPT"),(0,r.kt)("p",null,"\u5728\u4e0a\u4e00\u4e2a\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u89e3\u91ca\u4e86\u4e00\u4e9b\u6d41\u6c34\u5e76\u884c\u7684\u53c2\u6570\u542b\u4e49\u3002\u5728\u672c\u4f8b\u4e2d\uff0c\u6211\u4eec\u53ef\u4ee5\u786e\u5b9a\u5728\u6d41\u6c34\u9636\u6bb5\u4e4b\u95f4\u4ea4\u6362\u7684\u6bcf\u4e2a\u8f93\u51fa\u5f20\u91cf\u7684\u5f62\u72b6\u3002\u5bf9\u4e8e GPT\uff0c\u8be5\u5f62\u72b6\u4e3a\n",(0,r.kt)("inlineCode",{parentName:"p"},"(MICRO BATCH SIZE, SEQUENCE LEN, HIDDEN SIZE)"),"\u3002\u901a\u8fc7\u8bbe\u7f6e\u8be5\u53c2\u6570\uff0c\u6211\u4eec\u53ef\u4ee5\u907f\u514d\u4ea4\u6362\u6bcf\u4e2a\u9636\u6bb5\u7684\u5f20\u91cf\u5f62\u72b6\u3002\u5f53\u4f60\u4e0d\u786e\u5b9a\u5f20\u91cf\u7684\u5f62\u72b6\u65f6\uff0c\u4f60\u53ef\u4ee5\u628a\u5b83\u4fdd\u7559\u4e3a\n",(0,r.kt)("inlineCode",{parentName:"p"},"None"),", \u5f62\u72b6\u4f1a\u88ab\u81ea\u52a8\u63a8\u6d4b\u3002\u8bf7\u786e\u4fdd\u4f60\u7684\u6a21\u578b\u7684 ",(0,r.kt)("inlineCode",{parentName:"p"},"dtype")," \u662f\u6b63\u786e\u7684\uff1a\u5f53\u4f60\u4f7f\u7528 ",(0,r.kt)("inlineCode",{parentName:"p"},"fp16"),"\uff0c\u6a21\u578b\u7684 ",(0,r.kt)("inlineCode",{parentName:"p"},"dtype")," \u5fc5\u987b\u662f ",(0,r.kt)("inlineCode",{parentName:"p"},"torch.half"),"\uff1b\u5426\u5219\uff0c",(0,r.kt)("inlineCode",{parentName:"p"},"dtype")," \u5fc5\u987b\u662f ",(0,r.kt)("inlineCode",{parentName:"p"},"torch.float"),"\u3002\u5bf9\u4e8e\u6d41\u6c34\u5e76\u884c\uff0c\u4ec5\u652f\u6301 ",(0,r.kt)("inlineCode",{parentName:"p"},"AMP_TYPE.NAIVE"),"\u3002"),(0,r.kt)("p",null,"\u4f60\u53ef\u4ee5\u901a\u8fc7\u5728 ",(0,r.kt)("inlineCode",{parentName:"p"},"CONFIG")," \u91cc\u4f7f\u7528 ",(0,r.kt)("inlineCode",{parentName:"p"},"parallel")," \u6765\u8f7b\u677e\u4f7f\u7528\u5f20\u91cf\u5e76\u884c\u3002\u6570\u636e\u5e76\u884c\u7684\u5927\u5c0f\u662f\u6839\u636e GPU \u7684\u6570\u91cf\u81ea\u52a8\u8bbe\u7f6e\u7684\u3002"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"NUM_EPOCHS = 60\nSEQ_LEN = 1024\nBATCH_SIZE = 192\nNUM_CHUNKS = None\nTENSOR_SHAPE = (1, 1024, 1600)\n# only pipeline parallel\n# CONFIG = dict(NUM_MICRO_BATCHES = 192, parallel=dict(pipeline=2), fp16=dict(mode=AMP_TYPE.NAIVE))\n# pipeline + 1D model parallel\nCONFIG = dict(NUM_MICRO_BATCHES = 192, parallel=dict(pipeline=2, tensor=dict(mode='1d', size=2)), fp16=dict(mode=AMP_TYPE.NAIVE))\n\n\ndef train():\n    disable_existing_loggers()\n    parser = colossalai.get_default_parser()\n    args = parser.parse_args()\n    colossalai.launch_from_torch(config=CONFIG, backend=args.backend)\n    logger = get_dist_logger()\n\n    train_ds = WebtextDataset(os.environ['DATA'], seq_len=SEQ_LEN)\n    train_dataloader = utils.get_dataloader(train_ds,\n                                            seed=42,\n                                            batch_size=BATCH_SIZE,\n                                            pin_memory=True,\n                                            shuffle=True,\n                                            drop_last=True)\n\n    use_interleaved = NUM_CHUNKS is not None\n    num_chunks = 1 if not use_interleaved else NUM_CHUNKS\n    model = GPT2_exlarge_pipeline_hybrid(num_chunks=num_chunks, checkpoint=True, dtype=torch.half)\n    # model = GPT3_pipeline_hybrid(num_chunks=num_chunks, checkpoint=True, dtype=torch.half)\n    if use_interleaved and not isinstance(model, nn.ModuleList):\n        model = nn.ModuleList([model])\n\n    criterion = GPTLMLoss()\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.00015, weight_decay=1e-2,)\n\n    engine, train_dataloader, _, _ = colossalai.initialize(model,\n                                                           optimizer,\n                                                           criterion,\n                                                           train_dataloader=train_dataloader)\n    global_batch_size = BATCH_SIZE * \\\n        gpc.get_world_size(ParallelMode.DATA) * getattr(gpc.config, \"gradient_accumulation\", 1)\n    logger.info(f'Init done, global batch size = {global_batch_size}', ranks=[0])\n\n    timer = MultiTimer()\n\n    trainer = Trainer(\n        engine=engine,\n        logger=logger,\n        timer=timer\n    )\n\n    hook_list = [\n        hooks.LossHook(),\n        hooks.LogMetricByEpochHook(logger),\n        hooks.ThroughputHook(),\n        hooks.LogMetricByStepHook(),\n    ]\n\n    trainer.fit(\n        train_dataloader=train_dataloader,\n        epochs=NUM_EPOCHS,\n        test_interval=1,\n        hooks=hook_list,\n        display_progress=True,\n        return_output_label=False,\n    )\n")))}c.isMDXComponent=!0}}]);