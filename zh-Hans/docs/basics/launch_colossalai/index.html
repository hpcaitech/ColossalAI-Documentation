<!doctype html>
<html class="docs-version-v0.2.2" lang="zh-Hans" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.14">
<link rel="alternate" type="application/rss+xml" href="/zh-Hans/blog/rss.xml" title="Colossal-AI RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/zh-Hans/blog/atom.xml" title="Colossal-AI Atom Feed">
<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-1XKZVCCKRZ"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-1XKZVCCKRZ",{anonymize_ip:!0})</script>
<link rel="search" type="application/opensearchdescription+xml" title="Colossal-AI" href="/zh-Hans/opensearch.xml">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" integrity="sha384-Um5gpz1odJg5Z4HAmzPtgZKdTBHZdw8S29IecapCSB31ligYPhHQZMIlWLYQGVoc" crossorigin="anonymous">
<script src="https://snack.expo.io/embed.js" async></script>
<script src="https://js-eu1.hs-scripts.com/26563514.js" async defer="defer" id="hs-script-loader"></script><title data-react-helmet="true">启动 Colossal-AI | Colossal-AI</title><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:url" content="https://colossalai.org/zh-Hans/docs/basics/launch_colossalai"><meta data-react-helmet="true" name="docsearch:language" content="zh-Hans"><meta data-react-helmet="true" name="docsearch:version" content="v0.2.2"><meta data-react-helmet="true" name="docsearch:docusaurus_tag" content="docs-default-v0.2.2"><meta data-react-helmet="true" property="og:title" content="启动 Colossal-AI | Colossal-AI"><meta data-react-helmet="true" name="description" content="作者: Chuanrui Wang, Shenggui Li, Siqi Mai"><meta data-react-helmet="true" property="og:description" content="作者: Chuanrui Wang, Shenggui Li, Siqi Mai"><link data-react-helmet="true" rel="icon" href="/zh-Hans/img/favicon.ico"><link data-react-helmet="true" rel="canonical" href="https://colossalai.org/zh-Hans/docs/basics/launch_colossalai"><link data-react-helmet="true" rel="alternate" href="https://colossalai.org/docs/basics/launch_colossalai" hreflang="en"><link data-react-helmet="true" rel="alternate" href="https://colossalai.org/zh-Hans/docs/basics/launch_colossalai" hreflang="zh-Hans"><link data-react-helmet="true" rel="alternate" href="https://colossalai.org/docs/basics/launch_colossalai" hreflang="x-default"><link data-react-helmet="true" rel="preconnect" href="https://XP2V2KAOVI-dsn.algolia.net" crossorigin="anonymous"><link rel="stylesheet" href="/zh-Hans/assets/css/styles.b5c97f7b.css">
<link rel="preload" href="/zh-Hans/assets/js/runtime~main.aab5813c.js" as="script">
<link rel="preload" href="/zh-Hans/assets/js/main.255c5581.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<link rel="preconnect" href="https://fonts.googleapis.com"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&amp;display=swap" rel="stylesheet"><div class="Toastify"></div><div><a href="#" class="skipToContent_1oUP">跳到主要内容</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner InnerContainer_1wkI"><div class="navbar__items"><a class="navbar__brand" href="/zh-Hans/"><div class="navbar__logo"><img src="/zh-Hans/img/logo.svg" alt="Colossal-AI" class="themedImage_1VuW themedImage--light_3UqQ"><img src="/zh-Hans/img/logo.svg" alt="Colossal-AI" class="themedImage_1VuW themedImage--dark_hz6m"></div><b class="navbar__title"> </b></a><a class="navbar__item navbar__link" href="/zh-Hans/docs/get_started/installation">教程</a><a href="https://github.com/hpcaitech/ColossalAI/tree/main/examples" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">例子</a><a href="http://docs.colossalai.org" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">API文档</a><a href="https://www.hpc-ai.tech/blog" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">博客</a></div><div class="navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/zh-Hans/docs/advanced_tutorials/add_your_parallel">v0.2.2</a><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" class="navbar__link"><span><svg viewBox="0 0 20 20" width="20" height="20" aria-hidden="true" class="iconLanguage_3vod"><path fill="currentColor" d="M19.753 10.909c-.624-1.707-2.366-2.726-4.661-2.726-.09 0-.176.002-.262.006l-.016-2.063 3.525-.607c.115-.019.133-.119.109-.231-.023-.111-.167-.883-.188-.976-.027-.131-.102-.127-.207-.109-.104.018-3.25.461-3.25.461l-.013-2.078c-.001-.125-.069-.158-.194-.156l-1.025.016c-.105.002-.164.049-.162.148l.033 2.307s-3.061.527-3.144.543c-.084.014-.17.053-.151.143.019.09.19 1.094.208 1.172.018.08.072.129.188.107l2.924-.504.035 2.018c-1.077.281-1.801.824-2.256 1.303-.768.807-1.207 1.887-1.207 2.963 0 1.586.971 2.529 2.328 2.695 3.162.387 5.119-3.06 5.769-4.715 1.097 1.506.256 4.354-2.094 5.98-.043.029-.098.129-.033.207l.619.756c.08.096.206.059.256.023 2.51-1.73 3.661-4.515 2.869-6.683zm-7.386 3.188c-.966-.121-.944-.914-.944-1.453 0-.773.327-1.58.876-2.156a3.21 3.21 0 011.229-.799l.082 4.277a2.773 2.773 0 01-1.243.131zm2.427-.553l.046-4.109c.084-.004.166-.01.252-.01.773 0 1.494.145 1.885.361.391.217-1.023 2.713-2.183 3.758zm-8.95-7.668a.196.196 0 00-.196-.145h-1.95a.194.194 0 00-.194.144L.008 16.916c-.017.051-.011.076.062.076h1.733c.075 0 .099-.023.114-.072l1.008-3.318h3.496l1.008 3.318c.016.049.039.072.113.072h1.734c.072 0 .078-.025.062-.076-.014-.05-3.083-9.741-3.494-11.04zm-2.618 6.318l1.447-5.25 1.447 5.25H3.226z"></path></svg><span>简体中文</span></span></a><ul class="dropdown__menu"><li><a href="/docs/basics/launch_colossalai" target="_self" rel="noopener noreferrer" class="dropdown__link">English</a></li><li><a href="/zh-Hans/docs/basics/launch_colossalai" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active">简体中文</a></li></ul></div><div class="displayOnlyInLargeViewport_2uzv IconContainer_aWwG"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" class="Icon_3e04" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></div><div class="toggle_71bT toggleDisabled_3cF-"><div class="toggleTrack_32Fl" role="button" tabindex="-1"><div class="toggleTrackCheck_3lV7"><span class="toggleIcon_O4iE">🌜</span></div><div class="toggleTrackX_S2yS"><span class="toggleIcon_O4iE">🌞</span></div><div class="toggleTrackThumb_xI_Z"></div></div><input type="checkbox" class="toggleScreenReader_28Tw" aria-label="Switch between dark and light mode"></div><div class="searchBox_1ZXk"><button type="button" class="DocSearch DocSearch-Button" aria-label="搜索"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">搜索</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div aria-label="Navigation bar toggle" class="navbar__toggle" role="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="Wrapper_ReNv main-docs-wrapper"><div class="docPage_3AUJ"><button aria-label="回到顶部" class="clean-btn theme-back-to-top-button backToTopButton_35hR" type="button"></button><main class="docMainContainer_2AUC"><div class="padding-vert--lg container docItemWrapper_1WZa"><div class="row"><div class="col docItemCol_3FnS"><div class="docItemContainer_33ec"><article><div class="tocCollapsible_1PrD theme-doc-toc-mobile tocMobile_3Hoh"><button type="button" class="clean-btn tocCollapsibleButton_2O1e">本页总览</button></div><div class="theme-doc-markdown markdown"><header><h1>启动 Colossal-AI</h1></header><p>作者: Chuanrui Wang, Shenggui Li, Siqi Mai</p><p><strong>预备知识:</strong></p><ul><li><a href="/zh-Hans/docs/concepts/distributed_training">分布式训练</a></li><li><a href="/zh-Hans/docs/concepts/colossalai_overview">Colossal-AI 总览</a></li></ul><h2 class="anchor anchorWithHideOnScrollNavbar_3R7-" id="简介">简介<a class="hash-link" href="#简介" title="标题的直接链接">​</a></h2><p>正如我们在前面的教程中所提到的，在您的配置文件准备好后，您需要为 Colossal-AI 初始化分布式环境。我们把这个过程称为 <code>launch</code>。在本教程中，您将学习如何在您的服务器上启动 Colossal-AI，不管是小型的还是大型的。</p><p>在 Colossal-AI 中，我们提供了几种启动方法来初始化分布式后端。
在大多数情况下，您可以使用 <code>colossalai.launch</code> 和 <code>colossalai.get_default_parser</code> 来通过命令行传递参数。如果您想使用 SLURM、OpenMPI 和 PyTorch 等启动工具，我们也提供了几个启动的辅助方法以便您的使用。您可以直接从这些启动工具设置的环境变量中访问 rank 和 world size 大小。</p><p>在本教程中，我们将介绍如何启动 Colossal-AI 来初始化分布式后端：</p><ul><li>用 colossalai.launch 启动</li><li>用 Colossal-AI命令行 启动</li><li>用 SLURM 启动</li><li>用 OpenMPI 启动</li></ul><h2 class="anchor anchorWithHideOnScrollNavbar_3R7-" id="启动分布式环境">启动分布式环境<a class="hash-link" href="#启动分布式环境" title="标题的直接链接">​</a></h2><p>为了启动 Colossal-AI，我们需要两类参数:</p><ol><li>配置文件</li><li>分布式设置</li></ol><p>无论我们使用何种启动方式，配置文件是必须要求的，而分布式设置有可能依情况而定。配置文件可以是配置文件的路径或 Python dictionary 的形式。分布式设置可以通过命令行或多进程启动器传递。</p><h3 class="anchor anchorWithHideOnScrollNavbar_3R7-" id="命令行解析器">命令行解析器<a class="hash-link" href="#命令行解析器" title="标题的直接链接">​</a></h3><p>在使用 <code>launch</code> 之前, 我们首先需要了解我们需要哪些参数来进行初始化。
如<a href="/zh-Hans/docs/concepts/distributed_training">分布式训练</a> 中 <code>基本概念</code> 一节所述 ，涉及的重要参数是:</p><ol><li>host</li><li>port</li><li>rank</li><li>world_size</li><li>backend</li></ol><p>在 Colossal-AI 中，我们提供了一个命令行解析器，它已经提前添加了这些参数。您可以通过调用 <code>colossalai.get_default_parser()</code> 来获得这个解析器。这个解析器通常与 <code>colossalai.launch</code> 一起使用。</p><div class="codeBlockContainer_K1bP language-python theme-code-block"><div class="codeBlockContent_hGly python"><pre tabindex="0" class="prism-code language-python codeBlock_23N8 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_39YC"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># add these lines in your train.py</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> colossalai</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># get default parser</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">parser </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> colossalai</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">get_default_parser</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># if you want to add your own arguments</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">parser</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">add_argument</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">.</span><span class="token punctuation" style="color:#393A34">.</span><span class="token punctuation" style="color:#393A34">.</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># parse arguments</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">args </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> parser</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">parse_args</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><button type="button" aria-label="复制代码到剪贴板" class="copyButton_Ue-o clean-btn">复制</button></div></div><p>您可以在您的终端传入以下这些参数。</p><div class="codeBlockContainer_K1bP language-shell theme-code-block"><div class="codeBlockContent_hGly shell"><pre tabindex="0" class="prism-code language-shell codeBlock_23N8 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_39YC"><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">python train.py --host </span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain">host</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"> --rank </span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain">rank</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"> --world_size </span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain">world_size</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"> --port </span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain">port</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"> --backend </span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain">backend</span><span class="token operator" style="color:#393A34">&gt;</span><br></span></code></pre><button type="button" aria-label="复制代码到剪贴板" class="copyButton_Ue-o clean-btn">复制</button></div></div><p><code>backend</code> 是用户可选的，默认值是 nccl。</p><h3 class="anchor anchorWithHideOnScrollNavbar_3R7-" id="本地启动">本地启动<a class="hash-link" href="#本地启动" title="标题的直接链接">​</a></h3><p>为了初始化分布式环境，我们提供了一个通用的 <code>colossalai.launch</code> API。<code>colossalai.launch</code> 函数接收上面列出的参数，并在通信网络中创建一个默认的进程组。方便起见，这个函数通常与默认解析器一起使用。</p><div class="codeBlockContainer_K1bP language-python theme-code-block"><div class="codeBlockContent_hGly python"><pre tabindex="0" class="prism-code language-python codeBlock_23N8 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_39YC"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> colossalai</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># parse arguments</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">args </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> colossalai</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">get_default_parser</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">parse_args</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># launch distributed environment</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">colossalai</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">launch</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">config</span><span class="token operator" style="color:#393A34">=</span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain">CONFIG</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                  rank</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">args</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">rank</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                  world_size</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">args</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">world_size</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                  host</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">args</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">host</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                  port</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">args</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">port</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                  backend</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">args</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">backend</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span></code></pre><button type="button" aria-label="复制代码到剪贴板" class="copyButton_Ue-o clean-btn">复制</button></div></div><h3 class="anchor anchorWithHideOnScrollNavbar_3R7-" id="用-colossal-ai命令行工具-启动">用 Colossal-AI命令行工具 启动<a class="hash-link" href="#用-colossal-ai命令行工具-启动" title="标题的直接链接">​</a></h3><p>为了更好地支持单节点以及多节点的训练，我们通过封装PyTorch的启动器实现了一个更加方便的启动器。
PyTorch自带的启动器需要在每个节点上都启动命令才能启动多节点训练，而我们的启动器只需要一次调用即可启动训练。</p><p>首先，我们需要在代码里指定我们的启动方式。由于这个启动器是PyTorch启动器的封装，那么我们自然而然应该使用<code>colossalai.launch_from_torch</code>。
分布式环境所需的参数，如 rank, world size, host 和 port 都是由 PyTorch 启动器设置的，可以直接从环境变量中读取。</p><div class="codeBlockContainer_K1bP language-python theme-code-block"><div class="codeBlockContent_hGly python"><pre tabindex="0" class="prism-code language-python codeBlock_23N8 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_39YC"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> colossalai</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">colossalai</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">launch_from_torch</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    config</span><span class="token operator" style="color:#393A34">=</span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain">CONFIG</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><button type="button" aria-label="复制代码到剪贴板" class="copyButton_Ue-o clean-btn">复制</button></div></div><p>接下来，我们可以轻松地在终端使用<code>colossalai run</code>来启动训练。下面的命令可以在当前机器上启动一个4卡的训练任务。
你可以通过设置<code>nproc_per_node</code>来调整使用的GPU的数量，也可以改变<code>master_port</code>的参数来选择通信的端口。</p><div class="codeBlockContainer_K1bP language-shell theme-code-block"><div class="codeBlockContent_hGly shell"><pre tabindex="0" class="prism-code language-shell codeBlock_23N8 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_39YC"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># 在当前节点上启动4卡训练 （默认使用29500端口）</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">colossalai run --nproc_per_node </span><span class="token number" style="color:#36acaa">4</span><span class="token plain"> train.py</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># 在当前节点上启动4卡训练，并使用一个不同的端口</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">colossalai run --nproc_per_node </span><span class="token number" style="color:#36acaa">4</span><span class="token plain"> --master_port </span><span class="token number" style="color:#36acaa">29505</span><span class="token plain"> test.py</span><br></span></code></pre><button type="button" aria-label="复制代码到剪贴板" class="copyButton_Ue-o clean-btn">复制</button></div></div><p>如果你在使用一个集群，并且想进行多节点的训练，你需要使用Colossal-AI的命令行工具进行一键启动。我们提供了两种方式来启动多节点任务</p><ul><li>通过<code>--hosts</code>来启动</li></ul><p>这个方式适合节点数不多的情况。假设我们有两个节点，分别为<code>host</code>和<code>host2</code>。我们可以用以下命令进行多节点训练。
比起单节点训练，多节点训练需要手动设置<code>--master_addr</code> （在单节点训练中<code>master_addr</code>默认为<code>127.0.0.1</code>）。</p><div class="admonition admonition-caution alert alert--warning"><div class="admonition-heading"><h5><span class="admonition-icon">⚠️</span>caution</h5></div><div class="admonition-content"><p>多节点训练时，<code>master_addr</code>不能为<code>localhost</code>或者<code>127.0.0.1</code>，它应该是一个节点的名字或者IP地址。</p></div></div><div class="codeBlockContainer_K1bP language-shell theme-code-block"><div class="codeBlockContent_hGly shell"><pre tabindex="0" class="prism-code language-shell codeBlock_23N8 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_39YC"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># 在两个节点上训练</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">colossalai run --nproc_per_node </span><span class="token number" style="color:#36acaa">4</span><span class="token plain"> --host host1,host2 --master_addr host1 test.py</span><br></span></code></pre><button type="button" aria-label="复制代码到剪贴板" class="copyButton_Ue-o clean-btn">复制</button></div></div><ul><li>通过<code>--hostfile</code>来启动</li></ul><p>这个方式适用于节点数很大的情况。host file是一个简单的文本文件，这个文件里列出了可以使用的节点的名字。
在一个集群中，可用节点的列表一般由SLURM或者PBS Pro这样的集群资源管理器来提供。比如，在SLURM中，
你可以从<code>SLURM_NODELIST</code>这个环境变量中获取到当前分配列表。在PBS Pro中，这个环境变量为<code>PBS_NODEFILE</code>。
可以通过<code>echo $SLURM_NODELIST</code> 或者 <code>cat $PBS_NODEFILE</code> 来尝试一下。如果你没有这样的集群管理器，
那么你可以自己手动写一个这样的文本文件即可。</p><p>提供给Colossal-AI的host file需要遵循以下格式，每一行都是一个节点的名字。</p><div class="codeBlockContainer_K1bP language-text theme-code-block"><div class="codeBlockContent_hGly text"><pre tabindex="0" class="prism-code language-text codeBlock_23N8 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_39YC"><span class="token-line" style="color:#393A34"><span class="token plain">host1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">host2</span><br></span></code></pre><button type="button" aria-label="复制代码到剪贴板" class="copyButton_Ue-o clean-btn">复制</button></div></div><p>如果host file准备好了，那么我们就可以用以下命令开始多节点训练了。和使用<code>--host</code>一样，你也需要指定一个<code>master_addr</code>。
当使用host file时，我们可以使用一些额外的参数：</p><ul><li><p><code>--include</code>: 设置你想要启动训练的节点。比如，你的host file里有8个节点，但是你只想用其中的6个节点进行训练，
你可以添加<code>--include host1,host2,host3,...,host6</code>，这样训练任务只会在这6个节点上启动。</p></li><li><p><code>--exclude</code>: 设置你想排除在训练之外的节点。当你的某一些节点坏掉时，这个参数会比较有用。比如假如host1的GPU有一些问题，无法正常使用，
那么你就可以使用<code>--exclude host1</code>来将其排除在外，这样你就可以训练任务就只会在剩余的节点上启动。</p></li></ul><div class="codeBlockContainer_K1bP language-shell theme-code-block"><div class="codeBlockContent_hGly shell"><pre tabindex="0" class="prism-code language-shell codeBlock_23N8 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_39YC"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># 使用hostfile启动</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">colossalai run --nproc_per_node </span><span class="token number" style="color:#36acaa">4</span><span class="token plain"> --hostfile ./hostfile --master_addr host1  test.py</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># 只使用部分节点进行训练</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">colossalai run --nproc_per_node </span><span class="token number" style="color:#36acaa">4</span><span class="token plain"> --hostfile ./hostfile --master_addr host1  --include host1 test.py</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># 不使用某些节点进行训练</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">colossalai run --nproc_per_node </span><span class="token number" style="color:#36acaa">4</span><span class="token plain"> --hostfile ./hostfile --master_addr host1  --exclude host2 test.py</span><br></span></code></pre><button type="button" aria-label="复制代码到剪贴板" class="copyButton_Ue-o clean-btn">复制</button></div></div><h3 class="anchor anchorWithHideOnScrollNavbar_3R7-" id="用-slurm-启动">用 SLURM 启动<a class="hash-link" href="#用-slurm-启动" title="标题的直接链接">​</a></h3><p>如果您是在一个由 SLURM 调度器管理的系统上， 您也可以使用 <code>srun</code> 启动器来启动您的 Colossal-AI 脚本。我们提供了辅助函数 <code>launch_from_slurm</code> 来与 SLURM 调度器兼容。
<code>launch_from_slurm</code> 会自动从环境变量 <code>SLURM_PROCID</code> 和 <code>SLURM_NPROCS</code> 中分别读取 rank 和 world size ，并使用它们来启动分布式后端。</p><p>您可以在您的训练脚本中尝试以下操作。</p><div class="codeBlockContainer_K1bP language-python theme-code-block"><div class="codeBlockContent_hGly python"><pre tabindex="0" class="prism-code language-python codeBlock_23N8 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_39YC"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> colossalai</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">colossalai</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">launch_from_slurm</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    config</span><span class="token operator" style="color:#393A34">=</span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain">CONFIG</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    host</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">args</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">host</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    port</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">args</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">port</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><button type="button" aria-label="复制代码到剪贴板" class="copyButton_Ue-o clean-btn">复制</button></div></div><p>您可以通过在终端使用这个命令来初始化分布式环境。</p><div class="codeBlockContainer_K1bP language-bash theme-code-block"><div class="codeBlockContent_hGly bash"><pre tabindex="0" class="prism-code language-bash codeBlock_23N8 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_39YC"><span class="token-line" style="color:#393A34"><span class="token plain">srun python train.py --host </span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain">master_node</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"> --port </span><span class="token number" style="color:#36acaa">29500</span><br></span></code></pre><button type="button" aria-label="复制代码到剪贴板" class="copyButton_Ue-o clean-btn">复制</button></div></div><h3 class="anchor anchorWithHideOnScrollNavbar_3R7-" id="用-openmpi-启动">用 OpenMPI 启动<a class="hash-link" href="#用-openmpi-启动" title="标题的直接链接">​</a></h3><p>如果您对OpenMPI比较熟悉，您也可以使用 <code>launch_from_openmpi</code> 。
<code>launch_from_openmpi</code> 会自动从环境变量
<code>OMPI_COMM_WORLD_LOCAL_RANK</code>， <code>MPI_COMM_WORLD_RANK</code> 和 <code>OMPI_COMM_WORLD_SIZE</code> 中分别读取local rank、global rank 和 world size，并利用它们来启动分布式后端。</p><p>您可以在您的训练脚本中尝试以下操作。</p><div class="codeBlockContainer_K1bP language-python theme-code-block"><div class="codeBlockContent_hGly python"><pre tabindex="0" class="prism-code language-python codeBlock_23N8 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_39YC"><span class="token-line" style="color:#393A34"><span class="token plain">colossalai</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">launch_from_openmpi</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    config</span><span class="token operator" style="color:#393A34">=</span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain">CONFIG</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    host</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">args</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">host</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    port</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">args</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">port</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><button type="button" aria-label="复制代码到剪贴板" class="copyButton_Ue-o clean-btn">复制</button></div></div><p>以下是用 OpenMPI 启动多个进程的示例命令。</p><div class="codeBlockContainer_K1bP language-bash theme-code-block"><div class="codeBlockContent_hGly bash"><pre tabindex="0" class="prism-code language-bash codeBlock_23N8 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_39YC"><span class="token-line" style="color:#393A34"><span class="token plain">mpirun --hostfile </span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain">my_hostfile</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"> -np </span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain">num_process</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"> python train.py --host </span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain">node name or ip</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"> --port </span><span class="token number" style="color:#36acaa">29500</span><br></span></code></pre><button type="button" aria-label="复制代码到剪贴板" class="copyButton_Ue-o clean-btn">复制</button></div></div><ul><li>--hostfile: 指定一个要运行的主机列表。</li><li>--np: 设置总共要启动的进程（GPU）的数量。例如，如果 --np 4，4个 python 进程将被初始化以运行 train.py。</li></ul></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="文档分页导航"><div class="pagination-nav__item"></div><div class="pagination-nav__item pagination-nav__item--next"></div></nav></div></div><div class="col col--3"><div class="tableOfContents_35-E thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#简介" class="table-of-contents__link toc-highlight">简介</a></li><li><a href="#启动分布式环境" class="table-of-contents__link toc-highlight">启动分布式环境</a><ul><li><a href="#命令行解析器" class="table-of-contents__link toc-highlight">命令行解析器</a></li><li><a href="#本地启动" class="table-of-contents__link toc-highlight">本地启动</a></li><li><a href="#用-colossal-ai命令行工具-启动" class="table-of-contents__link toc-highlight">用 Colossal-AI命令行工具 启动</a></li><li><a href="#用-slurm-启动" class="table-of-contents__link toc-highlight">用 SLURM 启动</a></li><li><a href="#用-openmpi-启动" class="table-of-contents__link toc-highlight">用 OpenMPI 启动</a></li></ul></li></ul></div></div></div></div></main></div></div><footer class="footer Container_ZO7N"><div class="InnerContainer_kEOB"><div class="ContentContainer_Bd2W"><div class="FooterLeft_UU7Q"><div class="BrandContainer_37NB"><img class="BrandImage_1lbV" alt="AgileTs Logo" height="30" src="/img/logo.svg"></div><div class="Tagline_2AGk">具有高效并行化技术的集成大规模模型训练系统</div><button class="ButtonContainer_oPl2 GithubButton_1Y3L"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" class="GithubIcon_3htU" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg><div>GITHUB</div></button></div><div class="FooterRight_1NHD"><div class="SectionContainer_2QT6"><li class="LinkItemTitle_1y09">资源</li><ul class="LinkItemContainer_2Lmc"><a class="LinkText_1B0E" href="/zh-Hans/docs/get_started/installation" label="教程" to="docs/get_started/installation">教程</a></ul><ul class="LinkItemContainer_2Lmc"><a class="LinkText_1B0E" href="http://docs.colossalai.org" label="API文档" to="http://docs.colossalai.org">API文档</a></ul><ul class="LinkItemContainer_2Lmc"><a class="LinkText_1B0E" href="https://github.com/hpcaitech/ColossalAI/tree/main/examples" label="例子" to="https://github.com/hpcaitech/ColossalAI/tree/main/examples">例子</a></ul><ul class="LinkItemContainer_2Lmc"><a class="LinkText_1B0E" href="https://github.com/hpcaitech/ColossalAI/discussions" label="论坛" to="https://github.com/hpcaitech/ColossalAI/discussions">论坛</a></ul></div><div class="SectionContainer_2QT6"><li class="LinkItemTitle_1y09">社区</li><ul class="LinkItemContainer_2Lmc"><a class="LinkText_1B0E" href="https://github.com/hpcaitech/ColossalAI" rel="noopener noreferrer" target="_blank" label="GitHub">GitHub</a></ul><ul class="LinkItemContainer_2Lmc"><a class="LinkText_1B0E" href="https://www.hpc-ai.tech/blog" rel="noopener noreferrer" target="_blank" label="博客">博客</a></ul><ul class="LinkItemContainer_2Lmc"><a class="LinkText_1B0E" href="https://twitter.com/HPCAITech" rel="noopener noreferrer" target="_blank" label="Twitter">Twitter</a></ul></div></div></div><div class="BottomContainer_1let"><div class="CopyrightText_25Fq">Copyright © 2023 All Rights Reserved by Luchen Technology Inc.</div></div></div></footer></div>
<script src="/zh-Hans/assets/js/runtime~main.aab5813c.js"></script>
<script src="/zh-Hans/assets/js/main.255c5581.js"></script>
</body>
</html>