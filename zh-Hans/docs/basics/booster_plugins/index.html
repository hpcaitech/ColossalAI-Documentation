<!doctype html>
<html lang="zh-Hans" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-basics/booster_plugins">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.3.1">
<title data-rh="true">Booster 插件 | Colossal-AI</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://colossalai.org/zh-Hans/img/social-card.png"><meta data-rh="true" name="twitter:image" content="https://colossalai.org/zh-Hans/img/social-card.png"><meta data-rh="true" property="og:url" content="https://colossalai.org/zh-Hans/docs/basics/booster_plugins"><meta data-rh="true" name="docusaurus_locale" content="zh-Hans"><meta data-rh="true" name="docsearch:language" content="zh-Hans"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Booster 插件 | Colossal-AI"><meta data-rh="true" name="description" content="作者: Hongxin Liu, Baizhou Zhang, Pengtai Xu"><meta data-rh="true" property="og:description" content="作者: Hongxin Liu, Baizhou Zhang, Pengtai Xu"><link data-rh="true" rel="icon" href="/zh-Hans/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://colossalai.org/zh-Hans/docs/basics/booster_plugins"><link data-rh="true" rel="alternate" href="https://colossalai.org/docs/basics/booster_plugins" hreflang="en"><link data-rh="true" rel="alternate" href="https://colossalai.org/zh-Hans/docs/basics/booster_plugins" hreflang="zh-Hans"><link data-rh="true" rel="alternate" href="https://colossalai.org/docs/basics/booster_plugins" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://XP2V2KAOVI-dsn.algolia.net" crossorigin="anonymous"><link rel="alternate" type="application/rss+xml" href="/zh-Hans/blog/rss.xml" title="Colossal-AI RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/zh-Hans/blog/atom.xml" title="Colossal-AI Atom Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-1XKZVCCKRZ"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-1XKZVCCKRZ",{anonymize_ip:!0})</script>


<link rel="search" type="application/opensearchdescription+xml" title="Colossal-AI" href="/zh-Hans/opensearch.xml">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous">
<script src="https://js-eu1.hs-scripts.com/26563514.js" id="hs-script-loader" async defer="defer"></script><link rel="stylesheet" href="/zh-Hans/assets/css/styles.9a2d694b.css">
<link rel="preload" href="/zh-Hans/assets/js/runtime~main.6961964e.js" as="script">
<link rel="preload" href="/zh-Hans/assets/js/main.10afeca0.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="跳到主要内容"><a class="skipToContent_fXgn" href="#docusaurus_skipToContent_fallback">跳到主要内容</a></div><nav aria-label="主导航" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="切换导航栏" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/zh-Hans/"><div class="navbar__logo"><img src="/zh-Hans/img/logo.svg" alt="project-logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/zh-Hans/img/logo.svg" alt="project-logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">Colossal-AI</b></a><a class="navbar__item navbar__link" href="/zh-Hans/docs/get_started/installation">教程</a><a href="https://github.com/hpcaitech/ColossalAI/tree/main/examples" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">例子</a><a href="https://www.hpc-ai.tech/blog" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">博客</a></div><div class="navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/zh-Hans/docs/get_started/installation">Next</a><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_nlXk"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>简体中文</a><ul class="dropdown__menu"><li><a href="/docs/basics/booster_plugins" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="en">English</a></li><li><a href="/zh-Hans/docs/basics/booster_plugins" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="zh-Hans">简体中文</a></li></ul></div><a href="https://github.com/hpcaitech/ColossalAI" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-github-link"></a><div class="searchBox_ZlJk"><button type="button" class="DocSearch DocSearch-Button" aria-label="搜索"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">搜索</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="回到顶部" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd"><nav aria-label="文档侧边栏" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/zh-Hans/docs/get_started/installation">快速开始</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/zh-Hans/docs/concepts/distributed_training">概念</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/zh-Hans/docs/basics/command_line_tool">基础</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/zh-Hans/docs/basics/command_line_tool">命令行工具</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/zh-Hans/docs/basics/launch_colossalai">启动 Colossal-AI</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/zh-Hans/docs/basics/booster_api">Booster API</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/zh-Hans/docs/basics/booster_plugins">Booster 插件</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/zh-Hans/docs/basics/booster_checkpoint">Booster Checkpoint</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/zh-Hans/docs/features/shardformer">Features</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/zh-Hans/docs/advanced_tutorials/train_vit_with_hybrid_parallelism">高级教程</a></div></li></ul></nav></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="页面路径"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="主页面" class="breadcrumbs__link" href="/zh-Hans/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">基础</span><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Booster 插件</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">本页总览</button></div><div class="theme-doc-markdown markdown"><h1>Booster 插件</h1><p>作者: <a href="https://github.com/ver217" target="_blank" rel="noopener noreferrer">Hongxin Liu</a>, <a href="https://github.com/Fridge003" target="_blank" rel="noopener noreferrer">Baizhou Zhang</a>, <a href="https://github.com/ppt0011" target="_blank" rel="noopener noreferrer">Pengtai Xu</a></p><p><strong>前置教程:</strong></p><ul><li><a href="/zh-Hans/docs/basics/booster_api">Booster API</a></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="引言">引言<a href="#引言" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p>正如 <a href="/zh-Hans/docs/basics/booster_api">Booster API</a> 中提到的，我们可以使用 booster 插件来自定义并行训练。在本教程中，我们将介绍如何使用 booster 插件。</p><p>我们现在提供以下插件:</p><ul><li><a href="#torch-ddp-%E6%8F%92%E4%BB%B6">Torch DDP 插件</a>: 它包装了 <code>torch.nn.parallel.DistributedDataParallel</code> 并且可用于使用数据并行训练模型。</li><li><a href="#torch-fsdp-%E6%8F%92%E4%BB%B6">Torch FSDP 插件</a>: 它包装了 <code>torch.distributed.fsdp.FullyShardedDataParallel</code> 并且可用于使用 Zero-dp 训练模型。</li><li><a href="#low-level-zero-%E6%8F%92%E4%BB%B6">Low Level Zero 插件</a>: 它包装了 <code>colossalai.zero.low_level.LowLevelZeroOptimizer</code>，可用于使用 Zero-dp 训练模型。它仅支持 Zero 阶段1和阶段2。</li><li><a href="#gemini-%E6%8F%92%E4%BB%B6">Gemini 插件</a>: 它包装了 <a href="/zh-Hans/docs/features/zero_with_chunk">Gemini</a>，Gemini 实现了基于Chunk内存管理和异构内存管理的 Zero-3。</li><li><a href="#hybrid-parallel-%E6%8F%92%E4%BB%B6">Hybrid Pararllel 插件</a>: 它为Shardformer，流水线管理器，混合精度运算，TorchDDP以及Zero-1/Zero-2功能提供了一个统一且简洁的接口。使用该插件可以简单高效地实现transformer模型在张量并行，流水线并行以及数据并行（DDP, Zero）间任意组合并行训练策略，同时支持多种训练速度和内存的优化工具。有关这些训练策略和优化工具的具体信息将在下一章中阐述。</li></ul><p>更多插件即将推出。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="插件选择">插件选择<a href="#插件选择" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><ul><li><a href="#torch-ddp-%E6%8F%92%E4%BB%B6">Torch DDP 插件</a>: 适用于参数少于 20 亿的模型（例如 Bert-3m、GPT2-1.5b）。</li><li><a href="#torch-fsdp-%E6%8F%92%E4%BB%B6">Torch FSDP 插件</a> / <a href="#low-level-zero-%E6%8F%92%E4%BB%B6">Low Level Zero 插件</a>: 适用于参数少于 100 亿的模型（例如 GPTJ-6b、MegatronLM-8b）。</li><li><a href="#gemini-%E6%8F%92%E4%BB%B6">Gemini 插件</a>: 适合参数超过 100 亿的模型（例如 TuringNLG-17b），且<strong>跨节点带宽高、中小规模集群（千卡以下）</strong>的场景（例如 Llama2-70b）。</li><li><a href="#hybrid-parallel-%E6%8F%92%E4%BB%B6">Hybrid Pararllel 插件</a>: 适合参数超过 600 亿的模型、超长序列、超大词表等特殊模型，且<strong>跨节点带宽低、大规模集群（千卡以上）</strong>的场景（例如 GPT3-175b、Bloom-176b）。</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="插件">插件<a href="#插件" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="low-level-zero-插件">Low Level Zero 插件<a href="#low-level-zero-插件" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p>该插件实现了 Zero-1 和 Zero-2（使用/不使用 CPU 卸载），使用<code>reduce</code>和<code>gather</code>来同步梯度和权重。</p><p>Zero-1 可以看作是 Torch DDP 更好的替代品，内存效率更高，速度更快。它可以很容易地用于混合并行。</p><p>Zero-2 不支持局部梯度累积。如果您坚持使用，虽然可以积累梯度，但不能降低通信成本。也就是说，同时使用流水线并行和 Zero-2 并不是一个好主意。</p><div class="docstring-container"><div><div class="title-container"><div class="title-module"><h5>class</h5>  <h3>colossalai.booster.plugin.LowLevelZeroPlugin</h3></div><div class="title-source">&lt;<a href="https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/booster/plugin/low_level_zero_plugin.py#L213" class="title-source">source</a>&gt;</div></div><div class="signature">(<!-- -->stage: int = 1, precision: str = &#x27;fp16&#x27;, initial_scale: float = 4294967296, min_scale: float = 1, growth_factor: float = 2, backoff_factor: float = 0.5, growth_interval: int = 1000, hysteresis: int = 2, max_scale: float = 4294967296, max_norm: float = 0.0, norm_type: float = 2.0, reduce_bucket_size_in_m: int = 12, communication_dtype: typing.Optional[torch.dtype] = None, overlap_communication: bool = True, cpu_offload: bool = False, master_weights: bool = True, verbose: bool = False<!-- -->)</div><div><div class="divider"><span class="divider-text">Parameters</span></div><ul>
<li><strong>stage</strong> (int, optional) -- ZeRO stage. Defaults to 1.</li>
<li><strong>precision</strong> (str, optional) -- precision. Support &#x27;fp16&#x27;, &#x27;bf16&#x27; and &#x27;fp32&#x27;. Defaults to &#x27;fp16&#x27;.</li>
<li><strong>initial_scale</strong> (float, optional) -- Initial scale used by DynamicGradScaler. Defaults to 2**32.</li>
<li><strong>min_scale</strong> (float, optional) -- Min scale used by DynamicGradScaler. Defaults to 1.</li>
<li><strong>growth_factor</strong> (float, optional) -- growth_factor used by DynamicGradScaler. Defaults to 2.</li>
<li><strong>backoff_factor</strong> (float, optional) -- backoff_factor used by DynamicGradScaler. Defaults to 0.5.</li>
<li><strong>growth_interval</strong> (float, optional) -- growth_interval used by DynamicGradScaler. Defaults to 1000.</li>
<li><strong>hysteresis</strong> (float, optional) -- hysteresis used by DynamicGradScaler. Defaults to 2.</li>
<li><strong>max_scale</strong> (int, optional) -- max_scale used by DynamicGradScaler. Defaults to 2**32.</li>
<li><strong>max_norm</strong> (float, optional) -- max_norm used for <code>clip_grad_norm</code>. You should notice that you shall not do
clip_grad_norm by yourself when using ZeRO DDP. The ZeRO optimizer will take care of clip_grad_norm.</li>
<li><strong>norm_type</strong> (float, optional) -- norm_type used for <code>clip_grad_norm</code>.</li>
<li><strong>reduce_bucket_size_in_m</strong> (int, optional) -- grad reduce bucket size in M. Defaults to 12.</li>
<li><strong>communication_dtype</strong> (torch.dtype, optional) -- communication dtype. If not specified, the dtype of param will be used. Defaults to None.</li>
<li><strong>overlap_communication</strong> (bool, optional) -- whether to overlap communication and computation. Defaults to True.</li>
<li><strong>cpu_offload</strong> (bool, optional) -- whether to offload grad, master weight and optimizer state to cpu. Defaults to False.</li>
<li><strong>verbose</strong> (bool, optional) -- verbose mode. Debug info including grad overflow will be printed. Defaults to False.</li>
</ul></div></div><div><div class="divider"><span class="divider-text">Description</span></div><p>Plugin for low level zero.</p><div><div class="divider"><span class="divider-text">Example</span></div><pre><code class="language-python">from colossalai.booster import Booster
from colossalai.booster.plugin import LowLevelZeroPlugin

model, train_dataset, optimizer, criterion = ...
plugin = LowLevelZeroPlugin()

train_dataloader = plugin.prepare_dataloader(train_dataset, batch_size=8)
booster = Booster(plugin=plugin)
model, optimizer, train_dataloader, criterion = booster.boost(model, optimizer, train_dataloader, criterion)
</code></pre></div></div></div><p>我们已经测试了一些主流模型的兼容性，可能不支持以下模型：</p><ul><li><code>timm.models.convit_base</code></li><li>dlrm and deepfm models in <code>torchrec</code></li></ul><p>兼容性问题将在未来修复。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="gemini-插件">Gemini 插件<a href="#gemini-插件" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p>这个插件实现了基于Chunk内存管理和异构内存管理的 Zero-3。它可以训练大型模型而不会损失太多速度。它也不支持局部梯度累积。更多详细信息，请参阅 <a href="/zh-Hans/docs/features/zero_with_chunk">Gemini 文档</a>.</p><div class="docstring-container"><div><div class="title-container"><div class="title-module"><h5>class</h5>  <h3>colossalai.booster.plugin.GeminiPlugin</h3></div><div class="title-source">&lt;<a href="https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/booster/plugin/gemini_plugin.py#L228" class="title-source">source</a>&gt;</div></div><div class="signature">(<!-- -->chunk_config_dict: typing.Optional[dict] = None, chunk_init_device: typing.Optional[torch.device] = None, placement_policy: str = &#x27;static&#x27;, enable_gradient_accumulation: bool = False, shard_param_frac: float = 1.0, offload_optim_frac: float = 0.0, offload_param_frac: float = 0.0, warmup_non_model_data_ratio: float = 0.8, steady_cuda_cap_ratio: float = 0.9, precision: str = &#x27;fp16&#x27;, master_weights: bool = True, pin_memory: bool = False, force_outputs_fp32: bool = False, strict_ddp_mode: bool = False, search_range_m: int = 32, hidden_dim: typing.Optional[int] = None, min_chunk_size_m: float = 32, memstats: typing.Optional[colossalai.zero.gemini.memory_tracer.memory_stats.MemStats] = None, gpu_margin_mem_ratio: float = 0.0, initial_scale: float = 65536, min_scale: float = 1, growth_factor: float = 2, backoff_factor: float = 0.5, growth_interval: int = 1000, hysteresis: int = 2, max_scale: float = 4294967296, max_norm: float = 0.0, norm_type: float = 2.0, verbose: bool = False<!-- -->)</div><div><div class="divider"><span class="divider-text">Parameters</span></div><ul>
<li><strong>chunk_config_dict</strong> (dict, optional) -- chunk configuration dictionary.</li>
<li><strong>chunk_init_device</strong> (torch.device, optional) -- device to initialize the chunk.</li>
<li><strong>placement_policy</strong> (str, optional) -- &quot;static&quot; and &quot;auto&quot;. Defaults to &quot;static&quot;.</li>
<li><strong>enable_gradient_accumulation</strong> (bool, optional) -- Whether to enable gradient accumulation. When set to True, gradient will be stored after doing backward pass. Defaults to False.</li>
<li><strong>shard_param_frac</strong> (float, optional) -- fraction of parameters to be sharded. Only for &quot;static&quot; placement.
If <code>shard_param_frac</code> is 1.0, it&#x27;s equal to zero-3. If <code>shard_param_frac</code> is 0.0, it&#x27;s equal to zero-2. Defaults to 1.0.</li>
<li><strong>offload_optim_frac</strong> (float, optional) -- fraction of optimizer states to be offloaded. Only for &quot;static&quot; placement.
If <code>shard_param_frac</code> is 1.0 and <code>offload_optim_frac</code> is 0.0, it&#x27;s equal to old &quot;cuda&quot; placement. Defaults to 0.0.</li>
<li><strong>offload_param_frac</strong> (float, optional) -- fraction of parameters to be offloaded. Only for &quot;static&quot; placement.
For efficiency, this argument is useful only when <code>shard_param_frac</code> is 1.0 and <code>offload_optim_frac</code> is 1.0.
If <code>shard_param_frac</code> is 1.0, <code>offload_optim_frac</code> is 1.0 and <code>offload_param_frac</code> is 1.0, it&#x27;s equal to old &quot;cpu&quot; placement.
When using static placement, we recommend users to tune <code>shard_param_frac</code> first and then <code>offload_optim_frac</code>.
Defaults to 0.0.</li>
<li><strong>warmup_non_model_data_ratio</strong> (float, optional) -- ratio of expected non-model data memory during warmup. Only for &quot;auto&quot; placement. Defaults to 0.8.</li>
<li><strong>steady_cuda_cap_ratio</strong> (float, optional) -- ratio of allowed cuda capacity for model data during steady state. Only for &quot;auto&quot; placement. Defaults to 0.9.</li>
<li><strong>precision</strong> (str, optional) -- precision. Support &#x27;fp16&#x27; and &#x27;bf16&#x27;. Defaults to &#x27;fp16&#x27;.</li>
<li><strong>master_weights</strong> (bool, optional) -- Whether to keep fp32 master parameter weights in optimizer. Defaults to True.</li>
<li><strong>pin_memory</strong> (bool, optional) -- use pin memory on CPU. Defaults to False.</li>
<li><strong>force_outputs_fp32</strong> (bool, optional) -- force outputs are fp32. Defaults to False.</li>
<li><strong>strict_ddp_mode</strong> (bool, optional) -- use strict ddp mode (only use dp without other parallelism). Defaults to False.</li>
<li><strong>search_range_m</strong> (int, optional) -- chunk size searching range divided by 2^20. Defaults to 32.</li>
<li><strong>hidden_dim</strong> (int, optional) -- the hidden dimension of DNN.
Users can provide this argument to speed up searching.
If users do not know this argument before training, it is ok. We will use a default value 1024.</li>
<li><strong>min_chunk_size_m</strong> (float, optional) -- the minimum chunk size divided by 2^20.
If the aggregate size of parameters is still smaller than the minimum chunk size,
all parameters will be compacted into one small chunk.</li>
<li><strong>memstats</strong> (MemStats, optional) the memory statistics collector by a runtime memory tracer. --</li>
<li><strong>gpu_margin_mem_ratio</strong> (float, optional) -- The ratio of GPU remaining memory (after the first forward-backward)
which will be used when using hybrid CPU optimizer.
This argument is meaningless when <code>placement_policy</code> of <code>GeminiManager</code> is not &quot;auto&quot;.
Defaults to 0.0.</li>
<li><strong>initial_scale</strong> (float, optional) -- Initial scale used by DynamicGradScaler. Defaults to 2**16.</li>
<li><strong>min_scale</strong> (float, optional) -- Min scale used by DynamicGradScaler. Defaults to 1.</li>
<li><strong>growth_factor</strong> (float, optional) -- growth_factor used by DynamicGradScaler. Defaults to 2.</li>
<li><strong>backoff_factor</strong> (float, optional) -- backoff_factor used by DynamicGradScaler. Defaults to 0.5.</li>
<li><strong>growth_interval</strong> (float, optional) -- growth_interval used by DynamicGradScaler. Defaults to 1000.</li>
<li><strong>hysteresis</strong> (float, optional) -- hysteresis used by DynamicGradScaler. Defaults to 2.</li>
<li><strong>max_scale</strong> (int, optional) -- max_scale used by DynamicGradScaler. Defaults to 2**32.</li>
<li><strong>max_norm</strong> (float, optional) -- max_norm used for <code>clip_grad_norm</code>. You should notice that you shall not do
clip_grad_norm by yourself when using ZeRO DDP. The ZeRO optimizer will take care of clip_grad_norm.</li>
<li><strong>norm_type</strong> (float, optional) -- norm_type used for <code>clip_grad_norm</code>.</li>
<li><strong>verbose</strong> (bool, optional) -- verbose mode. Debug info including chunk search result will be printed. Defaults to False.</li>
</ul></div></div><div><div class="divider"><span class="divider-text">Description</span></div><p>Plugin for Gemini.</p><div><div class="divider"><span class="divider-text">Example</span></div><pre><code class="language-python">from colossalai.booster import Booster
from colossalai.booster.plugin import GeminiPlugin

model, train_dataset, optimizer, criterion = ...
plugin = GeminiPlugin()

train_dataloader = plugin.prepare_dataloader(train_dataset, batch_size=8)
booster = Booster(plugin=plugin)
model, optimizer, train_dataloader, criterion = booster.boost(model, optimizer, train_dataloader, criterion)
</code></pre></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="hybrid-parallel-插件">Hybrid Parallel 插件<a href="#hybrid-parallel-插件" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p>这个插件实现了多种并行训练策略和优化工具的组合。Hybrid Parallel插件支持的功能大致可以被分为以下四个部分：</p><ol><li>Shardformer: Shardformer负责在张量并行以及流水线并行下切分模型的逻辑，以及前向/后向方法的重载，这个插件为Shardformer功能提供了一个简单易用的接口。与此同时，Shardformer还负责将包括fused normalization, flash attention (xformers), JIT和序列并行在内的各类优化工具融入重载后的前向/后向方法。更多关于Shardformer的信息请参考 <a href="/zh-Hans/docs/features/shardformer">Shardformer文档</a>。下图展示了Shardformer与Hybrid Parallel插件所支持的功能。</li></ol><div align="center"><img loading="lazy" src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/shardformer/shardformer_and_hybridparallel.png" width="500" class="img_ev3q"></div><ol start="2"><li><p>混合精度训练：插件支持fp16/bf16的混合精度训练。更多关于混合精度训练的参数配置的详细信息请参考 <a href="/zh-Hans/docs/features/mixed_precision_training_with_booster">混合精度训练文档</a>。</p></li><li><p>Torch DDP: 当流水线并行和Zero不被使用的时候，插件会自动采用Pytorch DDP作为数据并行的策略。更多关于Torch DDP的参数配置的详细信息请参考 <a href="https://pytorch.org/docs/main/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel" target="_blank" rel="noopener noreferrer">Pytorch DDP 文档</a>。</p></li><li><p>Zero: 在初始化插件的时候，可以通过将<code>zero_stage</code>参数设置为1或2来让插件采用Zero 1/2作为数据并行的策略。Zero 1可以和流水线并行策略同时使用, 而Zero 2则不可以和流水线并行策略同时使用。更多关于Zero的参数配置的详细信息请参考 <a href="#low-level-zero-%E6%8F%92%E4%BB%B6">Low Level Zero 插件</a>.</p></li></ol><blockquote><p>⚠ 在使用该插件的时候, 只有支持Shardformer的部分Huggingface transformers模型才能够使用张量并行、流水线并行以及优化工具。Llama 1、Llama 2、OPT、Bloom、Bert以及GPT2等主流transformers模型均已支持Shardformer。</p></blockquote><div class="docstring-container"><div><div class="title-container"><div class="title-module"><h5>class</h5>  <h3>colossalai.booster.plugin.HybridParallelPlugin</h3></div><div class="title-source">&lt;<a href="https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/booster/plugin/hybrid_parallel_plugin.py#L568" class="title-source">source</a>&gt;</div></div><div class="signature">(<!-- -->tp_size: int, pp_size: int, precision: str = &#x27;fp16&#x27;, zero_stage: int = 0, enable_all_optimization: bool = False, enable_fused_normalization: bool = False, enable_flash_attention: bool = False, enable_jit_fused: bool = False, enable_sequence_parallelism: bool = False, enable_sequence_overlap: bool = False, num_microbatches: typing.Optional[int] = None, microbatch_size: typing.Optional[int] = None, initial_scale: float = 65536, min_scale: float = 1, growth_factor: float = 2, backoff_factor: float = 0.5, growth_interval: int = 1000, hysteresis: int = 2, max_scale: float = 4294967296, max_norm: float = 0, broadcast_buffers: bool = True, ddp_bucket_cap_mb: int = 25, find_unused_parameters: bool = False, check_reduction: bool = False, gradient_as_bucket_view: bool = False, static_graph: bool = False, zero_bucket_size_in_m: int = 12, cpu_offload: bool = False, communication_dtype: typing.Optional[torch.dtype] = None, overlap_communication: bool = True, custom_policy: Policy = None<!-- -->)</div><div><div class="divider"><span class="divider-text">Parameters</span></div><ul>
<li><strong>tp_size</strong> (int) -- The size of tensor parallelism. Tensor parallelism will not be used when tp_size is set to 1.</li>
<li><strong>pp_size</strong> (int) -- The number of pipeline stages in pipeline parallelism. Pipeline parallelism will not be used when pp_size is set to 1.</li>
<li><strong>precision</strong> (str, optional) -- Specifies the precision of parameters during training.
Auto-mixied precision will be used when this argument is set to &#x27;fp16&#x27; or &#x27;bf16&#x27;, otherwise model is trained with &#x27;fp32&#x27;.
Defaults to &#x27;fp16&#x27;.</li>
<li><strong>zero_stage</strong> (int, optional) -- The stage of ZeRO for data parallelism. Can only be choosed from [0, 1, 2].
When set to 0, ZeRO will not be used. Defaults to 0.</li>
<li><strong>enable_all_optimization</strong> (bool, optional) -- Whether to switch on all the optimizations supported by Shardformer.
Currently all the optimization methods include fused normalization, flash attention and JIT.
Defaults to False.</li>
<li><strong>enable_fused_normalization</strong> (bool, optional) -- Whether to switch on fused normalization in Shardformer. Defaults to False.</li>
<li><strong>enable_flash_attention</strong> (bool, optional) -- Whether to switch on flash attention in Shardformer. Defaults to False.</li>
<li><strong>enable_jit_fused</strong> (bool, optional) -- Whether to switch on JIT in Shardformer. Default to False.</li>
<li><strong>enable_sequence_parallelism</strong> (bool) -- Whether to turn on sequence parallelism in Shardformer. Defaults to False.</li>
<li><strong>enable_sequence_overlap</strong> (bool) -- Whether to turn on sequence overlap in Shardformer. Defaults to False.</li>
<li><strong>num_microbatches</strong> (int, optional) -- Number of microbatches when using pipeline parallelism. Defaults to None.</li>
<li><strong>microbatch_size</strong> (int, optional) -- Microbatch size when using pipeline parallelism.
Either <code>num_microbatches</code> or <code>microbatch_size</code> should be provided if using pipeline.
If <code>num_microbatches</code> is provided, this will be ignored. Defaults to None.</li>
<li><strong>initial_scale</strong> (float, optional) -- The initial loss scale of AMP. Defaults to 2**16.</li>
<li><strong>min_scale</strong> (float, optional) -- The minimum loss scale of AMP. Defaults to 1.</li>
<li><strong>growth_factor</strong> (float, optional) -- The multiplication factor for increasing loss scale when using AMP. Defaults to 2.</li>
<li><strong>backoff_factor</strong> (float, optional) -- The multiplication factor for decreasing loss scale when using AMP. Defaults to 0.5.</li>
<li><strong>growth_interval</strong> (int, optional) -- The number of steps to increase loss scale when no overflow occurs when using AMP. Defaults to 1000.</li>
<li><strong>hysteresis</strong> (int, optional) --  The number of overflows before decreasing loss scale when using AMP. Defaults to 2.</li>
<li><strong>max_scale</strong> (float, optional) -- The maximum loss scale of AMP. Defaults to 2**32.</li>
<li><strong>max_norm</strong> (float, optional) -- Maximum norm for gradient clipping. Defaults to 0.</li>
<li><strong>broadcast_buffers</strong> (bool, optional) -- Whether to broadcast buffers in the beginning of training when using DDP. Defaults to True.</li>
<li><strong>ddp_bucket_cap_mb</strong> (int, optional) -- The bucket size in MB when using DDP. Defaults to 25.</li>
<li><strong>find_unused_parameters</strong> (bool, optional) -- Whether to find unused parameters when using DDP. Defaults to False.</li>
<li><strong>check_reduction</strong> (bool, optional) -- Whether to check reduction when using DDP. Defaults to False.</li>
<li><strong>gradient_as_bucket_view</strong> (bool, optional) -- Whether to use gradient as bucket view when using DDP. Defaults to False.</li>
<li><strong>static_graph</strong> (bool, optional) -- Whether to use static graph when using DDP. Defaults to False.</li>
<li><strong>zero_bucket_size_in_m</strong> (int, optional) -- Gradient reduce bucket size in million elements when using ZeRO. Defaults to 12.</li>
<li><strong>cpu_offload</strong> (bool, optional) -- Whether to open cpu_offload when using ZeRO. Defaults to False.</li>
<li><strong>communication_dtype</strong> (torch.dtype, optional) -- Communication dtype when using ZeRO. If not specified, the dtype of param will be used. Defaults to None.</li>
<li><strong>overlap_communication</strong> (bool, optional) -- Whether to overlap communication and computation when using ZeRO. Defaults to True.</li>
<li><strong>custom_policy</strong> (Policy, optional) -- Custom policy for Shardformer. Defaults to None.</li>
</ul></div></div><div><div class="divider"><span class="divider-text">Description</span></div><p>Plugin for Hybrid Parallel Training.
Tensor parallel, pipeline parallel and data parallel(DDP/ZeRO) can be picked and combined in this plugin.
The size of tp and pp should be passed in by user, then the size of dp is automatically calculated from dp_size = world_size / (tp_size * pp_size).</p><div><div class="divider"><span class="divider-text">Example</span></div><pre><code class="language-python">from colossalai.booster import Booster
from colossalai.booster.plugin import HybridParallelPlugin

model, train_dataset, optimizer, criterion = ...
plugin =  HybridParallelPlugin(tp_size=2, pp_size=2)

train_dataloader = plugin.prepare_dataloader(train_dataset, batch_size=8)
booster = Booster(plugin=plugin)
model, optimizer, criterion, train_dataloader, _ = booster.boost(model, optimizer, criterion, train_dataloader)
</code></pre></div></div><div class="docstring-container"><div><div class="title-container"><div class="title-module"><h5>function</h5>  <h3>prepare_dataloader</h3></div><div class="title-source">&lt;<a href="https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/booster/plugin/hybrid_parallel_plugin.py#L842" class="title-source">source</a>&gt;</div></div><div class="signature">(<!-- -->dataset, batch_size, shuffle = False, seed = 1024, drop_last = False, pin_memory = False, num_workers = 0, **kwargs<!-- -->)</div><div><div class="divider"><span class="divider-text">Parameters</span></div><ul>
<li><strong>dataset</strong> (<em>torch.utils.data.Dataset</em>) -- The dataset to be loaded.</li>
<li><strong>shuffle</strong> (bool, optional) -- Whether to shuffle the dataset. Defaults to False.</li>
<li><strong>seed</strong> (int, optional) -- Random worker seed for sampling, defaults to 1024.
add_sampler -- Whether to add <code>DistributedDataParallelSampler</code> to the dataset. Defaults to True.</li>
<li><strong>drop_last</strong> (bool, optional) -- Set to True to drop the last incomplete batch, if the dataset size
is not divisible by the batch size. If False and the size of dataset is not divisible by
the batch size, then the last batch will be smaller, defaults to False.</li>
<li><strong>pin_memory</strong> (bool, optional) -- Whether to pin memory address in CPU memory. Defaults to False.</li>
<li><strong>num_workers</strong> (int, optional) -- Number of worker threads for this dataloader. Defaults to 0.</li>
<li><strong>kwargs</strong> (dict) -- optional parameters for <code>torch.utils.data.DataLoader</code>, more details could be found in
<a href="https://pytorch.org/docs/stable/_modules/torch/utils/data/dataloader.html#DataLoader">DataLoader</a>.</li>
</ul></div><div><div class="divider"><span class="divider-text">Returns</span></div><p>[<code>torch.utils.data.DataLoader</code>]: A DataLoader used for training or testing.</p></div></div><div><div class="divider"><span class="divider-text">Description</span></div><p>Prepare a dataloader for distributed training. The dataloader will be wrapped by
<em>torch.utils.data.DataLoader</em> and <em>torch.utils.data.DistributedSampler</em>.</p></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="torch-ddp-插件">Torch DDP 插件<a href="#torch-ddp-插件" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p>更多详细信息，请参阅 <a href="https://pytorch.org/docs/main/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel" target="_blank" rel="noopener noreferrer">Pytorch 文档</a>.</p><div class="docstring-container"><div><div class="title-container"><div class="title-module"><h5>class</h5>  <h3>colossalai.booster.plugin.TorchDDPPlugin</h3></div><div class="title-source">&lt;<a href="https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/booster/plugin/torch_ddp_plugin.py#L129" class="title-source">source</a>&gt;</div></div><div class="signature">(<!-- -->broadcast_buffers: bool = True, bucket_cap_mb: int = 25, find_unused_parameters: bool = False, check_reduction: bool = False, gradient_as_bucket_view: bool = False, static_graph: bool = False<!-- -->)</div><div><div class="divider"><span class="divider-text">Parameters</span></div><ul>
<li><strong>broadcast_buffers</strong> (bool, optional) -- Whether to broadcast buffers in the beginning of training. Defaults to True.</li>
<li><strong>bucket_cap_mb</strong> (int, optional) -- The bucket size in MB. Defaults to 25.</li>
<li><strong>find_unused_parameters</strong> (bool, optional) -- Whether to find unused parameters. Defaults to False.</li>
<li><strong>check_reduction</strong> (bool, optional) -- Whether to check reduction. Defaults to False.</li>
<li><strong>gradient_as_bucket_view</strong> (bool, optional) -- Whether to use gradient as bucket view. Defaults to False.</li>
<li><strong>static_graph</strong> (bool, optional) -- Whether to use static graph. Defaults to False.</li>
</ul></div></div><div><div class="divider"><span class="divider-text">Description</span></div><p>Plugin for PyTorch DDP.</p><div><div class="divider"><span class="divider-text">Example</span></div><pre><code class="language-python">from colossalai.booster import Booster
from colossalai.booster.plugin import TorchDDPPlugin

model, train_dataset, optimizer, criterion = ...
plugin = TorchDDPPlugin()

train_dataloader = plugin.prepare_dataloader(train_dataset, batch_size=8)
booster = Booster(plugin=plugin)
model, optimizer, train_dataloader, criterion = booster.boost(model, optimizer, train_dataloader, criterion)
</code></pre></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="torch-fsdp-插件">Torch FSDP 插件<a href="#torch-fsdp-插件" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><blockquote><p>⚠ 如果 torch 版本低于 1.12.0，此插件将不可用。</p></blockquote><blockquote><p>⚠ 该插件现在还不支持保存/加载分片的模型 checkpoint。</p></blockquote><blockquote><p>⚠ 该插件现在还不支持使用了multi params group的optimizer。</p></blockquote><p>更多详细信息，请参阅 <a href="https://pytorch.org/docs/main/fsdp.html" target="_blank" rel="noopener noreferrer">Pytorch 文档</a>.</p><div class="docstring-container"><div><div class="title-container"><div class="title-module"><h5>class</h5>  <h3>colossalai.booster.plugin.TorchFSDPPlugin</h3></div><div class="title-source">&lt;<a href="https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/booster/plugin/torch_fsdp_plugin.py#L142" class="title-source">source</a>&gt;</div></div><div class="signature">(<!-- -->process_group: typing.Optional[torch.distributed.distributed_c10d.ProcessGroup] = None, sharding_strategy: typing.Optional[torch.distributed.fsdp.api.ShardingStrategy] = None, cpu_offload: typing.Optional[torch.distributed.fsdp.api.CPUOffload] = None, auto_wrap_policy: typing.Optional[typing.Callable] = None, backward_prefetch: typing.Optional[torch.distributed.fsdp.api.BackwardPrefetch] = None, mixed_precision: typing.Optional[torch.distributed.fsdp.api.MixedPrecision] = None, ignored_modules: typing.Optional[typing.Iterable[torch.nn.modules.module.Module]] = None, param_init_fn: typing.Optional[typing.Callable[[torch.nn.modules.module.Module]], NoneType] = None, sync_module_states: bool = False<!-- -->)</div><div><div class="divider"><span class="divider-text">Parameters</span></div><ul>
<li><strong>See</strong> https --//pytorch.org/docs/stable/fsdp.html for details.</li>
</ul></div></div><div><div class="divider"><span class="divider-text">Description</span></div><p>Plugin for PyTorch FSDP.</p><div><div class="divider"><span class="divider-text">Example</span></div><pre><code class="language-python">from colossalai.booster import Booster
from colossalai.booster.plugin import TorchFSDPPlugin

model, train_dataset, optimizer, criterion = ...
plugin = TorchFSDPPlugin()

train_dataloader = plugin.prepare_train_dataloader(train_dataset, batch_size=8)
booster = Booster(plugin=plugin)
model, optimizer, train_dataloader, criterion = booster.boost(model, optimizer, train_dataloader, criterion)
</code></pre></div></div></div></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/basics/booster_plugins.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>编辑此页</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="文档分页导航"><a class="pagination-nav__link pagination-nav__link--prev" href="/zh-Hans/docs/basics/booster_api"><div class="pagination-nav__sublabel">上一页</div><div class="pagination-nav__label">Booster API</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/zh-Hans/docs/basics/booster_checkpoint"><div class="pagination-nav__sublabel">下一页</div><div class="pagination-nav__label">Booster Checkpoint</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#引言" class="table-of-contents__link toc-highlight">引言</a></li><li><a href="#插件选择" class="table-of-contents__link toc-highlight">插件选择</a></li><li><a href="#插件" class="table-of-contents__link toc-highlight">插件</a><ul><li><a href="#low-level-zero-插件" class="table-of-contents__link toc-highlight">Low Level Zero 插件</a></li><li><a href="#gemini-插件" class="table-of-contents__link toc-highlight">Gemini 插件</a></li><li><a href="#hybrid-parallel-插件" class="table-of-contents__link toc-highlight">Hybrid Parallel 插件</a></li><li><a href="#torch-ddp-插件" class="table-of-contents__link toc-highlight">Torch DDP 插件</a></li><li><a href="#torch-fsdp-插件" class="table-of-contents__link toc-highlight">Torch FSDP 插件</a></li></ul></li></ul></div></div></div></div></main></div></div><footer class="footer"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">资源</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/zh-Hans/docs/get_started/installation">教程</a></li><li class="footer__item"><a href="https://github.com/hpcaitech/ColossalAI/tree/main/examples" target="_blank" rel="noopener noreferrer" class="footer__link-item">例子</a></li><li class="footer__item"><a href="https://github.com/hpcaitech/ColossalAI/discussions" target="_blank" rel="noopener noreferrer" class="footer__link-item">论坛</a></li></ul></div><div class="col footer__col"><div class="footer__title">社区</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/hpcaitech/ColossalAI" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub</a></li><li class="footer__item"><a href="https://www.hpc-ai.tech/blog" target="_blank" rel="noopener noreferrer" class="footer__link-item">博客</a></li><li class="footer__item"><a href="https://twitter.com/HPCAITech" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter</a></li></ul></div><div class="col footer__col"><div class="footer__title">关于</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://www.hpc-ai.tech/" target="_blank" rel="noopener noreferrer" class="footer__link-item">公司</a></li><li class="footer__item"><a href="https://www.hpc-ai.tech/services" target="_blank" rel="noopener noreferrer" class="footer__link-item">服务</a></li><li class="footer__item"><a href="https://www.hpc-ai.tech/customers" target="_blank" rel="noopener noreferrer" class="footer__link-item">客户</a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2023 All Rights Reserved by HPC-AI Technology Inc.</div></div></div></footer></div>
<script src="/zh-Hans/assets/js/runtime~main.6961964e.js"></script>
<script src="/zh-Hans/assets/js/main.10afeca0.js"></script>
</body>
</html>