<!doctype html>
<html lang="zh-Hans" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-features/zerobubble_pipeline_parallelism">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.3.1">
<title data-rh="true">零气泡流水线并行 | Colossal-AI</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://colossalai.org/zh-Hans/img/social-card.png"><meta data-rh="true" name="twitter:image" content="https://colossalai.org/zh-Hans/img/social-card.png"><meta data-rh="true" property="og:url" content="https://colossalai.org/zh-Hans/docs/features/zerobubble_pipeline_parallelism"><meta data-rh="true" name="docusaurus_locale" content="zh-Hans"><meta data-rh="true" name="docsearch:language" content="zh-Hans"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="零气泡流水线并行 | Colossal-AI"><meta data-rh="true" name="description" content="作者: Junwen Duan, Hongxin Liu"><meta data-rh="true" property="og:description" content="作者: Junwen Duan, Hongxin Liu"><link data-rh="true" rel="icon" href="/zh-Hans/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://colossalai.org/zh-Hans/docs/features/zerobubble_pipeline_parallelism"><link data-rh="true" rel="alternate" href="https://colossalai.org/docs/features/zerobubble_pipeline_parallelism" hreflang="en"><link data-rh="true" rel="alternate" href="https://colossalai.org/zh-Hans/docs/features/zerobubble_pipeline_parallelism" hreflang="zh-Hans"><link data-rh="true" rel="alternate" href="https://colossalai.org/docs/features/zerobubble_pipeline_parallelism" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://XP2V2KAOVI-dsn.algolia.net" crossorigin="anonymous"><link rel="alternate" type="application/rss+xml" href="/zh-Hans/blog/rss.xml" title="Colossal-AI RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/zh-Hans/blog/atom.xml" title="Colossal-AI Atom Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-1XKZVCCKRZ"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-1XKZVCCKRZ",{anonymize_ip:!0})</script>


<link rel="search" type="application/opensearchdescription+xml" title="Colossal-AI" href="/zh-Hans/opensearch.xml">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous">
<script src="https://js-eu1.hs-scripts.com/26563514.js" id="hs-script-loader" async defer="defer"></script><link rel="stylesheet" href="/zh-Hans/assets/css/styles.9a2d694b.css">
<link rel="preload" href="/zh-Hans/assets/js/runtime~main.33da9ac8.js" as="script">
<link rel="preload" href="/zh-Hans/assets/js/main.2f022937.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="跳到主要内容"><a class="skipToContent_fXgn" href="#docusaurus_skipToContent_fallback">跳到主要内容</a></div><nav aria-label="主导航" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="切换导航栏" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/zh-Hans/"><div class="navbar__logo"><img src="/zh-Hans/img/logo.svg" alt="project-logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/zh-Hans/img/logo.svg" alt="project-logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">Colossal-AI</b></a><a class="navbar__item navbar__link" href="/zh-Hans/docs/get_started/installation">教程</a><a href="https://github.com/hpcaitech/ColossalAI/tree/main/examples" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">例子</a><a href="https://www.hpc-ai.tech/blog" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">博客</a></div><div class="navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/zh-Hans/docs/get_started/installation">Next</a><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_nlXk"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>简体中文</a><ul class="dropdown__menu"><li><a href="/docs/features/zerobubble_pipeline_parallelism" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="en">English</a></li><li><a href="/zh-Hans/docs/features/zerobubble_pipeline_parallelism" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="zh-Hans">简体中文</a></li></ul></div><a href="https://github.com/hpcaitech/ColossalAI" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-github-link"></a><div class="searchBox_ZlJk"><button type="button" class="DocSearch DocSearch-Button" aria-label="搜索"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">搜索</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="回到顶部" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><main class="docMainContainer_gTbr docMainContainerEnhanced_Uz_u"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">本页总览</button></div><div class="theme-doc-markdown markdown"><h1>零气泡流水线并行</h1><p>作者: <a href="https://github.com/duanjunwen" target="_blank" rel="noopener noreferrer">Junwen Duan</a>, <a href="https://github.com/ver217" target="_blank" rel="noopener noreferrer">Hongxin Liu</a></p><p><strong>相关论文</strong></p><ul><li><a href="https://arxiv.org/abs/2401.10241" target="_blank" rel="noopener noreferrer">Zero Bubble Pipeline Parallelism</a></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="介绍">介绍<a href="#介绍" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p>零气泡（V Schedule）：
与早期工作中的1F1B方案相比，零气泡流水线并行将B分成两个阶段（也称为激活梯度和权重梯度），形如1F1B1W这样的方案可以进一步减少气泡。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="使用">使用<a href="#使用" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p>我们将演示如何在 4 个 GPU 上使用带有 booster API 的 ZeroBubble</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="step-1-引用仓库">step 1. 引用仓库<a href="#step-1-引用仓库" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> torch</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> torch</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">distributed </span><span class="token keyword" style="color:#00009f">as</span><span class="token plain"> dist</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> torch</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">nn </span><span class="token keyword" style="color:#00009f">as</span><span class="token plain"> nn</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> torch</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">testing </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> assert_close</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> transformers</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">models</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">llama</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">configuration_llama </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> LlamaConfig</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> transformers</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">models</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">llama</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">modeling_llama </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> LlamaModel</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> colossalai</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> colossalai</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">booster</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">booster </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> Booster</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> colossalai</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">booster</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">plugin</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">moe_hybrid_parallel_plugin </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> HybridParallelPlugin</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> colossalai</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">pipeline</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">schedule</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">zero_bubble_pp </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> ZeroBubbleVPipeScheduler</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="step-2-初始化分布式环境">step 2. 初始化分布式环境<a href="#step-2-初始化分布式环境" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">colossalai</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">launch</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">rank</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">rank</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> world_size</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">world_size</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> host</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">&quot;localhost&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> port</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">port</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> backend</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">&quot;nccl&quot;</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="step-3-初始化模型优化器">step 3. 初始化模型优化器<a href="#step-3-初始化模型优化器" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p>建立我们的模型和优化器 我们创建了一个带有8层Decoder-Layer的 Llama。然后，使用get_v_schedule()函数创建PipelineGraph和Pipeline schedule。</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># Global Param</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">NUM_BATCH </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">8</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">NUM_TOK_PER_BATCH </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">4</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">NUM_LAYERS </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">8</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">HIDDEN_SIZE_PER_HEAD </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">4</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">NUM_HEADS </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">4</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># Init Llama from huggingface</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">configuration </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> LlamaConfig</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    hidden_size</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">HIDDEN_SIZE_PER_HEAD </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> NUM_HEADS</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    intermediate_size</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">HIDDEN_SIZE_PER_HEAD </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> NUM_HEADS </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">2</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    num_hidden_layers</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">NUM_LAYERS</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    num_attention_heads</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">NUM_HEADS</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    num_key_value_heads</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">NUM_HEADS</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    attn_implementation</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">&quot;flash_attention_2&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">model </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> LlamaModel</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">configuration</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">cuda</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">optimizer </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> torch</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">optim</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">Adam</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">torch_model</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">parameters</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> lr</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="step-4初始化流水线schedule">step 4.初始化流水线Schedule<a href="#step-4初始化流水线schedule" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p>然后，我们需要使用 get_v_schedule() 函数创建 PipelineGraph 和 PipelineSchedule。我们需要用以下参数初始化 PipelineGraph。
x_cost 表示每个模型块的操作 x 所消耗的运行时间。
x_mem 表示每个模型块的操作 x 所消耗的内存量。
这些参数都是在流水线启动前估算并填入的。事实上，在模型的实际计算过程中，根据运行时间和内存成本可以获得更好的结果。
在下面的例子中，我们假设模型的正向、反向 B 和反向 W 的计算时间分别为 1、1、1，p2p 通信时间为 1。</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># Init schedule</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">h</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> a</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> s </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> config</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">hidden_size</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> config</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">num_attention_heads</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1024</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">mem_f </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">34</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> h </span><span class="token operator" style="color:#393A34">+</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">5</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> a </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> s</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">mem_w </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">-</span><span class="token number" style="color:#36acaa">32</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> h</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">mem_b </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">-</span><span class="token plain">mem_w </span><span class="token operator" style="color:#393A34">-</span><span class="token plain"> mem_f</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">graph </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> PipelineGraph</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    n_stage</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">pp_size</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    n_micro</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">num_microbatches</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    f_cost</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    b_cost</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    w_cost</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    c_cost</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    f_mem</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">mem_f</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    b_mem</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">mem_b</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    w_mem</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">mem_w</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">zbv_schedule </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> graph</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">get_v_schedule</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="step-5初始化booster">step 5.初始化Booster<a href="#step-5初始化booster" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p>在初始化Plugin时输入pp_style=&quot;zbv&quot;，以使用ZeroBubble流水线并行。</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">plugin </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> HybridParallelPlugin</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    pp_size</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">4</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    num_microbatches</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">4</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    tp_size</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    sp_size</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    zero_stage</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    initial_scale</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    find_unused_parameters</span><span class="token operator" style="color:#393A34">=</span><span class="token boolean" style="color:#36acaa">True</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    pp_style</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">&quot;zbv&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    scheduler_nodes</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">zbv_schedule</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    num_model_chunks</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">2</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">dp_size </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> plugin</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">dp_size</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">booster </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> Booster</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">plugin</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">plugin</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="step-6训练模型">step 6.训练模型<a href="#step-6训练模型" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">steps </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">10</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> step </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> </span><span class="token builtin">range</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">steps</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    input_embeddings </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> torch</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">rand</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        NUM_BATCH</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> NUM_TOK_PER_BATCH</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> HIDDEN_SIZE_PER_HEAD </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> NUM_HEADS</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> requires_grad</span><span class="token operator" style="color:#393A34">=</span><span class="token boolean" style="color:#36acaa">True</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">cuda</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    dist</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">all_reduce</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        input_embeddings</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> group</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">plugin</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">pp_group</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    data_iter </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token builtin">iter</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">[</span><span class="token punctuation" style="color:#393A34">{</span><span class="token string" style="color:#e3116c">&quot;inputs_embeds&quot;</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> input_embeddings</span><span class="token punctuation" style="color:#393A34">}</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    output </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> booster</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">execute_pipeline</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        data_iter</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        model</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token keyword" style="color:#00009f">lambda</span><span class="token plain"> x</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> y</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> x</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">last_hidden_state</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">mean</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        optimizer</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        return_loss</span><span class="token operator" style="color:#393A34">=</span><span class="token boolean" style="color:#36acaa">True</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        return_outputs</span><span class="token operator" style="color:#393A34">=</span><span class="token boolean" style="color:#36acaa">True</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    optimizer</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">step</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    optimizer</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">zero_grad</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h2 class="anchor anchorWithStickyNavbar_LWe7" id="进阶使用技巧">进阶使用技巧<a href="#进阶使用技巧" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p>在 ColossalAI 中，通过使用MetaCache和混合并行的ZeroBubble，可以获得更好的训练性能。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="1在zerobubble中使用元数据缓存">1.在ZeroBubble中使用元数据缓存<a href="#1在zerobubble中使用元数据缓存" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p>在初始化Plugin时输入 &quot;enable_metadata_cache=True&quot;，以便在ZeroBubble管道中使用元数据缓存。</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">plugin </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> HybridParallelPlugin</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    pp_size</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">2</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    num_microbatches</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">4</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    tp_size</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">2</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    sp_size</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">2</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    zero_stage</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    initial_scale</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    enable_metadata_cache</span><span class="token operator" style="color:#393A34">=</span><span class="token boolean" style="color:#36acaa">True</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    find_unused_parameters</span><span class="token operator" style="color:#393A34">=</span><span class="token boolean" style="color:#36acaa">True</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    pp_style</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">&quot;zbv&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    scheduler_nodes</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">zbv_schedule</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    num_model_chunks</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">2</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="2同时使用zerobubble和混合并行">2.同时使用ZeroBubble和混合并行<a href="#2同时使用zerobubble和混合并行" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p>在初始化插件时传递 pp_size, tp_size, sp_size, 以便使用零气泡混合并行管道（HybridParallel with ZeroBubble Pipeline）。</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">plugin </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> HybridParallelPlugin</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    pp_size</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">2</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    num_microbatches</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">2</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    tp_size</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">2</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    sp_size</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">2</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    zero_stage</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    initial_scale</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    find_unused_parameters</span><span class="token operator" style="color:#393A34">=</span><span class="token boolean" style="color:#36acaa">True</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    pp_style</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">&quot;zbv&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    scheduler_nodes</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">zbv_schedule</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    num_model_chunks</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">2</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>性能指标</p><table><tr><th nowrap="nowrap">HybridParallel Strategy</th><th nowrap="nowrap" align="center">Pipeline Parallel</th><th nowrap="nowrap" align="center">Sequence Parallel + Pipeline Parallel</th><th nowrap="nowrap" align="center">Data Parallel + Pipeline Parallel</th></tr><tr><td nowrap="nowrap" align="center" title="1F1B">With 1F1B</td><td nowrap="nowrap" align="center">15.27 samples/sec</td><td nowrap="nowrap" align="center">17.22 samples/sec</td><td nowrap="nowrap" align="center">14.06 samples/sec</td></tr><tr><td nowrap="nowrap" align="center" title="Zero Bubble">With Zero Bubble</td><td nowrap="nowrap" align="center">17.36 samples/sec</td><td nowrap="nowrap" align="center">18.38 samples/sec</td><td nowrap="nowrap" align="center">14.44 samples/sec</td></tr><tr><td colspan="39"></td></tr></table><h2 class="anchor anchorWithStickyNavbar_LWe7" id="模型兼容性">模型兼容性<a href="#模型兼容性" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><table><tr><th nowrap="nowrap">Shardformer/Model</th><th nowrap="nowrap" align="center">Bert</th><th nowrap="nowrap" align="center">Blip2</th><th nowrap="nowrap" align="center">Bloom</th><th nowrap="nowrap" align="center">Chatglm2</th><th nowrap="nowrap" align="center">Command</th><th nowrap="nowrap" align="center">Deepseek</th><th nowrap="nowrap" align="center">Falcon</th><th nowrap="nowrap" align="center">GPT2</th><th nowrap="nowrap" align="center">Gptj</th><th nowrap="nowrap" align="center">Llama</th><th nowrap="nowrap" align="center">Mistral</th><th nowrap="nowrap" align="center">Opt</th><th nowrap="nowrap" align="center">Qwen2</th><th nowrap="nowrap" align="center">Sam</th><th nowrap="nowrap" align="center">T5</th><th nowrap="nowrap" align="center">Vit</th><th nowrap="nowrap" align="center">Whisper</th></tr><tr><td nowrap="nowrap" align="center" title="ZeroBubble">ZeroBubble</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td><td nowrap="nowrap" align="center">✔️</td></tr><tr><td colspan="39"></td></tr></table><h2 class="anchor anchorWithStickyNavbar_LWe7" id="api-参考">API 参考<a href="#api-参考" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><div class="docstring-container"><div><div class="title-container"><div class="title-module"><h5>class</h5>  <h3>colossalai.pipeline.ZeroBubbleVPipeScheduler</h3></div><div class="title-source">&lt;<a href="https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/pipeline/schedule/zero_bubble_pp.py#L40" class="title-source">source</a>&gt;</div></div><div class="signature">(<!-- -->stage_manager: PipelineStageManager, schedule: typing.List[colossalai.pipeline.schedule.v_schedule.ScheduledNode], num_model_chunks: int, num_microbatch: typing.Optional[int] = None, microbatch_size: typing.Optional[int] = None, enable_metadata_cache: bool = True, overlap_p2p: bool = True<!-- -->)</div><div><div class="divider"><span class="divider-text">Parameters</span></div><ul>
<li><strong>stage_manager</strong> (PipelineStageManager) -- If using pipeline parallelism, it&#x27;s necessary to specify a pipeline stage manager for inter-process communication in pipeline parallelism. Defaults to None, which means not using pipeline parallelism.</li>
<li><strong>schedule</strong> (List[ScheduledNode]) -- Schedule for ZeroBubbleVPipe.</li>
<li><strong>num_model_chunks</strong> (int)  -- The number of model chunk in a device.</li>
<li><strong>num_microbatch</strong> (Optional[int]) -- The number of microbatch.</li>
<li><strong>microbatch_size</strong> (Optional[int]) -- The size per microbatch.</li>
<li><strong>enable_metadata_cache</strong> (bool) -- whether to enable metadata cache to acclerate communication.</li>
<li><strong>overlap_p2p</strong> (bool) -- whether to use overlap_p2p.</li>
</ul></div></div><div><div class="divider"><span class="divider-text">Description</span></div><p>ZeroBubbleVPipeScheduler</p></div><div class="docstring-container"><div><div class="title-container"><div class="title-module"><h5>function</h5>  <h3>backward_b_step</h3></div><div class="title-source">&lt;<a href="https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/pipeline/schedule/zero_bubble_pp.py#L516" class="title-source">source</a>&gt;</div></div><div class="signature">(<!-- -->model_chunk: typing.Union[torch.nn.modules.container.ModuleList, torch.nn.modules.module.Module], model_chunk_id: int, optimizer: OptimizerWrapper, input_obj: typing.Optional[dict], output_obj: typing.Union[dict, torch.Tensor], output_obj_grad: typing.Optional[dict]<!-- -->)</div><div><div class="divider"><span class="divider-text">Parameters</span></div><ul>
<li><strong>model_chunk</strong> (ModuleList or Module) -- Model Chunk to be run;</li>
<li><strong>model_chunk_id</strong> (int) -- The current model chunk idx;</li>
<li><strong>optimizer</strong> (OptimizerWrapper) -- Optimizer to update the model</li>
<li><strong>input_obj</strong> (Optional[Tuple(dict)]) -- x. (microbatch, input_obj)</li>
<li><strong>output_obj</strong> (Union[dict, torch.Tensor]) -- y.</li>
<li><strong>output_obj_grad</strong> (dict) -- dy.</li>
</ul></div><div><div class="divider"><span class="divider-text">Returns</span></div><p>Optional[dict]: dx.</p></div></div><div><div class="divider"><span class="divider-text">Description</span></div>Backward dx step of the pipeline; we calculate &quot;dx = w*dy&quot; here;</div></div><div class="docstring-container"><div><div class="title-container"><div class="title-module"><h5>function</h5>  <h3>backward_w_step</h3></div><div class="title-source">&lt;<a href="https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/pipeline/schedule/zero_bubble_pp.py#L591" class="title-source">source</a>&gt;</div></div><div class="signature">(<!-- -->model_chunk: typing.Union[torch.nn.modules.container.ModuleList, torch.nn.modules.module.Module], model_chunk_id: int, optimizer: OptimizerWrapper, output_obj: typing.Union[dict, torch.Tensor], output_obj_grad: typing.Optional[dict]<!-- -->)</div><div><div class="divider"><span class="divider-text">Parameters</span></div><ul>
<li><strong>model_chunk</strong> (ModuleList or Module) -- Model Chunk to be run;</li>
<li><strong>model_chunk_id</strong> (int) -- The current model chunk idx;</li>
<li><strong>optimizer</strong> (OptimizerWrapper) -- Optimizer to update the model</li>
<li><strong>output_obj</strong> (Union[dict, torch.Tensor]) -- y.</li>
<li><strong>output_obj_grad</strong> (dict) -- dy.</li>
</ul></div><div><div class="divider"><span class="divider-text">Returns</span></div><p>: Nothing need to return; we only calculate dw then update w;</p></div></div><div><div class="divider"><span class="divider-text">Description</span></div>Backward dw step of the pipeline; we calculate &quot;dw = x*dy&quot; here;</div></div><div class="docstring-container"><div><div class="title-container"><div class="title-module"><h5>function</h5>  <h3>forward_backward_step</h3></div><div class="title-source">&lt;<a href="https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/pipeline/schedule/zero_bubble_pp.py#L938" class="title-source">source</a>&gt;</div></div><div class="signature">(<!-- -->model_chunk: typing.Union[torch.nn.modules.container.ModuleList, torch.nn.modules.module.Module], data_iter: typing.Iterable, criterion: typing.Callable[..., typing.Any], optimizer: typing.Optional[colossalai.interface.optimizer.OptimizerWrapper] = None, return_loss: bool = False, return_outputs: bool = False<!-- -->)</div><div><div class="divider"><span class="divider-text">Parameters</span></div><ul>
<li><strong>model_chunk</strong> (ModuleList or Module) -- Model Chunk to be trained. Original interleaved uses a module list whereas shardformer uses entire model + layer specification</li>
<li><strong>data_iter</strong> (Iterable) -- Data iterator.</li>
<li><strong>criterion</strong> (Callable[[Any, Any], Tensor]) -- Criterion to be used. It should take two arguments: model outputs and inputs, and returns loss tensor.</li>
<li><strong>optimizer</strong> (OptimizerWrapper, optional) -- Optimizer to be used. Can be None when only forward is executed. Defaults to None.</li>
<li><strong>return_loss</strong> (bool, optional) -- Whether to return loss. Defaults to False. Whether to return loss.</li>
<li><strong>return_outputs</strong> (bool, optional) -- Whether to return model outputs. Defaults to False. Whether to return model outputs.</li>
</ul></div><div><div class="divider"><span class="divider-text">Returns</span></div><p>dict: A dict with keys: &#x27;loss&#x27; and &#x27;outputs&#x27;.</p></div></div><div><div class="divider"><span class="divider-text">Description</span></div></div></div><div class="docstring-container"><div><div class="title-container"><div class="title-module"><h5>function</h5>  <h3>forward_step</h3></div><div class="title-source">&lt;<a href="https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/pipeline/schedule/zero_bubble_pp.py#L474" class="title-source">source</a>&gt;</div></div><div class="signature">(<!-- -->model_chunk: typing.Union[torch.nn.modules.container.ModuleList, torch.nn.modules.module.Module], model_chunk_id: int, micro_batch: typing.Optional[dict], input_obj: typing.Optional[dict], criterion: typing.Callable, accum_loss: typing.Optional[torch.Tensor] = None, outputs: typing.Optional[typing.List[typing.Any]] = None<!-- -->)</div><div><div class="divider"><span class="divider-text">Parameters</span></div><ul>
<li><strong>model_chunk</strong> (ModuleList or Module) -- Model Chunk to be run;</li>
<li><strong>model_chunk_id</strong> (int) -- The current model chunk idx;</li>
<li><strong>input_obj</strong> (Optional[dict]) -- x;</li>
<li><strong>criterion</strong> (Callable) -- loss function;</li>
<li><strong>accum_loss</strong> (Optional[torch.Tensor], optional) -- Accumulated loss. Defaults to None.</li>
<li><strong>outputs</strong> (Optional[List[Any]], optional) -- List to store the output of the last stage (final output). Defaults to None.</li>
</ul></div><div><div class="divider"><span class="divider-text">Returns</span></div><p>Union[torch.Tensor, dict]: The intermediate output (dict) of the current stage. If it is the last stage, the output is the loss (Tensor).</p></div></div><div><div class="divider"><span class="divider-text">Description</span></div>Forward one step of the pipeline</div></div><div class="docstring-container"><div><div class="title-container"><div class="title-module"><h5>function</h5>  <h3>get_model_chunk_id</h3></div><div class="title-source">&lt;<a href="https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/pipeline/schedule/zero_bubble_pp.py#L219" class="title-source">source</a>&gt;</div></div><div class="signature">(<!-- -->microbatch_id: int, is_forward: bool<!-- -->)</div><div><div class="divider"><span class="divider-text">Parameters</span></div><ul>
<li><strong>microbatch_id</strong> (int) -- the current microbatch idx</li>
<li><strong>forward</strong> (bool) -- if is the forward process</li>
</ul></div><div><div class="divider"><span class="divider-text">Returns</span></div><p>int: The model chunk idx of the input microbatch_id</p></div></div><div><div class="divider"><span class="divider-text">Description</span></div>Helper method to get the model chunk ID given the iteration number.</div></div><div class="docstring-container"><div><div class="title-container"><div class="title-module"><h5>function</h5>  <h3>load_batch</h3></div><div class="title-source">&lt;<a href="https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/pipeline/schedule/zero_bubble_pp.py#L170" class="title-source">source</a>&gt;</div></div><div class="signature">(<!-- -->data_iter: typing.Iterable, device: typing.Optional[torch.device] = None<!-- -->)</div><div><div class="divider"><span class="divider-text">Parameters</span></div><ul>
<li><strong>data_iter</strong> (Iterable) -- Data iterator.</li>
<li><strong>device</strong> (Optional[torch.device], optional) -- Target device. Defaults to None.</li>
</ul></div></div><div><div class="divider"><span class="divider-text">Description</span></div>Load a batch from data iterator.</div></div><div class="docstring-container"><div><div class="title-container"><div class="title-module"><h5>function</h5>  <h3>load_micro_batch</h3></div><div class="title-source">&lt;<a href="https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/pipeline/schedule/zero_bubble_pp.py#L205" class="title-source">source</a>&gt;</div></div><div class="signature">(<!-- -->model_chunk_id: int<!-- -->)</div><div><div class="divider"><span class="divider-text">Parameters</span></div><ul>
<li><strong>microbatch_id</strong> (int) -- the current model chunk idx.</li>
</ul></div><div><div class="divider"><span class="divider-text">Returns</span></div><p>Any: Micro batch.</p></div></div><div><div class="divider"><span class="divider-text">Description</span></div>Load a micro batch from the current batch.</div></div><div class="docstring-container"><div><div class="title-container"><div class="title-module"><h5>function</h5>  <h3>recv_backward</h3></div><div class="title-source">&lt;<a href="https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/pipeline/schedule/zero_bubble_pp.py#L297" class="title-source">source</a>&gt;</div></div><div class="signature">(<!-- -->model_chunk_id: int, next_rank: int = None<!-- -->)</div><div><div class="divider"><span class="divider-text">Parameters</span></div><ul>
<li><strong>model_chunk_id</strong> (int) -- The current model chunk idx.</li>
<li><strong>next_rank</strong> (int, optional) -- The rank of the source of the tensor.</li>
</ul></div><div><div class="divider"><span class="divider-text">Returns</span></div><p>Any: The input gradient tensor or gradient tensor list.
Any: The wait handles for the communication.</p></div></div><div><div class="divider"><span class="divider-text">Description</span></div>Copy the gradient tensor from the next stage in pipeline as the input gradient of this stage. For ZBV.</div></div><div class="docstring-container"><div><div class="title-container"><div class="title-module"><h5>function</h5>  <h3>recv_forward</h3></div><div class="title-source">&lt;<a href="https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/pipeline/schedule/zero_bubble_pp.py#L239" class="title-source">source</a>&gt;</div></div><div class="signature">(<!-- -->model_chunk_id: int, prev_rank: int = None<!-- -->)</div><div><div class="divider"><span class="divider-text">Parameters</span></div><ul>
<li><strong>model_chunk_id</strong> (int) -- The current model chunk idx.</li>
<li><strong>prev_rank</strong> (int, optional) -- The rank of the source of the tensor.</li>
</ul></div><div><div class="divider"><span class="divider-text">Returns</span></div><p>Any: The input tensor or input tensor list.
Any: The wait handles for the communication.</p></div></div><div><div class="divider"><span class="divider-text">Description</span></div>Copy the forward output from the previous stage in pipeline as the input tensor of this stage. For ZBV.</div></div><div class="docstring-container"><div><div class="title-container"><div class="title-module"><h5>function</h5>  <h3>run_forward_backward</h3></div><div class="title-source">&lt;<a href="https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/pipeline/schedule/zero_bubble_pp.py#L871" class="title-source">source</a>&gt;</div></div><div class="signature">(<!-- -->model_chunk: typing.Union[torch.nn.modules.container.ModuleList, torch.nn.modules.module.Module], data_iter: typing.Iterable, criterion: typing.Callable[..., typing.Any], optimizer: typing.Optional[colossalai.interface.optimizer.OptimizerWrapper] = None, return_loss: bool = False, return_outputs: bool = False<!-- -->)</div></div><div><div class="divider"><span class="divider-text">Description</span></div><p>Runs Zerobubble schedule, with communication between pipeline stages.</p></div></div><div class="docstring-container"><div><div class="title-container"><div class="title-module"><h5>function</h5>  <h3>schedule_b</h3></div><div class="title-source">&lt;<a href="https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/pipeline/schedule/zero_bubble_pp.py#L741" class="title-source">source</a>&gt;</div></div><div class="signature">(<!-- -->scheduled_node, model_chunk: typing.Union[torch.nn.modules.container.ModuleList, torch.nn.modules.module.Module], model_chunk_id: int, optimizer: OptimizerWrapper<!-- -->)</div><div><div class="divider"><span class="divider-text">Parameters</span></div><p>scheduled_node --</p>
<ul>
<li><strong>model_chunk</strong> (ModuleList or Module) -- Model Chunk to be run;</li>
<li><strong>model_chunk_id</strong> (int) -- The current model chunk idx;</li>
</ul></div><div><div class="divider"><span class="divider-text">Returns</span></div><p>: Nothing.</p></div></div><div><div class="divider"><span class="divider-text">Description</span></div>A complete backward b schedule; Include recv bwd --&gt; cal bwd step --&gt; send bwd;</div></div><div class="docstring-container"><div><div class="title-container"><div class="title-module"><h5>function</h5>  <h3>schedule_f</h3></div><div class="title-source">&lt;<a href="https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/pipeline/schedule/zero_bubble_pp.py#L636" class="title-source">source</a>&gt;</div></div><div class="signature">(<!-- -->scheduled_node, model_chunk: ModuleList, model_chunk_id: int, criterion: typing.Callable, accum_loss: typing.Optional[torch.Tensor] = None, outputs: typing.Optional[typing.List[typing.Any]] = None<!-- -->)</div><div><div class="divider"><span class="divider-text">Parameters</span></div><p>scheduled_node --</p>
<ul>
<li><strong>model_chunk</strong> (ModuleList or Module) -- Model Chunk to be run;</li>
<li><strong>model_chunk_id</strong> (int) -- The current model chunk idx;</li>
<li><strong>criterion</strong> (Callable) -- loss function;</li>
<li><strong>accum_loss</strong> (Optional[torch.Tensor], optional) -- Accumulated loss. Defaults to None.</li>
<li><strong>outputs</strong> (Optional[List[Any]], optional) -- List to store the output of the last stage (final output). Defaults to None.</li>
</ul></div><div><div class="divider"><span class="divider-text">Returns</span></div><p>: Nothing.</p></div></div><div><div class="divider"><span class="divider-text">Description</span></div>A complete forward schedule; Include recv fwd --&gt; cal fwd --&gt; send fwd;</div></div><div class="docstring-container"><div><div class="title-container"><div class="title-module"><h5>function</h5>  <h3>schedule_w</h3></div><div class="title-source">&lt;<a href="https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/pipeline/schedule/zero_bubble_pp.py#L809" class="title-source">source</a>&gt;</div></div><div class="signature">(<!-- -->scheduled_node, model_chunk: typing.Union[torch.nn.modules.container.ModuleList, torch.nn.modules.module.Module], model_chunk_id: int, optimizer: OptimizerWrapper<!-- -->)</div><div><div class="divider"><span class="divider-text">Parameters</span></div><p>scheduled_node --</p>
<ul>
<li><strong>model_chunk</strong> (ModuleList or Module) -- Model Chunk to be run;</li>
<li><strong>model_chunk_id</strong> (int) -- The current model chunk idx;</li>
</ul></div><div><div class="divider"><span class="divider-text">Returns</span></div><p>: Nothing.</p></div></div><div><div class="divider"><span class="divider-text">Description</span></div>A complete backward w schedule; Include get y &amp; dy from buffer --&gt; cal bwd w step(cal dw &amp; update w);</div></div><div class="docstring-container"><div><div class="title-container"><div class="title-module"><h5>function</h5>  <h3>send_backward</h3></div><div class="title-source">&lt;<a href="https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/pipeline/schedule/zero_bubble_pp.py#L415" class="title-source">source</a>&gt;</div></div><div class="signature">(<!-- -->model_chunk_id: int, prev_rank: int = None<!-- -->)</div><div><div class="divider"><span class="divider-text">Parameters</span></div><ul>
<li><strong>model_chunk_id</strong> (int) -- The current model chunk idx.</li>
<li><strong>prev_rank</strong> (int, optional) -- The rank of the recipient of the tensor</li>
</ul></div><div><div class="divider"><span class="divider-text">Returns</span></div><p>Any: The wait handles for the communication.</p></div></div><div><div class="divider"><span class="divider-text">Description</span></div>Sends the gradient tensor to the previous stage in pipeline. For ZBV.</div></div><div class="docstring-container"><div><div class="title-container"><div class="title-module"><h5>function</h5>  <h3>send_forward</h3></div><div class="title-source">&lt;<a href="https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/pipeline/schedule/zero_bubble_pp.py#L356" class="title-source">source</a>&gt;</div></div><div class="signature">(<!-- -->model_chunk_id: int, next_rank: int = None<!-- -->)</div><div><div class="divider"><span class="divider-text">Parameters</span></div><ul>
<li><strong>model_chunk_id</strong> (int) -- The current model chunk idx.</li>
<li><strong>next_rank</strong> (int, optional) -- The rank of the recipient of the tensor.</li>
</ul></div><div><div class="divider"><span class="divider-text">Returns</span></div><p>Any: The wait handles for the communication.</p></div></div><div><div class="divider"><span class="divider-text">Description</span></div>Sends the input tensor to the next stage in pipeline. For ZBV.</div></div></div></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/features/zerobubble_pipeline_parallelism.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>编辑此页</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="文档分页导航"></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#介绍" class="table-of-contents__link toc-highlight">介绍</a></li><li><a href="#使用" class="table-of-contents__link toc-highlight">使用</a><ul><li><a href="#step-1-引用仓库" class="table-of-contents__link toc-highlight">step 1. 引用仓库</a></li><li><a href="#step-2-初始化分布式环境" class="table-of-contents__link toc-highlight">step 2. 初始化分布式环境</a></li><li><a href="#step-3-初始化模型优化器" class="table-of-contents__link toc-highlight">step 3. 初始化模型优化器</a></li><li><a href="#step-4初始化流水线schedule" class="table-of-contents__link toc-highlight">step 4.初始化流水线Schedule</a></li><li><a href="#step-5初始化booster" class="table-of-contents__link toc-highlight">step 5.初始化Booster</a></li><li><a href="#step-6训练模型" class="table-of-contents__link toc-highlight">step 6.训练模型</a></li></ul></li><li><a href="#进阶使用技巧" class="table-of-contents__link toc-highlight">进阶使用技巧</a><ul><li><a href="#1在zerobubble中使用元数据缓存" class="table-of-contents__link toc-highlight">1.在ZeroBubble中使用元数据缓存</a></li><li><a href="#2同时使用zerobubble和混合并行" class="table-of-contents__link toc-highlight">2.同时使用ZeroBubble和混合并行</a></li></ul></li><li><a href="#模型兼容性" class="table-of-contents__link toc-highlight">模型兼容性</a></li><li><a href="#api-参考" class="table-of-contents__link toc-highlight">API 参考</a></li></ul></div></div></div></div></main></div></div><footer class="footer"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">资源</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/zh-Hans/docs/get_started/installation">教程</a></li><li class="footer__item"><a href="https://github.com/hpcaitech/ColossalAI/tree/main/examples" target="_blank" rel="noopener noreferrer" class="footer__link-item">例子</a></li><li class="footer__item"><a href="https://github.com/hpcaitech/ColossalAI/discussions" target="_blank" rel="noopener noreferrer" class="footer__link-item">论坛</a></li></ul></div><div class="col footer__col"><div class="footer__title">社区</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/hpcaitech/ColossalAI" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub</a></li><li class="footer__item"><a href="https://www.hpc-ai.tech/blog" target="_blank" rel="noopener noreferrer" class="footer__link-item">博客</a></li><li class="footer__item"><a href="https://twitter.com/HPCAITech" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter</a></li></ul></div><div class="col footer__col"><div class="footer__title">关于</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://www.hpc-ai.tech/" target="_blank" rel="noopener noreferrer" class="footer__link-item">公司</a></li><li class="footer__item"><a href="https://www.hpc-ai.tech/services" target="_blank" rel="noopener noreferrer" class="footer__link-item">服务</a></li><li class="footer__item"><a href="https://www.hpc-ai.tech/customers" target="_blank" rel="noopener noreferrer" class="footer__link-item">客户</a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 All Rights Reserved by HPC-AI Technology Inc.</div></div></div></footer></div>
<script src="/zh-Hans/assets/js/runtime~main.33da9ac8.js"></script>
<script src="/zh-Hans/assets/js/main.2f022937.js"></script>
</body>
</html>