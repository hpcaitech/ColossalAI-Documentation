<!doctype html>
<html class="docs-version-v0.2.2" lang="zh-Hans" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.14">
<link rel="alternate" type="application/rss+xml" href="/zh-Hans/blog/rss.xml" title="Colossal-AI RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/zh-Hans/blog/atom.xml" title="Colossal-AI Atom Feed">
<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-1XKZVCCKRZ"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-1XKZVCCKRZ",{anonymize_ip:!0})</script>
<link rel="search" type="application/opensearchdescription+xml" title="Colossal-AI" href="/zh-Hans/opensearch.xml">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" integrity="sha384-Um5gpz1odJg5Z4HAmzPtgZKdTBHZdw8S29IecapCSB31ligYPhHQZMIlWLYQGVoc" crossorigin="anonymous">
<script src="https://snack.expo.io/embed.js" async></script>
<script src="https://js-eu1.hs-scripts.com/26563514.js" async defer="defer" id="hs-script-loader"></script><title data-react-helmet="true">使用ColoTensor让串行程序像Megatron-LM一样并行 | Colossal-AI</title><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:url" content="https://colossalai.org/zh-Hans/docs/advanced_tutorials/parallelize_your_training_like_Megatron"><meta data-react-helmet="true" name="docsearch:language" content="zh-Hans"><meta data-react-helmet="true" name="docsearch:version" content="v0.2.2"><meta data-react-helmet="true" name="docsearch:docusaurus_tag" content="docs-default-v0.2.2"><meta data-react-helmet="true" property="og:title" content="使用ColoTensor让串行程序像Megatron-LM一样并行 | Colossal-AI"><meta data-react-helmet="true" name="description" content="Author: Haichen Huang and Jiarui Fang"><meta data-react-helmet="true" property="og:description" content="Author: Haichen Huang and Jiarui Fang"><link data-react-helmet="true" rel="icon" href="/zh-Hans/img/favicon.ico"><link data-react-helmet="true" rel="canonical" href="https://colossalai.org/zh-Hans/docs/advanced_tutorials/parallelize_your_training_like_Megatron"><link data-react-helmet="true" rel="alternate" href="https://colossalai.org/docs/advanced_tutorials/parallelize_your_training_like_Megatron" hreflang="en"><link data-react-helmet="true" rel="alternate" href="https://colossalai.org/zh-Hans/docs/advanced_tutorials/parallelize_your_training_like_Megatron" hreflang="zh-Hans"><link data-react-helmet="true" rel="alternate" href="https://colossalai.org/docs/advanced_tutorials/parallelize_your_training_like_Megatron" hreflang="x-default"><link data-react-helmet="true" rel="preconnect" href="https://XP2V2KAOVI-dsn.algolia.net" crossorigin="anonymous"><link rel="stylesheet" href="/zh-Hans/assets/css/styles.b5c97f7b.css">
<link rel="preload" href="/zh-Hans/assets/js/runtime~main.aab5813c.js" as="script">
<link rel="preload" href="/zh-Hans/assets/js/main.255c5581.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<link rel="preconnect" href="https://fonts.googleapis.com"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&amp;display=swap" rel="stylesheet"><div class="Toastify"></div><div><a href="#" class="skipToContent_1oUP">跳到主要内容</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner InnerContainer_1wkI"><div class="navbar__items"><a class="navbar__brand" href="/zh-Hans/"><div class="navbar__logo"><img src="/zh-Hans/img/logo.svg" alt="Colossal-AI" class="themedImage_1VuW themedImage--light_3UqQ"><img src="/zh-Hans/img/logo.svg" alt="Colossal-AI" class="themedImage_1VuW themedImage--dark_hz6m"></div><b class="navbar__title"> </b></a><a class="navbar__item navbar__link" href="/zh-Hans/docs/get_started/installation">教程</a><a href="https://github.com/hpcaitech/ColossalAI/tree/main/examples" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">例子</a><a href="http://docs.colossalai.org" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">API文档</a><a href="https://www.hpc-ai.tech/blog" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">博客</a></div><div class="navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/zh-Hans/docs/advanced_tutorials/add_your_parallel">v0.2.2</a><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" class="navbar__link"><span><svg viewBox="0 0 20 20" width="20" height="20" aria-hidden="true" class="iconLanguage_3vod"><path fill="currentColor" d="M19.753 10.909c-.624-1.707-2.366-2.726-4.661-2.726-.09 0-.176.002-.262.006l-.016-2.063 3.525-.607c.115-.019.133-.119.109-.231-.023-.111-.167-.883-.188-.976-.027-.131-.102-.127-.207-.109-.104.018-3.25.461-3.25.461l-.013-2.078c-.001-.125-.069-.158-.194-.156l-1.025.016c-.105.002-.164.049-.162.148l.033 2.307s-3.061.527-3.144.543c-.084.014-.17.053-.151.143.019.09.19 1.094.208 1.172.018.08.072.129.188.107l2.924-.504.035 2.018c-1.077.281-1.801.824-2.256 1.303-.768.807-1.207 1.887-1.207 2.963 0 1.586.971 2.529 2.328 2.695 3.162.387 5.119-3.06 5.769-4.715 1.097 1.506.256 4.354-2.094 5.98-.043.029-.098.129-.033.207l.619.756c.08.096.206.059.256.023 2.51-1.73 3.661-4.515 2.869-6.683zm-7.386 3.188c-.966-.121-.944-.914-.944-1.453 0-.773.327-1.58.876-2.156a3.21 3.21 0 011.229-.799l.082 4.277a2.773 2.773 0 01-1.243.131zm2.427-.553l.046-4.109c.084-.004.166-.01.252-.01.773 0 1.494.145 1.885.361.391.217-1.023 2.713-2.183 3.758zm-8.95-7.668a.196.196 0 00-.196-.145h-1.95a.194.194 0 00-.194.144L.008 16.916c-.017.051-.011.076.062.076h1.733c.075 0 .099-.023.114-.072l1.008-3.318h3.496l1.008 3.318c.016.049.039.072.113.072h1.734c.072 0 .078-.025.062-.076-.014-.05-3.083-9.741-3.494-11.04zm-2.618 6.318l1.447-5.25 1.447 5.25H3.226z"></path></svg><span>简体中文</span></span></a><ul class="dropdown__menu"><li><a href="/docs/advanced_tutorials/parallelize_your_training_like_Megatron" target="_self" rel="noopener noreferrer" class="dropdown__link">English</a></li><li><a href="/zh-Hans/docs/advanced_tutorials/parallelize_your_training_like_Megatron" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active">简体中文</a></li></ul></div><div class="displayOnlyInLargeViewport_2uzv IconContainer_aWwG"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" class="Icon_3e04" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></div><div class="toggle_71bT toggleDisabled_3cF-"><div class="toggleTrack_32Fl" role="button" tabindex="-1"><div class="toggleTrackCheck_3lV7"><span class="toggleIcon_O4iE">🌜</span></div><div class="toggleTrackX_S2yS"><span class="toggleIcon_O4iE">🌞</span></div><div class="toggleTrackThumb_xI_Z"></div></div><input type="checkbox" class="toggleScreenReader_28Tw" aria-label="Switch between dark and light mode"></div><div class="searchBox_1ZXk"><button type="button" class="DocSearch DocSearch-Button" aria-label="搜索"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">搜索</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div aria-label="Navigation bar toggle" class="navbar__toggle" role="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="Wrapper_ReNv main-docs-wrapper"><div class="docPage_3AUJ"><button aria-label="回到顶部" class="clean-btn theme-back-to-top-button backToTopButton_35hR" type="button"></button><main class="docMainContainer_2AUC"><div class="padding-vert--lg container docItemWrapper_1WZa"><div class="row"><div class="col docItemCol_3FnS"><div class="docItemContainer_33ec"><article><div class="tocCollapsible_1PrD theme-doc-toc-mobile tocMobile_3Hoh"><button type="button" class="clean-btn tocCollapsibleButton_2O1e">本页总览</button></div><div class="theme-doc-markdown markdown"><header><h1>使用ColoTensor让串行程序像Megatron-LM一样并行</h1></header><p>Author: <a href="https://github.com/1SAA" target="_blank" rel="noopener noreferrer">Haichen Huang</a> and <a href="https://github.com/feifeibear" target="_blank" rel="noopener noreferrer">Jiarui Fang</a></p><p><strong>Prerequisite:</strong></p><ul><li><a href="/zh-Hans/docs/basics/colotensor_concept">ColoTensor Concepts</a></li></ul><h2 class="anchor anchorWithHideOnScrollNavbar_3R7-" id="介绍">介绍<a class="hash-link" href="#介绍" title="标题的直接链接">​</a></h2><p>在新版本中，我们引入了ColoTensor。ColoTensor为用户使用并行训练提供了极大的便利，使得用户可以在原本的串行代码上，通过较小的修改将训练改为并行。在本教程中，我们将说明如何修改训练模型以自动使代码采取像 Megatron-LM 一样的方式并行训练。我们以 HuggingFace 提供的 GPT-2 模型为例，并提供一种方式让你可以在单个GPU上预训练GPT-2模型。</p><p>Megatron-LM 提供了一个具有影响力的并行化范式，这个范式主要应用于Transformer大模型的训练。然而，为了大规模训练 Transformer 语言大模型，用户必须使用Megatron-LM提供的特殊模块来构建他们的模型。这给用户带来了一些困难的工作，例如从预先训练的模型中加载权重，或是构建自己的并行训练模型。为了减轻用户的麻烦，我们提供 ColoTensor 类，以完成自动启用张量模型并行。</p><h2 class="anchor anchorWithHideOnScrollNavbar_3R7-" id="定义模型和损失函数">定义模型和损失函数<a class="hash-link" href="#定义模型和损失函数" title="标题的直接链接">​</a></h2><p>首先，我们直接调用 HuggingFace 库中的 GPTModel 和 GPTLoss。</p><div class="codeBlockContainer_K1bP language-python theme-code-block"><div class="codeBlockContent_hGly python"><pre tabindex="0" class="prism-code language-python codeBlock_23N8 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_39YC"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> torch</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> torch</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">nn </span><span class="token keyword" style="color:#00009f">as</span><span class="token plain"> nn</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> transformers </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> GPT2Config</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> GPT2LMHeadModel</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">class</span><span class="token plain"> </span><span class="token class-name">GPTLMModel</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">nn</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">Module</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">__init__</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">self</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> hidden_size</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">768</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> num_layers</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">12</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> num_attention_heads</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">12</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> max_seq_len</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">1024</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> vocab_size</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">50257</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> checkpoint</span><span class="token operator" style="color:#393A34">=</span><span class="token boolean" style="color:#36acaa">False</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token builtin">super</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">__init__</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">checkpoint </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> checkpoint</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">model </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> GPT2LMHeadModel</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">GPT2Config</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">n_embd</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">hidden_size</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> n_layer</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">num_layers</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                     n_head</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">num_attention_heads</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> n_positions</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">max_seq_len</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> n_ctx</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">max_seq_len</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> vocab_size</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">vocab_size</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token keyword" style="color:#00009f">if</span><span class="token plain"> checkpoint</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">model</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">gradient_checkpointing_enable</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">forward</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">self</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> input_ids</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> attention_mask</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># Only return lm_logits</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token keyword" style="color:#00009f">return</span><span class="token plain"> self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">model</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">input_ids</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">input_ids</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> attention_mask</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">attention_mask</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> use_cache</span><span class="token operator" style="color:#393A34">=</span><span class="token keyword" style="color:#00009f">not</span><span class="token plain"> self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">checkpoint</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">[</span><span class="token number" style="color:#36acaa">0</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">class</span><span class="token plain"> </span><span class="token class-name">GPTLMLoss</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">nn</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">Module</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">__init__</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">self</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token builtin">super</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">__init__</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">loss_fn </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> nn</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">CrossEntropyLoss</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">forward</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">self</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> logits</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> labels</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        shift_logits </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> logits</span><span class="token punctuation" style="color:#393A34">[</span><span class="token punctuation" style="color:#393A34">.</span><span class="token punctuation" style="color:#393A34">.</span><span class="token punctuation" style="color:#393A34">.</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">:</span><span class="token operator" style="color:#393A34">-</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">:</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">contiguous</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        shift_labels </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> labels</span><span class="token punctuation" style="color:#393A34">[</span><span class="token punctuation" style="color:#393A34">.</span><span class="token punctuation" style="color:#393A34">.</span><span class="token punctuation" style="color:#393A34">.</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">:</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">contiguous</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># Flatten the tokens</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token keyword" style="color:#00009f">return</span><span class="token plain"> self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">loss_fn</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">shift_logits</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">view</span><span class="token punctuation" style="color:#393A34">(</span><span class="token operator" style="color:#393A34">-</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> shift_logits</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">size</span><span class="token punctuation" style="color:#393A34">(</span><span class="token operator" style="color:#393A34">-</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> shift_labels</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">view</span><span class="token punctuation" style="color:#393A34">(</span><span class="token operator" style="color:#393A34">-</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><button type="button" aria-label="复制代码到剪贴板" class="copyButton_Ue-o clean-btn">复制</button></div></div><h2 class="anchor anchorWithHideOnScrollNavbar_3R7-" id="对gpt-2的简短回顾">对GPT-2的简短回顾<a class="hash-link" href="#对gpt-2的简短回顾" title="标题的直接链接">​</a></h2><p>现在，我们回顾一下 GPT-2 模型的结构。每个 GPT-2 模型都可以表示为一个 DAG。如下图所示，每个圆圈代表一个算子，每个方块代表一个权重。每个箭头表示输入数据的流向，而箭头旁边的符号表示输入数据的形状。</p><p>然后，让我们深入了解一下这个 GPT-2 模型。它由三部分组成，分别是<strong>嵌入模块</strong>、<strong>转换器层</strong>和<strong>分类头</strong>。</p><p>嵌入模块包含两个权重，符号嵌入权重和位置嵌入权重。在嵌入模块的前向操作之后，原始输入数据的所有序列中的每个单词都会被嵌入到隐藏状态。</p><figure style="text-align:center"><img src="https://s2.loli.net/2022/08/17/omfkIEN6ui5jcL3.png"><figcaption>嵌入模块</figcaption></figure><p>每个转换器层包含两个块。自注意操作在第一个块中调用，同时一个双层感知器位于第二个块中。</p><figure style="text-align:center"><img src="https://s2.loli.net/2022/08/17/LAVzDlpRcj4dYeb.png"><figcaption>转换器层</figcaption></figure><p>最后，分类头只是一个不加偏差的线性模块，里面只有一个线性权重。</p><h2 class="anchor anchorWithHideOnScrollNavbar_3R7-" id="应用colotensor">应用ColoTensor<a class="hash-link" href="#应用colotensor" title="标题的直接链接">​</a></h2><p>两个步骤使您的串行代码采取 Megatron-LM 张量并行风格。</p><ol><li>在ColoInitContext的上下文中初始化模型。</li><li>为每个参数设置 ColoTensorSpec。</li></ol><h3 class="anchor anchorWithHideOnScrollNavbar_3R7-" id="使用-coloinitcontext-初始化">使用 ColoInitContext 初始化<a class="hash-link" href="#使用-coloinitcontext-初始化" title="标题的直接链接">​</a></h3><p>我们应该在 ColoInitContext 中构建模型。在该种上下文中，任何初始化的参数都将转换为 ColoParameter 并自动移动到相应的设备上。</p><div class="codeBlockContainer_K1bP language-python theme-code-block"><div class="codeBlockContent_hGly python"><pre tabindex="0" class="prism-code language-python codeBlock_23N8 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_39YC"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> colossalai</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">utils</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">model</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">colo_init_context </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> ColoInitContext</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">with</span><span class="token plain"> ColoInitContext</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">device</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">torch</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">device</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&#x27;cpu&#x27;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    model </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> GPTLMModel</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><button type="button" aria-label="复制代码到剪贴板" class="copyButton_Ue-o clean-btn">复制</button></div></div><h3 class="anchor anchorWithHideOnScrollNavbar_3R7-" id="为每个参数设置-colotensorspec">为每个参数设置 ColoTensorSpec<a class="hash-link" href="#为每个参数设置-colotensorspec" title="标题的直接链接">​</a></h3><p>模型创建完成后，我们通过ProcessGroup建立分布式环境。这里，我们将张量并行度指定为所有GPU的数量，即数据并行度为一。</p><div class="codeBlockContainer_K1bP language-python theme-code-block"><div class="codeBlockContent_hGly python"><pre tabindex="0" class="prism-code language-python codeBlock_23N8 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_39YC"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> torch</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">distributed </span><span class="token keyword" style="color:#00009f">as</span><span class="token plain"> dist</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> colossalai</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">tensor </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> ProcessGroup</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">pg </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> ProcessGroup</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">tp_degree</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">dist</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">get_world_size</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><button type="button" aria-label="复制代码到剪贴板" class="copyButton_Ue-o clean-btn">复制</button></div></div><p>现在，我们需要一些辅助函数为下一步做准备。我们定义了两个函数来切分参数。Megatron-LM张量并行需要沿参数的第一维或最后一维切分参数张量。</p><div class="codeBlockContainer_K1bP language-python theme-code-block"><div class="codeBlockContent_hGly python"><pre tabindex="0" class="prism-code language-python codeBlock_23N8 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_39YC"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> colossalai</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">tensor </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> ShardSpec</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> ComputeSpec</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> ComputePattern</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> ColoParameter</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> ProcessGroup</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">split_param_single_dim_tp1d</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">dim</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token builtin">int</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> param</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> ColoParameter</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> pg</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> ProcessGroup</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    spec </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">ShardSpec</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">dim</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">pg</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">tp_world_size</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> ComputeSpec</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">ComputePattern</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">TP1D</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">if</span><span class="token plain"> param</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">process_group</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">tp_world_size</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">==</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        param</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">set_process_group</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">pg</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    param</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">set_tensor_spec</span><span class="token punctuation" style="color:#393A34">(</span><span class="token operator" style="color:#393A34">*</span><span class="token plain">spec</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">split_param_row_tp1d</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">param</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> ColoParameter</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> pg</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> ProcessGroup</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    split_param_single_dim_tp1d</span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">0</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> param</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> pg</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">split_param_col_tp1d</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">param</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> ColoParameter</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> pg</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> ProcessGroup</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    split_param_single_dim_tp1d</span><span class="token punctuation" style="color:#393A34">(</span><span class="token operator" style="color:#393A34">-</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> param</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> pg</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><button type="button" aria-label="复制代码到剪贴板" class="copyButton_Ue-o clean-btn">复制</button></div></div><p>然后我们使模型采用张量并行。根据 Megatron 中使用的张量并行，应该沿着张量的最后一个维度进行切片，包括符号嵌入的权重，位置嵌入的权重，自注意力块中的所有线性权重和偏差，以及每个双层感知器中的第一个线性权重和偏差。且需要沿第一个维度切分双层感知器中的第二个线性权重。</p><div class="codeBlockContainer_K1bP language-python theme-code-block"><div class="codeBlockContent_hGly python"><pre tabindex="0" class="prism-code language-python codeBlock_23N8 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_39YC"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> mn</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> module </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> model</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">named_modules</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> pn</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> param </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> module</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">named_parameters</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">recurse</span><span class="token operator" style="color:#393A34">=</span><span class="token boolean" style="color:#36acaa">False</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># set process group for all parameters</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        param</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">set_process_group</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">pg</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token keyword" style="color:#00009f">if</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&#x27;mlp.c_fc&#x27;</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> mn</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token keyword" style="color:#00009f">if</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&#x27;weight&#x27;</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> pn </span><span class="token keyword" style="color:#00009f">or</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&#x27;bias&#x27;</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> pn</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                split_param_col_tp1d</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">param</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> pg</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain">  </span><span class="token comment" style="color:#999988;font-style:italic"># colmn slice</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                </span><span class="token comment" style="color:#999988;font-style:italic"># keep the shape of the output from c_fc</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                param</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">compute_spec</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">set_output_replicate</span><span class="token punctuation" style="color:#393A34">(</span><span class="token boolean" style="color:#36acaa">False</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token keyword" style="color:#00009f">elif</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&#x27;mlp.c_proj&#x27;</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> mn</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token keyword" style="color:#00009f">if</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&#x27;weight&#x27;</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> pn</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                split_param_row_tp1d</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">param</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> pg</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain">  </span><span class="token comment" style="color:#999988;font-style:italic"># row slice</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token keyword" style="color:#00009f">elif</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&#x27;wte&#x27;</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> mn </span><span class="token keyword" style="color:#00009f">or</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&#x27;wpe&#x27;</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> mn</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            split_param_col_tp1d</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">param</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> pg</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain">  </span><span class="token comment" style="color:#999988;font-style:italic"># colmn slice</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token keyword" style="color:#00009f">elif</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&#x27;c_attn&#x27;</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> mn </span><span class="token keyword" style="color:#00009f">or</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&#x27;c_proj&#x27;</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> mn</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            split_param_col_tp1d</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">param</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> pg</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain">  </span><span class="token comment" style="color:#999988;font-style:italic"># colmn slice</span><br></span></code></pre><button type="button" aria-label="复制代码到剪贴板" class="copyButton_Ue-o clean-btn">复制</button></div></div><p>修改后的模型如下图所示。</p><p>嵌入模块:</p><figure style="text-align:center"><img src="https://s2.loli.net/2022/08/17/Yu2xzXEabHV7pwe.png"><figcaption>修改后的嵌入模块</figcaption></figure><p>转换器层:</p><figure style="text-align:center"><img src="https://s2.loli.net/2022/08/17/4HWsA2xz51IhPFO.png"><figcaption>修改后的转换器层</figcaption></figure><p>一旦用户指定了每个参数的在并行中的分布模式，ColoTensor 就能够推断出所有算子的计算模式，包括矩阵乘法、线性函数、torch.nn.functional 中的其他逐元素函数，以及其他的一些常用函数。这样，用户可以像往常一样训练他们的模型。</p><p>在我们最新示例中还定义了一个Gemini + ZeRO DDP 的模型从而减小开销，提升效率。这一部分的详细内容可以参考<a href="/zh-Hans/docs/features/zero_with_chunk">ZeRO</a>，你可以将这两部分内容结合起来看从而理解我们整个训练流程：</p><div class="codeBlockContainer_K1bP language-python theme-code-block"><div class="codeBlockContent_hGly python"><pre tabindex="0" class="prism-code language-python codeBlock_23N8 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_39YC"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">gemini_zero_dpp</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">model</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> torch</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">nn</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">Module</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> pg</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> ProcessGroup</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> placememt_policy</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token builtin">str</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;auto&quot;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> colossalai</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">nn</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">parallel </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> GeminiDDP</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    model </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> GeminiDDP</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">model</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                        device</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">get_current_device</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                        placement_policy</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">placememt_policy</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                        pin_memory</span><span class="token operator" style="color:#393A34">=</span><span class="token boolean" style="color:#36acaa">True</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                        search_range_mb</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">32</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">return</span><span class="token plain"> model</span><br></span></code></pre><button type="button" aria-label="复制代码到剪贴板" class="copyButton_Ue-o clean-btn">复制</button></div></div><h2 class="anchor anchorWithHideOnScrollNavbar_3R7-" id="在单个gpu上预训练gpt-2">在单个GPU上预训练GPT-2<a class="hash-link" href="#在单个gpu上预训练gpt-2" title="标题的直接链接">​</a></h2><p>我们做的上述优化让我们可以在单GPU上训练GPT-2模型，只需要将<code>run.sh</code>中设置参数<code>GPUNUM</code>=1，再运行文件时就可以在单个GPU上完成模型的训练。</p><p>GPT-2 示例在<a href="https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/gpt" target="_blank" rel="noopener noreferrer">Train GPT with Colossal-AI</a>. 获得。</p></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="文档分页导航"><div class="pagination-nav__item"></div><div class="pagination-nav__item pagination-nav__item--next"></div></nav></div></div><div class="col col--3"><div class="tableOfContents_35-E thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#介绍" class="table-of-contents__link toc-highlight">介绍</a></li><li><a href="#定义模型和损失函数" class="table-of-contents__link toc-highlight">定义模型和损失函数</a></li><li><a href="#对gpt-2的简短回顾" class="table-of-contents__link toc-highlight">对GPT-2的简短回顾</a></li><li><a href="#应用colotensor" class="table-of-contents__link toc-highlight">应用ColoTensor</a><ul><li><a href="#使用-coloinitcontext-初始化" class="table-of-contents__link toc-highlight">使用 ColoInitContext 初始化</a></li><li><a href="#为每个参数设置-colotensorspec" class="table-of-contents__link toc-highlight">为每个参数设置 ColoTensorSpec</a></li></ul></li><li><a href="#在单个gpu上预训练gpt-2" class="table-of-contents__link toc-highlight">在单个GPU上预训练GPT-2</a></li></ul></div></div></div></div></main></div></div><footer class="footer Container_ZO7N"><div class="InnerContainer_kEOB"><div class="ContentContainer_Bd2W"><div class="FooterLeft_UU7Q"><div class="BrandContainer_37NB"><img class="BrandImage_1lbV" alt="AgileTs Logo" height="30" src="/img/logo.svg"></div><div class="Tagline_2AGk">具有高效并行化技术的集成大规模模型训练系统</div><button class="ButtonContainer_oPl2 GithubButton_1Y3L"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" class="GithubIcon_3htU" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg><div>GITHUB</div></button></div><div class="FooterRight_1NHD"><div class="SectionContainer_2QT6"><li class="LinkItemTitle_1y09">资源</li><ul class="LinkItemContainer_2Lmc"><a class="LinkText_1B0E" href="/zh-Hans/docs/get_started/installation" label="教程" to="docs/get_started/installation">教程</a></ul><ul class="LinkItemContainer_2Lmc"><a class="LinkText_1B0E" href="http://docs.colossalai.org" label="API文档" to="http://docs.colossalai.org">API文档</a></ul><ul class="LinkItemContainer_2Lmc"><a class="LinkText_1B0E" href="https://github.com/hpcaitech/ColossalAI/tree/main/examples" label="例子" to="https://github.com/hpcaitech/ColossalAI/tree/main/examples">例子</a></ul><ul class="LinkItemContainer_2Lmc"><a class="LinkText_1B0E" href="https://github.com/hpcaitech/ColossalAI/discussions" label="论坛" to="https://github.com/hpcaitech/ColossalAI/discussions">论坛</a></ul></div><div class="SectionContainer_2QT6"><li class="LinkItemTitle_1y09">社区</li><ul class="LinkItemContainer_2Lmc"><a class="LinkText_1B0E" href="https://github.com/hpcaitech/ColossalAI" rel="noopener noreferrer" target="_blank" label="GitHub">GitHub</a></ul><ul class="LinkItemContainer_2Lmc"><a class="LinkText_1B0E" href="https://www.hpc-ai.tech/blog" rel="noopener noreferrer" target="_blank" label="博客">博客</a></ul><ul class="LinkItemContainer_2Lmc"><a class="LinkText_1B0E" href="https://twitter.com/HPCAITech" rel="noopener noreferrer" target="_blank" label="Twitter">Twitter</a></ul></div></div></div><div class="BottomContainer_1let"><div class="CopyrightText_25Fq">Copyright © 2023 All Rights Reserved by Luchen Technology Inc.</div></div></div></footer></div>
<script src="/zh-Hans/assets/js/runtime~main.aab5813c.js"></script>
<script src="/zh-Hans/assets/js/main.255c5581.js"></script>
</body>
</html>